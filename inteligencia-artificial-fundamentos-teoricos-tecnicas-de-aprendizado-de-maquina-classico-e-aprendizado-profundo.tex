% ===================================================================
% ARQUIVO MESTRE DO LIVRO
% Autor: Seu Nome
% Projeto: Um Mergulho Profundo no Aprendizado de Máquina
% ===================================================================

% --- CLASSE DO DOCUMENTO ---
% Usamos a classe 'abntex2' para seguir as normas da ABNT.
% As opções configuram o formato para um livro acadêmico padrão.
\documentclass[
    12pt,            % Tamanho da fonte do corpo do texto
    a4paper,         % Tamanho do papel
    book,            % Formato de livro
    openright,       % Força capítulos a começarem em páginas ímpares (da direita)
    twoside,         % Layout para impressão frente e verso (margens diferentes)
    brazil,          % Configurações para o idioma português do Brasil
    citacao=authoryear
]{abntex2}


% --- PREÂMBULO ---
% Importa todas as configurações, pacotes e comandos customizados
% do arquivo 'preambulo.tex'. Isso mantém este arquivo principal limpo
% e focado apenas na estrutura do conteúdo do livro.
\input{preambulo.tex}


% --- INFORMAÇÕES DO DOCUMENTO (para a folha de rosto) ---
\title{INTELIGÊNCIA ARTIFICIAL \\ 
    \large Fundamentos Teóricos, Técnicas de Aprendizado de Máquina Clássico e Aprendizado Profundo}
\author{Luiz Guilherme Morais da Costa Faria}
\date{\today} % Usa a data atual no momento da compilação

% Informações adicionais que o abntex2 usa (opcional)
\instituicao{Universidade de Brasília}
\local{Brasília, DF}
\orientador{Nome do Orientador/Revisor (se aplicável)}


% ===================================================================
% INÍCIO DO DOCUMENTO
% ===================================================================
\begin{document}

\begin{acronym}
    % --- Siglas Gerais ---
    \acro{IA}{Inteligência Artificial}
    \acro{ML}{Aprendizado de Máquina (Machine Learning)} % Embora não explícito, é o tema central
    \acro{RNA}{Rede Neural Artificial} % Implícito no contexto de redes neurais
    \acro{DNN}{Rede Neural Profunda (Deep Neural Network)}
    \acro{GPU}{Unidade de Processamento Gráfico (Graphics Processing Unit)} % Mencionada no contexto de processamento

    % --- Otimizadores (Cap. 6 e Apêndice A) ---
    \acro{GD}{Gradiente Descendente (Gradient Descent)}
    \acro{SGD}{Gradiente Descendente Estocástico (Stochastic Gradient Descent)}
    \acro{NAG}{Gradiente Acelerado de Nesterov (Nesterov Accelerated Gradient)}
    \acro{AdaGrad}{Adaptive Gradient Algorithm}
    \acro{RMSProp}{Root Mean Square Propagation}
    \acro{Adam}{Adaptive Moment Estimation}
    \acro{AdaMax}{Adaptive Moment Estimation based on the infinity norm}
    \acro{Nadam}{Nesterov-accelerated Adaptive Moment Estimation}
    \acro{AdamW}{Adam with Decoupled Weight Decay}
    \acro{RAdam}{Rectified Adam}

    % --- Funções de Ativação (Cap. 7, 8, 9 e Apêndice B) ---
    \acro{ReLU}{Unidade Linear Retificada (Rectified Linear Unit)}
    \acro{LReLU}{Leaky Rectified Linear Unit}
    \acro{PReLU}{Parametric Rectified Linear Unit}
    \acro{RReLU}{Randomized Leaky Rectified Linear Unit}
    \acro{ELU}{Unidade Linear Exponencial (Exponential Linear Unit)}
    \acro{SELU}{Unidade Linear Exponencial Escalonada (Scaled Exponential Linear Unit)}
    \acro{GELU}{Unidade Linear de Erro Gaussiano (Gaussian Error Linear Unit)}
    \acro{SiLU}{Sigmoid Linear Unit}
    \acro{h-swish}{Hard-Swish}
    \acro{h-mish}{Hard-Mish}

    % --- Funções de Perda (Cap. 10, 11, 12 e Apêndice C) ---
    \acro{MSE}{Erro Quadrático Médio (Mean Squared Error)}
    \acro{MAE}{Erro Absoluto Médio (Mean Absolute Error)}
    \acro{MSLE}{Erro Quadrático Médio Logarítmico (Mean Squared Logarithmic Error)}
    \acro{MAPE}{Erro Percentual Absoluto Médio (Mean Absolute Percentage Error)}
    \acro{sMAPE}{Erro Percentual Absoluto Médio Simétrico (Symmetric Mean Absolute Percentage Error)}
    \acro{BCE}{Entropia Cruzada Binária (Binary Cross-Entropy)}
    \acro{WCE}{Entropia Cruzada Ponderada Binária (Binary Weighted Cross-Entropy)}
    \acro{CCE}{Entropia Cruzada Categórica (Categorical Cross-Entropy)}
    \acro{WCCE}{Weighted Categorical Cross-Entropy}
    \acro{KL}{Divergência Kullback-Leibler (Kullback-Leibler Divergence)}
    \acro{FL}{Focal Loss}

    % --- Métricas (Cap. 13 e Apêndice D) ---
    \acro{VP}{Verdadeiro Positivo (True Positive)}
    \acro{VN}{Verdadeiro Negativo (True Negative)}
    \acro{FP}{Falso Positivo (False Positive)}
    \acro{FN}{Falso Negativo (False Negative)}
    \acro{ROC}{Característica de Operação do Receptor (Receiver Operating Characteristic)}
    \acro{AUC}{Área Sob a Curva (Area Under the Curve)}
    \acro{RMSE}{Raiz do Erro Quadrático Médio (Root Mean Square Error)}
    \acro{R2}{Coeficiente de Determinação (Coefficient of Determination)}

    % --- Redes Neurais e Componentes (Parte V) ---
    \acro{MLP}{Perceptron Multicamadas (Multi-Layer Perceptron)}
    \acro{FFN}{Rede FeedForward (FeedForward Network)}
    \acro{DBN}{Rede de Crença Profunda (Deep Belief Network)}
    \acro{RBM}{Máquina de Boltzmann Restrita (Restricted Boltzmann Machine)}
    \acro{CNN}{Rede Neural Convolucional (Convolutional Neural Network)}
    \acro{FCN}{Rede Totalmente Convolucional (Fully Convolutional Network)}
    \acro{YOLO}{You Only Look Once}
    \acro{ResNet}{Rede Residual (Residual Network)}
    \acro{SENet}{Squeeze-and-Excitation Network}
    \acro{RNN}{Rede Neural Recorrente (Recurrent Neural Network)}
    \acro{LSTM}{Memória Longa de Curto Prazo (Long Short-Term Memory)}
    \acro{GRU}{Unidade Recorrente Gated (Gated Recurrent Unit)}
    \acro{ViT}{Vision Transformer}
    \acro{GAN}{Rede Adversária Generativa (Generative Adversarial Network)}
    \acro{MoE}{Mistura de Especialistas (Mixture of Experts)}
    \acro{GNN}{Rede Neural de Grafos (Graph Neural Network)}

    % --- Outras ---
    \acro{PCA}{Análise de Componentes Principais (Principal Component Analysis)}
    \acro{SVD}{Decomposição em Valores Singulares (Singular Value Decomposition)}
    \acro{t-SNE}{t-Distributed Stochastic Neighbor Embedding}
    \acro{UMAP}{Uniform Manifold Approximation and Projection}
    \acro{DBSCAN}{Density-Based Spatial Clustering of Applications with Noise}
    \acro{SVM}{Máquina de Vetores de Suporte (Support Vector Machine)}
    \acro{ILSVRC}{Desafio de Reconhecimento Visual em Larga Escala ImageNet (ImageNet Large Scale Visual Recognition Challenge)}
    \acro{SIFT}{Scale-Invariant Feature Transform}
    \acro{FVs}{Fisher Vectors}
    \acro{SNN}{Rede Neural Auto-Normalizadora (Self-Normalizing Neural Network)}
    \acro{WER}{Taxa de Erro de Palavra (Word Error Rate)}
    \acro{SWBD}{Switchboard (dataset)}
    \acro{CH}{CallHome (dataset)}
    \acro{EV}{Conjunto de Avaliação (Evaluation set)}
    \acro{LAD}{Mínimo Desvio Absoluto (Least Absolute Deviation)}
    \acro{EW-RSM}{Erro Quadrático Médio Exponencialmente Ponderado (Exponentially Weighted Mean Squared Error)}
    \acro{fMAE}{Erro Absoluto Médio Ponderado pela Frequência (Frequency-weighted Mean Absolute Error)}
    \acro{RDA}{Regularized Dual Averaging}
    \acro{FB}{Forward-Backward splitting} 
    \acro{PA}{Passive-Aggressive algorithms} 
    \acro{AROW}{Adaptive Regularization of Weight Vectors} 
\end{acronym}

\newglossaryentry{labelUnica}{
    name={Nome do Termo},
    description={A explicação clara e concisa do que diabos esse termo significa. Pode ser longa, pode ter \textit{itálico}, o que você precisar.},
    symbol={\ensuremath{\eta}} % Opcional: Se tiver um símbolo associado, use \ensuremath{} para modo matemático
}

% Exemplo Concreto do seu livro:
\newglossaryentry{retropapagacao}{
    name={Retropropagação},
    description={Um dos principais algoritmos para o treinamento de redes neurais. Permite o aprendizado atráves do ajuste sucessivo dos parâmetros da rede com auxílio do cálculo do gradiente da perda propagado das camadas finais para as camadas iniciais.} \index{Glossário!Retropropagação} % Indexar no índice remissivo também? Boa ideia!
}

\newglossaryentry{gradiente-descendente}{ 
    name={Gradiente descendente},
    description={Método de otimização iterativo que tem como objetivo encontrar pontos de mínimo de uma função (geralmente funções de perda) dando pequenos "passos" na direção contrária do vetor gradiente.} \index{Glossário!Gradiente Descendente}
}

\newglossaryentry{sigmoide}{
    name={Sigmoide logística},
    description={Função de ativação do tipo sigmoidal que foi comumente empregada em redes neurais antes da popularização das funções retificadoras. Pode ser utilizada na camada de saída de um modelo para que a sua saída fique limitada em um intervalo [0,1], sendo útil para problemas de classificação binárias.}
    symbol={\Loss_{\text{sigmoid}}}
}

\newglossaryentry{tangente-hiperbolica}{
    name={Tangente hiperbólica},
    description={Função de ativação do tipo sigmoidal que, assim como a sigmoide, foi amplamente utilizada em redes neurais antes do surgimento das funções retificadoras. Ela é uma função limitada em um intervalo de [-1,1], que tem como característica empurrar os valores de sua entrada para esses extremos.}
    symbol={\Loss_{\text{tanh}}}
}

% --- ELEMENTOS PRÉ-TEXTUAIS ---
% O comando \frontmatter inicia a contagem de páginas em algarismos romanos (i, ii, ...)
% e desativa a numeração de capítulos para os elementos iniciais.
\frontmatter

% Gera a capa e a folha de rosto com base nas informações acima,
% seguindo o padrão ABNT.
\imprimircapa
\imprimirfolhaderosto

% Páginas opcionais como Dedicatória, Agradecimentos, Epígrafe...
% \begin{dedicatoria}
%    \vspace*{\fill} % Centraliza verticalmente
%    \noindent
%    \textit{Para ...}
%    \vspace*{\fill}
% \end{dedicatoria}

% Gera o Sumário automaticamente com base nos comandos \part, \chapter, \section, etc.
\tableofcontents

% --- IMPRIMIR A LISTA DE SIGLAS ---
\cleardoublepage
\phantomsection
\chapter*{Lista de Siglas}
\addcontentsline{toc}{chapter}{Lista de Siglas}

% --- ELEMENTOS TEXTUAIS (O CONTEÚDO PRINCIPAL DO LIVRO) ---
% O comando \mainmatter reinicia a contagem de páginas em algarismos arábicos (1, 2, ...)
% e reativa a numeração de capítulos.
\mainmatter

% =======================================================
% PARTE I: HISTÓRIA DA IA E DO COMPUTADOR
% =======================================================
\part{História da IA e do Computador}

\include{capitulos/parte-I-historia/cap-01-historia-do-computador}
\include{capitulos/parte-I-historia/cap-02-historia-da-ia}

% =======================================================
% PARTE II: Conceitos Matemáticos
% =======================================================
\part{Conceitos Matemáticos}

\include{capitulos/parte-II-matematica/cap-03-calculo}
\include{capitulos/parte-II-matematica/cap-04-algebra-linear}
\include{capitulos/parte-II-matematica/cap-05-probabilidade-e-estatistica}

% =======================================================
% PARTE III: Pilares das Redes Neurais
% =======================================================
\part{Pilares das Redes Neurais}

\include{capitulos/parte-III-pilares/cap-06-retropropagacao-e-gradiente}
\include{capitulos/parte-III-pilares/cap-07-sigmoidais}
\include{capitulos/parte-III-pilares/cap-08-retificadoras}
\include{capitulos/parte-III-pilares/cap-09-modernas}
\include{capitulos/parte-III-pilares/cap-xx-perda-regressao}
\include{capitulos/parte-III-pilares/cap-xx-perda-classificacao}
\include{capitulos/parte-III-pilares/cap-xx-perdas-especificas}
\include{capitulos/parte-III-pilares/cap-xx-metricas}
\include{capitulos/parte-III-pilares/cap-xx-autodiff}
\include{capitulos/parte-III-pilares/cap-12-metaheuristicas}

% =======================================================
% PARTE IV: APRENDIZADO DE MÁQUINA CLÁSSICO
% =======================================================
\part{Aprendizado de Máquina Clássico}

% O comando '\include' inicia uma nova página para cada capítulo e
% carrega o conteúdo do arquivo .tex especificado.
\include{capitulos/parte-IV-ml-classico/cap-13-regressao}
\include{capitulos/parte-IV-ml-classico/cap-14-arvores}
\include{capitulos/parte-IV-ml-classico/cap-15-svm}
\include{capitulos/parte-IV-ml-classico/cap-16-ensemble}
\include{capitulos/parte-IV-ml-classico/cap-17-dimensionalidade}
\include{capitulos/parte-IV-ml-classico/cap-18-clusterizacao}

% =======================================================
% PARTE V: REDES NEURAIS PROFUNDAS (DNNs)
% =======================================================
\part{Redes Neurais Profundas (DNNs)}

\include{capitulos/parte-V-deep-learning/cap-19-mlp}
\include{capitulos/parte-V-deep-learning/cap-20-ffns}
\include{capitulos/parte-V-deep-learning/cap-21-dbns}
\include{capitulos/parte-V-deep-learning/cap-22-cnns}
\include{capitulos/parte-V-deep-learning/cap-23-resnets}
\include{capitulos/parte-V-deep-learning/cap-24-rnns}
\include{capitulos/parte-V-deep-learning/cap-25-regularizacao}
\include{capitulos/parte-V-deep-learning/cap-26-transformers}
\include{capitulos/parte-V-deep-learning/cap-27-gans}
\include{capitulos/parte-V-deep-learning/cap-28-moe}
\include{capitulos/parte-V-deep-learning/cap-29-diffusion}
\include{capitulos/parte-V-deep-learning/cap-30-gnns}
% Adicione o capítulo de otimizadores se ele estiver aqui


% --- APÊNDICES ---
% O comando \appendix muda a formatação dos capítulos para "Apêndice A", "Apêndice B", etc.
\appendix
\part{Apêndices}

\include{capitulos/apendices/tabela-otimizadores}
\include{capitulos/apendices/tabela-funcoes-de-ativacao}
\include{capitulos/apendices/tabela-funcoes-de-perda}
\include{capitulos/apendices/tabela-metricas}


% \include{apendices/ap_B_guia_setup}


% --- ELEMENTOS PÓS-TEXTUAIS ---
% O comando \backmatter é usado para as seções finais do livro.
\backmatter

% Gera a lista de Referências a partir do arquivo 'bibliografia.bib',
% formatada no estilo ABNT pelo biblatex.
\printbibliography[title={Referências}]

% --- IMPRIMIR O GLOSSÁRIO ---
\cleardoublepage % Começar em página nova
\phantomsection % Ajuda links
\printglossaries % Este comando imprime TODAS as listas de glossário definidas
% Não precisa de \addcontentsline se usou a opção [toc] no \usepackage

\printindex


\end{document}
% ===================================================================
% FIM DO DOCUMENTO
% ===================================================================