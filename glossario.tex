\newglossaryentry{labelUnica}{
    name={Nome do Termo},
    description={A explicação clara e concisa do que diabos esse termo significa. Pode ser longa, pode ter \textit{itálico}, o que você precisar.},
    symbol={\ensuremath{\eta}} % Opcional: Se tiver um símbolo associado, use \ensuremath{} para modo matemático
}

% Exemplo Concreto do seu livro:
\newglossaryentry{retropapagacao}{
    name={Retropropagação},
    description={Um dos principais algoritmos para o treinamento de redes neurais. Permite o aprendizado atráves do ajuste sucessivo dos parâmetros da rede com auxílio do cálculo do gradiente da perda propagado das camadas finais para as camadas iniciais.} \index{Glossário!Retropropagação} % Indexar no índice remissivo também? Boa ideia!
}

\newglossaryentry{gradiente-descendente}{ 
    name={Gradiente descendente},
    description={Método de otimização iterativo que tem como objetivo encontrar pontos de mínimo de uma função (geralmente funções de perda) dando pequenos "passos" na direção contrária do vetor gradiente.} \index{Glossário!Gradiente Descendente}
}

\newglossaryentry{sigmoide}{
    name={Sigmoide logística},
    description={Função de ativação do tipo sigmoidal que foi comumente empregada em redes neurais antes da popularização das funções retificadoras. Pode ser utilizada na camada de saída de um modelo para que a sua saída fique limitada em um intervalo [0,1], sendo útil para problemas de classificação binárias.} \index{Glossário!Sigmoide Logística}
}

\newglossaryentry{tangente-hiperbolica}{
    name={Tangente hiperbólica},
    description={Função de ativação do tipo sigmoidal que, assim como a sigmoide, foi amplamente utilizada em redes neurais antes do surgimento das funções retificadoras. Ela é uma função limitada em um intervalo de [-1,1], que tem como característica empurrar os valores de sua entrada para esses extremos.} \index{Glossário!Tangente Hiperbólica}
}

\newglossaryentry{softsign}{
    name={\textit{softsign}},
    description={Função de ativação do tipo sigmoidal que tem como principal objetivo ser uma alternativa mais "barata" em termos de custo computacional para as opções mais tradicionais: sigmoide logística e tangente hiperbólica.} \index{Glossário!\textit{Softsign}}
}

\newglossaryentry{metodo-do-gradiente-em-batch}{
    name={Método do gradiente em \textit{batch}},
    description={Otimizador para redes neurais que utiliza todos os parâmetros do modelo de uma vez para serem atualizados com base no vetor gradiente.} \index{Glossário!Método do gradiente em \textit{batch}}
}

\newglossaryentry{metodo-do-gradiente-estocastico}{
    name={Método do gradiente estocástico},
    description={Otimizador para redes neurais que utiliza um parâmetro por vez, escolhido aleatoriamente, para ser atualizado com base no vetor gradiente. Por ser um processo de natureza estocástica, o \textit{SGD} apresenta uma trajetória ruidosa, isso o ajuda a escapar de mínimos locais, contudo, também atrapalha a convergência para o ponto de mínimo global.} \index{Glossário!Método do gradiente estocástico}
}

\newglossaryentry{metodo-do-gradiente-em-mini-batch}{
    name={Método do gradiente em \textit{mini-batch}},
    description={Otimizador para redes neurais que busca ser uma alternativa entre o método do gradiente estocastico e o método do gradiente em batch. Nessa forma, o \textit{GD Mini-Batch} escolhe uma lote (\textit{batch}) aleatoriamente e calcula a atualização de parâmetros para esse lote.} \index{Glossário!Método do gradiente em \textit{mini-batch}}
}

\newglossaryentry{gradiente-acelerado-de-nesterov}{
    name={Gradiente acelerado de Nesterov},
    description={Otimizador para redes neurais que faz uso de um ponto de \textit{lookahead} para acelerar a convergência do modelo que está sendo treinado. Para isso, o \textit{NAG} utiliza do momento para encontrar o ponto de \textit{lookahead}, e a partir desse novo ponto calcula  o vetor gradiente e atualização de parâmetros.} \index{Glossário!Gradiente acelerado de Nesterov}
}

\newglossaryentry{metodo-do-gradiente-com-momento}{
    name={Método do gradiente com momento},
    description={Otimizador para redes neurais que faz uso do momento para acelerar a convergência do modelo que está sendo treinado.} \index{Glossário!Método do gradiente com momento}
}

\newglossaryentry{adagrad}{
    name={\textit{Adadptive Gradient Algorithm (AdaGrad)}},
    description={Otimizador moderno para redes neurais que tem como principal diferencial a distribuição do gradiente de forma diferente com base na frequência que cada característica aparece no treinamento. Características menos frequentes recebem um gradiente maior, resultadando em uma atualização de pesos maior e uma forma de fazer o modelo "prestar mais atenção" em características incomuns.} \index{Glossário!\textit{Adaptive Gradient Algorithm(AdaGrad)}}
}