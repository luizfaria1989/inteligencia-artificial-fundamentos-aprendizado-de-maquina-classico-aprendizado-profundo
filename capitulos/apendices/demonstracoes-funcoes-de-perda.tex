\chapter{Propriedades Analíticas das Funções de Perda}%
\label{apendice:propriedades-analiticas-funcoes-de-perda}

\section{Definições Preliminares e Notação}%
\label{sec:definicoes}

Para assegurar a precisão matemática necessária, harmonizamos as definições clássicas de Análise Real com a notação moderna de Aprendizado Profundo.

\subsection{Classes de Diferenciabilidade}

Embora a literatura clássica, como \textcite{rudin1976principles}, utilize a notação $\mathscr{C}'$ e $\mathscr{C}''$ para denotar espaços de funções continuamente diferenciáveis, adotamos a notação moderna $C^k$, comum em otimização, para facilitar a leitura.

\begin{definition}[Classes $C^k$]
    Baseado em Rudin [Definições 9.20 e 9.39], dizemos que uma função $f: U \subset \mathbb{R}^n \to \mathbb{R}$ é de classe $C^k$ se suas derivadas parciais até a ordem $k$ existem e são contínuas em $U$.
    \begin{itemize}
        \item $C^0$: Funções contínuas.
        \item $C^1$: Funções com gradiente contínuo (Rudin $\mathscr{C}'$).
        \item $C^2$: Funções com Hessiana contínua (Rudin $\mathscr{C}''$).
        \item $C^\infty$: Funções infinitamente diferenciáveis (Suaves).
    \end{itemize}
\end{definition}

\subsection{Robustez e Continuidade de Lipschitz}

Em Deep Learning, a estabilidade do treinamento está ligada à limitação da taxa de variação da função de perda. Adotamos a definição vetorial encontrada em \textcite{DeepLearningBook}, adaptando a notação da constante para evitar ambiguidade com a função de perda $\Loss$.

\begin{definition}[Continuidade de Lipschitz]
    Uma função $f: \mathbb{R}^n \to \mathbb{R}$ é dita \textbf{Lipschitz-contínua} se existe uma constante $K \in \mathbb{R}^+$ (chamada constante de Lipschitz) tal que, para todos $\boldsymbol{x}, \boldsymbol{y}$ no domínio:
    \begin{equation}
        |f(\boldsymbol{x}) - f(\boldsymbol{y})| \le K \|\boldsymbol{x} - \boldsymbol{y}\|_2
    \end{equation}
    Onde $\|\cdot\|_2$ denota a norma Euclidiana ($L^2$).
\end{definition}

\begin{remark}[Interpretação Escalar]
    Nas demonstrações deste apêndice, onde analisamos o erro $e \in \mathbb{R}$ (escalar), a norma Euclidiana reduz-se ao valor absoluto: $\|\boldsymbol{x} - \boldsymbol{y}\|_2 \to |x - y|$. A condição torna-se $|f(x) - f(y)| \le K |x - y|$.
\end{remark}

\subsection{Critérios de Verificação de Convexidade}

Para demonstrar a convexidade das funções de perda sem recorrer exaustivamente à definição geométrica, utilizaremos três teoremas fundamentais de operações que preservam a convexidade, conforme estabelecido em \textcite{boyd2004convex}\footnote{Apresentamos aqui as versões escalares dos teoremas de convexidade de \textcite{boyd2004convex}, simplificadas para o contexto de funções de perda unidimensionais. Para a formulação geral vetorial envolvendo Matrizes Hessianas, consulte o texto original, Capítulo 3.}.

\begin{theorem}[Condição de Segunda Ordem]
    Seja $f$ uma função duas vezes diferenciável. $f$ é convexa se, e somente se, sua segunda derivada for não-negativa em todo o domínio:
    \[ f''(x) \ge 0, \quad \forall x \in \text{dom } f \]
\end{theorem}

\begin{theorem}[Preservação por Composição Afim]
    Se $f: \mathbb{R} \to \mathbb{R}$ é convexa, então a composição $g(x) = f(ax + b)$ é convexa para quaisquer escalares $a, b$. Isso garante que provar a convexidade em relação ao erro $e$ implica convexidade em relação à predição $\hat{y}$.
\end{theorem}

\begin{theorem}[Convexidade de Normas e Máximos]
    \begin{enumerate}
        \item Se $f$ e $g$ são funções convexas, então $h(x) = \max(f(x), g(x))$ é convexa.
        \item Qualquer norma $\|\cdot\|$ é convexa, consequência direta da Desigualdade Triangular: $\|x+y\| \le \|x\| + \|y\|$.
    \end{enumerate}
\end{theorem}

\section{Perdas para regressão}

\subsection{Erro quadrático médio (MSE)}%
\label{ap:deducoes-mse}

\begin{definition}
    Seja $e = \hat{y} - y$ o resíduo entre a predição e o alvo. A função de perda erro quadrático médio é definida por $\Loss(e) = e^2$.
\end{definition}

\begin{proposition}[Regularidade]
    A função erro quadrático médio pertence à classe $C^{\infty}$ (é infinitamente diferenciável).
\end{proposition}

\begin{proof}
    Para provar que $\Loss \in C^{\infty}$ devemos demonstrar que existem derivadas contínuas de todas as ordens $k \in \mathbb{N}$.

    Calculamos as derivadas sucessivas de $\Loss(e)$ em relação a $e$:

    \begin{align*}
        \Loss'(e) &= \frac{d}{de} (e^2) = 2e \\
        \Loss''(e) &= \frac{d}{de} (2e) = 2 \\
        \Loss^{(k)}(e) &= 0, \quad \forall k \ge 3
    \end{align*}

    Analisamos a continuidade das funções resultantes:

    \begin{itemize}
        \item $\Loss'(e) = 2e$, é polinômio linear, contínuo em todo $\mathbb{R}$.
        \item $\Loss''(e) = 2$ e $\mathcal{L}^{(k)}(e) = 0$ são funções constantes, contínuas em todo $\mathbb{R}$.
    \end{itemize}

    Como $\Loss^{(k)}$ existe e é contínua para todo $k \ge 1$, conclui-se que $\mathcal{L} \in C^{\infty}$.
\end{proof}

\begin{proposition}[Convexidade]
    A função erro quadrático médio é estritamente convexa.
\end{proposition}

\begin{proof}
    Utilizamos o critério da segunda derivada. Uma função duas vezes diferenciável é estritamente convexa se, e somente se, sua segunda derivada for estritamente positiva em todo o domínio.
    
    Da proposição anterior, temos:
    \[ \Loss''(e) = 2 \]

    Observamos que $2 > 0$ para todo $e \in \mathbb{R}$. Portanto, a função possui curvatura positiva constante, satisfazendo a condição de convexidade estrita.
\end{proof}

\begin{proposition}[Robustez]
    A função erro quadrático médio não é Lipschitz-contínua (não é robusta a \textit{outliers})
\end{proposition}

\begin{proof}
    Uma função diferenciável é Lipschitz-contínua se existir uma constante $K \in \mathbb{R}$ tal que $\sup_{e} |\mathcal{L}'(e)| \le K$.
    
    Analisamos o comportamento assintótico da magnitude do gradiente:
    \[ \lim_{e \to \infty} |\Loss'(e)| = \lim_{e \to \infty} |2e| = \infty \]

    Como o gradiente cresce indefinidamente à medida que o erro aumenta, não existe uma constante $K$ que limite a derivada superiormente. Logo, a função não é Lipschitz-contínua.
\end{proof}

\subsection{Erro absoluto médio (MAE)}%
\label{ap:deducoes-mae}

\begin{definition}
    Seja $e = \hat{y} - y$ o resíduo entre a predição e o alvo. A função de perda erro absoluto médio é definida por $\Loss(e) = |e|$.
\end{definition}

\begin{proposition}[Regularidade]
    A função erro absoluto médio pertence à classe $C^0$ (contínua), mas não pertence à classe $C^1$.
\end{proposition}

\begin{proof}
    A função módulo é contínua por definição, logo $\Loss \in C^0$. Para verificar se é $C^1$, analisamos a derivada no ponto $e=0$.
    Calculamos os limites laterais da derivada:
    \begin{align*}
        \lim_{e \to 0^+} \frac{d}{de}(e) &= 1 \\
        \lim_{e \to 0^-} \frac{d}{de}(-e) &= -1
    \end{align*}
    Como os limites laterais são distintos ($1 \neq -1$), a derivada clássica não existe em $e=0$. Portanto, a função não é continuamente diferenciável em todo o domínio ($\mathcal{L} \notin C^1$).
\end{proof}

\begin{proposition}[Convexidade]
    A função Erro Absoluto Médio é convexa.
\end{proposition}

\begin{proof}
    A função é definida por $\mathcal{L}(e) = |e|$, que constitui a norma $L_1$ no espaço unidimensional ($\mathbb{R}$).
    
    Conforme estabelecido no \textbf{Teorema de Convexidade de Normas} (Seção \ref{sec:definicoes}), toda norma é necessariamente uma função convexa. Portanto, a convexidade do MAE segue diretamente dessa propriedade, sem necessidade de demonstração algébrica adicional.
\end{proof}

\begin{proposition}[Robustez]
    A função erro absoluto médio é Lipschitz-contínua (é robusta a \textit{outliers}).
\end{proposition}

\begin{proof}
    Como a função não é diferenciável na origem, analisamos o seu subdiferencial $\partial \mathcal{L}(e)$. O subgradiente do valor absoluto é dado por:
    \[
    \partial |e| = \begin{cases} 
    \{-1\} & \text{se } e < 0 \\
    [-1, 1] & \text{se } e = 0 \\
    \{1\} & \text{se } e > 0 
    \end{cases}
    \]
    Para provar a robustez (Lipschitz), verificamos se o subgradiente é limitado. Observamos que para qualquer $g \in \partial \mathcal{L}(e)$:
    \[ |g| \le 1, \quad \forall e \in \mathbb{R} \]
    Como a magnitude da ``inclinação'' generalizada nunca excede 1, a função é 1-Lipschitz contínua.
\end{proof}

\subsection{Perda de Huber (Huber loss)}%
\label{ap:deducoes-huber-loss}

\begin{definition}
    Seja $\delta > 0$ um hiperparâmetro de transição. A função de perda de Huber é definida por partes:
    \begin{equation}
        \mathcal{L}_\delta(e) = \begin{cases} 
        \frac{1}{2}e^2 & \text{se } |e| \le \delta \\
        \delta(|e| - \frac{1}{2}\delta) & \text{se } |e| > \delta 
        \end{cases}
    \end{equation}
\end{definition}

\begin{proposition}[Regularidade e Colagem Suave]
    A função de Huber pertence à classe $C^1$ (continuamente diferenciável), mas não pertence à classe $C^2$.
\end{proposition}

\begin{proof}
    A função é composta por polinômios ($C^\infty$) em cada região aberta. A regularidade global depende do comportamento nos pontos de transição $e = \pm \delta$.
    
    Calculamos a primeira derivada:
    \[
    \mathcal{L}'(e) = \begin{cases} 
    e & \text{se } |e| \le \delta \\
    \delta \cdot \text{sgn}(e) & \text{se } |e| > \delta 
    \end{cases}
    \]
    Verificamos a continuidade da derivada em $e = \delta$:
    \begin{align*}
        \lim_{e \to \delta^-} \mathcal{L}'(e) &= \delta \\
        \lim_{e \to \delta^+} \mathcal{L}'(e) &= \delta \cdot 1 = \delta
    \end{align*}
    Como os limites coincidem, $\mathcal{L}'$ é contínua em $\delta$ (e analogamente em $-\delta$). Logo, $\mathcal{L} \in C^1$.
    
    Para a segunda derivada:
    \[
    \mathcal{L}''(e) = \begin{cases} 
    1 & \text{se } |e| < \delta \\
    0 & \text{se } |e| > \delta 
    \end{cases}
    \]
    Observa-se uma descontinuidade de salto nos pontos $\pm \delta$ (de 1 para 0). Portanto, a segunda derivada não é contínua, implicando $\mathcal{L} \notin C^2$.
\end{proof}

\begin{proposition}[Convexidade]
    A função de Huber é convexa em todo o domínio $\mathbb{R}$.
\end{proposition}

\begin{proof}
    Utilizamos o \textbf{Critério de Segunda Ordem} (Teorema \ref{thm:segunda_ordem}). Analisamos o sinal da segunda derivada calculada anteriormente:
    \begin{itemize}
        \item Na região quadrática ($|e| < \delta$): $\mathcal{L}''(e) = 1 > 0$.
        \item Na região linear ($|e| > \delta$): $\mathcal{L}''(e) = 0 \ge 0$.
    \end{itemize}
    Como $\mathcal{L}''(e) \ge 0$ em todos os pontos onde é definida (e a função é $C^1$), conclui-se que a função possui curvatura não-negativa globalmente, satisfazendo a condição de convexidade.
\end{proof}

\begin{proposition}[Robustez]
    A função de Huber é Lipschitz-contínua com constante $K=\delta$, sendo robusta a \textit{outliers}.
\end{proposition}

\begin{proof}
    Analisamos a magnitude do gradiente globalmente. A derivada primeira é dada pela função de corte (\textit{clip}):
    \[ \mathcal{L}'(e) = \max(-\delta, \min(e, \delta)) \]
    
    Portanto, o valor absoluto da derivada é limitado superiormente pelo hiperparâmetro $\delta$:
    \[ |\mathcal{L}'(e)| \le \delta, \quad \forall e \in \mathbb{R} \]
    
    Pelo critério do gradiente limitado (Seção \ref{sec:definicoes}), a função é Lipschitz-contínua. Diferente do MSE, onde o gradiente cresce indefinidamente, a Huber Loss satura a força da penalidade em $\delta$, garantindo estabilidade frente a grandes erros.
\end{proof}

\subsection{Perda log-cosh (log-cosh loss)}%
\label{ap:deducoes-log-cosh}

\begin{definition}
    Seja $e = \hat{y} - y$ o resíduo. A função de perda Log-Cosh é definida pelo logaritmo natural do cosseno hiperbólico do erro:
    \begin{equation}
        \mathcal{L}(e) = \log(\cosh(e))
    \end{equation}
\end{definition}

\begin{proposition}[Regularidade e Suavidade]
    A função Log-Cosh pertence à classe $C^\infty$ (é infinitamente diferenciável em todo o domínio).
\end{proposition}

\begin{proof}
    A função é uma composição de funções elementares: $f(x) = \log(x)$ e $g(e) = \cosh(e)$.
    Sabemos que $\cosh(e)$ é $C^\infty$ em $\mathbb{R}$ e que $\cosh(e) \ge 1$ para todo $e$. Como o logaritmo é $C^\infty$ em $(0, \infty)$, a composição $\log(\cosh(e))$ preserva a suavidade.
    
    Verificamos as primeiras derivadas:
    \begin{align*}
        \mathcal{L}'(e) &= \frac{d}{de} \log(\cosh(e)) = \frac{1}{\cosh(e)} \cdot \sinh(e) = \tanh(e) \\
        \mathcal{L}''(e) &= \frac{d}{de} \tanh(e) = \text{sech}^2(e)
    \end{align*}
    Como $\tanh(e)$ e $\text{sech}(e)$ são funções suaves compostas por exponenciais, todas as derivadas de ordem superior existem e são contínuas. Portanto, $\mathcal{L} \in C^\infty$.
\end{proof}

\begin{proposition}[Convexidade]
    A função Log-Cosh é estritamente convexa.
\end{proposition}

\begin{proof}
    Utilizamos o critério da segunda derivada. Da demonstração anterior, temos:
    \[ \Loss''(e) = \text{sech}^2(e) = \frac{1}{\cosh^2(e)} \]
    
    Sabemos que $\cosh(e) \ge 1$ para todo número real, o que implica $1 \le \cosh^2(e) < \infty$. Consequentemente, o inverso quadrado é estritamente positivo:
    \[ 0 < \frac{1}{\cosh^2(e)} \le 1 \]
    
    Como $\mathcal{L}''(e) > 0$ em todo $\mathbb{R}$, a função é estritamente convexa.
\end{proof}

\begin{proposition}[Robustez]
    A função Log-Cosh é Lipschitz-contínua e robusta a \textit{outliers}.
\end{proposition}

\begin{proof}
    Analisamos a magnitude do gradiente $\mathcal{L}'(e) = \tanh(e)$.
    Sabemos pelas propriedades da tangente hiperbólica que sua imagem é o intervalo aberto $(-1, 1)$. Portanto:
    \[ |\mathcal{L}'(e)| = |\tanh(e)| < 1, \quad \forall e \in \mathbb{R} \]
    
    Como a derivada é limitada globalmente por $K=1$, a função é 1-Lipschitz contínua. 
    
    \textit{Observação:} Note que assintoticamente, quando $|e| \to \infty$, a derivada se aproxima de $\pm 1$ (comportamento do MAE), mas na origem ela se aproxima de $0$ linearmente (comportamento do MSE), combinando as melhores propriedades de ambas sem perder a diferenciabilidade.
\end{proof}

\subsection{Erro quadrático logarítmico médio (MSLE)}
\label{ap:deducoes-msle}

\begin{definition}
    Seja $y \ge 0$ o valor alvo e $\hat{y} \ge 0$ a predição (com a restrição de não-negatividade usual para dados log-normais). A função MSLE é definida por:
    \begin{equation}
        \mathcal{L}(y, \hat{y}) = (\log(1 + y) - \log(1 + \hat{y}))^2
    \end{equation}
    Para simplificar a notação nas demonstrações, definimos o termo constante $C = \log(1+y)$ e analisamos a função em relação à variável $\hat{y}$.
\end{definition}

\begin{proposition}[Regularidade]
    A função MSLE pertence à classe $C^{\infty}$ no domínio $\hat{y} > -1$.
\end{proposition}

\begin{proof}
    A função é uma composição de funções elementares: a função logarítmica (suave em $\mathbb{R}^+$), a função afim $1+\hat{y}$ e a função quadrática.
    
    Calculamos as derivadas em relação a $\hat{y}$:
    \begin{align*}
        \mathcal{L}'(\hat{y}) &= 2(\log(1+y) - \log(1+\hat{y})) \cdot \frac{d}{d\hat{y}}(-\log(1+\hat{y})) \\
        &= -2 \frac{\log(1+y) - \log(1+\hat{y})}{1+\hat{y}}
    \end{align*}
    
    Como o denominador $(1+\hat{y})$ nunca é zero para $\hat{y} \ge 0$ e o logaritmo é infinitamente diferenciável nesse domínio, todas as derivadas de ordem superior existem e são contínuas.
\end{proof}

\begin{proposition}[Convexidade no Espaço Logarítmico]
    A função MSLE é convexa em relação à predição transformada $z = \log(1+\hat{y})$, mas não é globalmente convexa em relação a $\hat{y}$.
\end{proposition}

\begin{proof}
    A análise de convexidade direta em $\hat{y}$ revela que $\mathcal{L}''(\hat{y})$ pode assumir valores negativos para $\hat{y}$ muito grandes, violando a convexidade global no espaço original.
    
    No entanto, se considerarmos a transformação do espaço de saída para a escala logarítmica, definindo $z = \log(1+\hat{y})$ e $c = \log(1+y)$, a função se torna:
    \[ G(z) = (c - z)^2 \]
    
    A derivada segunda em relação a $z$ é:
    \[ G''(z) = 2 > 0 \]
    
    Portanto, a MSLE é estritamente convexa \textit{no espaço logarítmico}. Isso justifica seu uso em problemas onde a magnitude das variáveis varia exponencialmente, pois a otimização se comporta como um MSE sobre os expoentes.
\end{proof}

\begin{proposition}[Robustez e Continuidade de Lipschitz]
    A função MSLE é Lipschitz-contínua (com constante $K > 0$) e seu gradiente tende a zero para erros extremos.
\end{proposition}

\begin{proof}
    Para provar que a função é Lipschitz, precisamos mostrar que a magnitude da derivada é limitada superiormente em todo o domínio ($\sup |\mathcal{L}'| < \infty$).
    
    A derivada é dada por:
    \[ \mathcal{L}'(\hat{y}) = -2 \frac{\log(1+y) - \log(1+\hat{y})}{1+\hat{y}} \]
    
    Observamos que:
    \begin{enumerate}
        \item A função $\mathcal{L}'$ é contínua no intervalo $[0, \infty)$.
        \item No limite $\hat{y} \to \infty$, a derivada tende a 0.
        \item No ponto $\hat{y} = y$ (erro zero), a derivada é 0.
    \end{enumerate}
    
    Pelo Teorema de Weierstrass generalizado, uma função contínua que tende a zero no infinito é necessariamente limitada (possui um máximo e um mínimo globais finitos). 
    
    Portanto, existe uma constante máxima $K$ (o pico da derivada) tal que $|\mathcal{L}'(\hat{y})| \le K$ para todo $\hat{y}$. Isso classifica a MSLE como Lipschitz-contínua. Diferente do MAE (onde o gradiente trava em 1), na MSLE o gradiente diminui para erros muito grandes, o que pode ser visto como uma "super-robustez" ou um risco de o modelo parar de aprender se o erro inicial for absurdo.
\end{proof}

\subsection{Perda quantílica (quantile loss)}
\label{ap:deducoes-quantile-loss}

\begin{definition}
    Seja $e = \hat{y} - y$ o resíduo e $\tau \in (0, 1)$ o parâmetro do quantil desejado. A função de perda quantílica é definida como o máximo entre duas funções lineares:
    \begin{equation}
        \mathcal{L}_\tau(e) = \max(\tau e, (\tau - 1)e)
    \end{equation}
    Ou, de forma explícita por partes:
    \[
    \mathcal{L}_\tau(e) = \begin{cases} 
    \tau e & \text{se } e \ge 0 \\
    (\tau - 1)e & \text{se } e < 0 
    \end{cases}
    \]
\end{definition}

\begin{proposition}[Singularidade]
    A função Perda Quantílica pertence à classe $C^0$ (contínua), mas não pertence à classe $C^1$.
\end{proposition}

\begin{proof}
    A continuidade ($C^0$) segue da composição de funções contínuas (funções lineares e máximo). Para a diferenciabilidade ($C^1$), analisamos a derivada no ponto de inflexão $e=0$.
    
    Calculamos os limites laterais da derivada:
    \begin{align*}
        \lim_{e \to 0^+} \mathcal{L}'_\tau(e) &= \frac{d}{de}(\tau e) = \tau \\
        \lim_{e \to 0^-} \mathcal{L}'_\tau(e) &= \frac{d}{de}((\tau - 1)e) = \tau - 1
    \end{align*}
    
    Como $\tau \in (0, 1)$, temos que $\tau \neq \tau - 1$. Sendo os limites laterais distintos, a derivada não existe na origem. Logo, $\mathcal{L}_\tau \notin C^1$.
\end{proof}

\begin{proposition}[Convexidade]
    A função Perda Quantílica é convexa.
\end{proposition}

\begin{proof}
    Utilizamos a propriedade de preservação de convexidade em operações de máximo (Teorema \ref{thm:convex_tools}).
    
    Definimos duas funções auxiliares:
    \[ f_1(e) = \tau e \quad \text{e} \quad f_2(e) = (\tau - 1)e \]
    
    Ambas $f_1$ e $f_2$ são funções lineares (afins) e, portanto, convexas. Como a função de perda é definida por $\mathcal{L}_\tau(e) = \max(f_1(e), f_2(e))$, a convexidade é preservada.
\end{proof}

\begin{proposition}[Robustez via Subgradiente]
    A função Perda Quantílica é Lipschitz-contínua e robusta a \textit{outliers}.
\end{proposition}

\begin{proof}
    Analisamos o \textbf{Subdiferencial} na origem e as derivadas nas caudas. O subgradiente é dado por:
    \[
    \partial \mathcal{L}_\tau(e) = \begin{cases} 
    \{\tau - 1\} & \text{se } e < 0 \\
    [\tau - 1, \tau] & \text{se } e = 0 \\
    \{\tau\} & \text{se } e > 0 
    \end{cases}
    \]
    
    Para verificar a condição de Lipschitz, buscamos um limite superior para a magnitude da inclinação. Definimos $K = \max(\tau, 1-\tau)$. Como $\tau \in (0,1)$, $K < 1$.
    Observamos que:
    \[ |g| \le K < 1, \quad \forall g \in \partial \mathcal{L}_\tau(e) \]
    
    Como a inclinação é globalmente limitada, a função é Lipschitz-contínua. Isso garante robustez similar ao MAE, penalizando erros linearmente, mas com pesos assimétricos para superestimação e subestimação.
\end{proof}

\subsection{Perda epsilon-insensível (epsilon-insensitive)}
\label{ap:deducoes-epsilon-insensitive}

\begin{definition}
    Seja $\epsilon > 0$ um limiar de tolerância. A função de perda $\epsilon$-Insensitive ignora erros cuja magnitude seja menor que $\epsilon$, penalizando linearmente o excedente. É definida por:
    \begin{equation}
        \mathcal{L}_\epsilon(e) = \max(0, |e| - \epsilon)
    \end{equation}
    Ou, explicitamente por partes:
    \[
    \mathcal{L}_\epsilon(e) = \begin{cases} 
    0 & \text{se } |e| \le \epsilon \\
    |e| - \epsilon & \text{se } |e| > \epsilon 
    \end{cases}
    \]
\end{definition}

\begin{proposition}[Singularidade]
    A função $\epsilon$-Insensitive pertence à classe $C^0$ (contínua), mas não pertence à classe $C^1$.
\end{proposition}

\begin{proof}
    A função é contínua, pois é composta por funções contínuas (máximo e módulo). A falha na diferenciabilidade ocorre nos pontos de transição da zona morta: $e = \epsilon$ e $e = -\epsilon$.
    
    Analisamos os limites laterais da derivada no ponto $e = \epsilon$:
    \begin{align*}
        \lim_{e \to \epsilon^-} \mathcal{L}'_\epsilon(e) &= \frac{d}{de}(0) = 0 \\
        \lim_{e \to \epsilon^+} \mathcal{L}'_\epsilon(e) &= \frac{d}{de}(e - \epsilon) = 1
    \end{align*}
    
    Como os limites laterais são distintos ($0 \neq 1$), a derivada não existe nesses pontos. Logo, $\mathcal{L}_\epsilon \notin C^1$.
\end{proof}

\begin{proposition}[Convexidade]
    A função $\epsilon$-Insensitive é convexa (embora não estritamente convexa).
\end{proposition}

\begin{proof}
    Utilizamos o \textbf{Teorema de Convexidade de Máximos} (Seção \ref{sec:definicoes}).
    A função é definida como o máximo pontual entre duas funções:
    \[ f_1(e) = 0 \quad \text{e} \quad f_2(e) = |e| - \epsilon \]
    
    \begin{enumerate}
        \item $f_1(e) = 0$ é uma função constante, trivialmente convexa.
        \item $f_2(e) = |e| - \epsilon$ é uma função convexa, pois é a norma $L_1$ deslocada por uma constante.
    \end{enumerate}
    
    Como o máximo de funções convexas é convexo, $\mathcal{L}_\epsilon(e)$ é convexa em todo o domínio. Note que na região $|e| \le \epsilon$, a segunda derivada é nula, caracterizando convexidade não-estrita (fundo plano).
\end{proof}

\begin{proposition}[Robustez e Esparsidade]
    A função é Lipschitz-contínua (robusta) e induz esparsidade no gradiente.
\end{proposition}

\begin{proof}
    Analisamos o subdiferencial da função:
    \[
    \partial \mathcal{L}_\epsilon(e) = \begin{cases} 
    \{0\} & \text{se } |e| < \epsilon \quad (\text{Zona Morta}) \\
    [0, 1] & \text{se } e = \epsilon \\
    \{1\} & \text{se } e > \epsilon \\
    \dots & (\text{Simétrico para } e < 0)
    \end{cases}
    \]
    
    \textbf{1. Robustez:} O valor absoluto de qualquer subgradiente é limitado por 1 ($|g| \le 1$). Portanto, a função é 1-Lipschitz contínua, garantindo robustez a \textit{outliers}.
    
    \textbf{2. Esparsidade:} Diferente do MAE (cujo gradiente é quase sempre não-nulo), a $\epsilon$-Insensitive possui uma região de medida positiva ($[-\epsilon, \epsilon]$) onde o gradiente é exatamente zero. Isso significa que erros pequenos não provocam atualizações nos pesos, uma propriedade fundamental para a eficiência e generalização das Support Vector Machines (SVM).
\end{proof}

\subsection{Perda de Poisson (Poisson loss)}
\label{ap:deducoes-poisson-loss}

\begin{definition}
    Seja $y \in \mathbb{N}_0$ o alvo (número de contagens) e $\hat{y} \in \mathbb{R}^+$ a predição (taxa esperada). A função de perda é derivada da verossimilhança negativa da distribuição de Poisson. Descartando o termo $\log(y!)$ que independe do modelo, definimos:
    \begin{equation}
        \mathcal{L}(\hat{y}) = \hat{y} - y\log(\hat{y})
    \end{equation}
    Nota: O domínio da função é restrito a $\hat{y} > 0$.
\end{definition}

\begin{proposition}[Regularidade]
    A função Poisson Loss pertence à classe $C^\infty$ (suave) em seu domínio de definição $(0, \infty)$.
\end{proposition}

\begin{proof}
    A função é composta por uma função linear ($\hat{y}$) e uma função logarítmica ($y\log\hat{y}$). Sendo o logaritmo infinitamente diferenciável para argumentos estritamente positivos, a função herda essa propriedade.
    
    Calculamos as derivadas sucessivas em relação a $\hat{y}$:
    \begin{align*}
        \mathcal{L}'(\hat{y}) &= \frac{d}{d\hat{y}}(\hat{y} - y\log\hat{y}) = 1 - \frac{y}{\hat{y}} \\
        \mathcal{L}''(\hat{y}) &= \frac{d}{d\hat{y}}\left(1 - y\hat{y}^{-1}\right) = \frac{y}{\hat{y}^2} \\
        \mathcal{L}'''(\hat{y}) &= -2\frac{y}{\hat{y}^3}
    \end{align*}
    
    De forma geral, para $k \ge 2$, a derivada é proporcional a $\hat{y}^{-k}$. Como $\hat{y} \neq 0$ (por definição do domínio), todas as derivadas existem e são contínuas.
\end{proof}

\begin{proposition}[Convexidade]
    A função Poisson Loss é convexa (e estritamente convexa se $y > 0$).
\end{proposition}

\begin{proof}
    Utilizamos o Critério de Segunda Ordem (Teorema \ref{thm:segunda_ordem}). Analisamos o sinal da segunda derivada:
    \[ \mathcal{L}''(\hat{y}) = \frac{y}{\hat{y}^2} \]
    
    Considerando que o alvo $y$ é uma contagem não-negativa ($y \ge 0$) e o quadrado da predição é estritamente positivo ($\hat{y}^2 > 0$):
    \begin{itemize}
        \item Se $y=0$, $\mathcal{L}''(\hat{y}) = 0$ (Convexa).
        \item Se $y>0$, $\mathcal{L}''(\hat{y}) > 0$ (Estritamente Convexa).
    \end{itemize}
    
    Portanto, a função é globalmente convexa, garantindo a unicidade do ótimo durante o treinamento de Modelos Lineares Generalizados (GLM).
\end{proof}

\begin{proposition}[Não-Robustez na Origem]
    A função Poisson Loss não é Lipschitz-contínua globalmente devido à singularidade na origem.
\end{proposition}

\begin{proof}
    Para ser Lipschitz-contínua, o gradiente deve ser limitado. Analisamos o comportamento da derivada $\mathcal{L}'(\hat{y}) = 1 - \frac{y}{\hat{y}}$ nos limites do domínio.
    
    \begin{enumerate}
        \item \textbf{No infinito:} $\lim_{\hat{y} \to \infty} (1 - \frac{y}{\hat{y}}) = 1$. (Comportamento estável, similar ao MAE).
        \item \textbf{Na origem:} Supondo que existe um alvo real $y > 0$, analisamos o caso onde o modelo prevê um valor próximo de zero:
        \[ \lim_{\hat{y} \to 0^+} \left( 1 - \frac{y}{\hat{y}} \right) = -\infty \]
    \end{enumerate}
    
    Como o gradiente explode negativamente quando a predição se aproxima de zero (contradizendo a evidência $y$), não existe uma constante $K$ que limite a derivada. Portanto, a função não é Lipschitz-contínua. Isso exige cuidado numérico (como adicionar um $\epsilon$ de estabilidade ou usar a função de ativação Softplus) para evitar instabilidade numérica.
\end{proof}

\subsection{Perda Gamma (Gamma loss)}
\label{ap:deducoes-gamma-loss}

\begin{definition}
    Seja $y > 0$ o valor alvo e $\hat{y} > 0$ a predição. A função de perda Gamma deriva da \textit{Negative Log-Likelihood} da distribuição Gamma. Ignorando termos constantes que dependem apenas de $y$, a forma canônica para minimização é:
    \begin{equation}
        \mathcal{L}(y, \hat{y}) = \frac{y}{\hat{y}} + \log(\hat{y})
    \end{equation}
    O domínio é restrito estritamente aos reais positivos ($(0, \infty)$), pois a distribuição Gamma não admite valores nulos ou negativos.
\end{definition}

\begin{proposition}[Regularidade]
    A função Gamma Loss pertence à classe $C^\infty$ (suave) em seu domínio $(0, \infty)$.
\end{proposition}

\begin{proof}
    A função é composta pela soma de uma função racional ($y/\hat{y}$) e uma função logarítmica ($\log \hat{y}$). Ambas são $C^\infty$ em $\mathbb{R}^+$.
    
    Calculamos as derivadas sucessivas:
    \begin{align*}
        \mathcal{L}'(\hat{y}) &= -\frac{y}{\hat{y}^2} + \frac{1}{\hat{y}} = \frac{\hat{y} - y}{\hat{y}^2} \\
        \mathcal{L}''(\hat{y}) &= \frac{2y}{\hat{y}^3} - \frac{1}{\hat{y}^2} = \frac{2y - \hat{y}}{\hat{y}^3}
    \end{align*}
    
    Como o denominador $\hat{y}^k$ nunca é zero no domínio definido, todas as derivadas existem e são contínuas.
\end{proof}

\begin{proposition}[Não-Convexidade Global]
    A função Gamma Loss \textbf{não é convexa} em relação à predição $\hat{y}$ globalmente, tornando-se côncava para grandes superestimativas.
\end{proposition}

\begin{proof}
    Pelo critério da segunda derivada, para haver convexidade, exige-se $\mathcal{L}''(\hat{y}) \ge 0$ em todo o domínio. Retomando a expressão derivada anteriormente:
    \[ \mathcal{L}''(\hat{y}) = \frac{2y - \hat{y}}{\hat{y}^3} \]
    
    Como o denominador $\hat{y}^3$ é sempre positivo, o sinal depende do numerador:
    \begin{itemize}
        \item Se $\hat{y} < 2y$ (erro pequeno ou subestimação), $\mathcal{L}'' > 0$ (Convexa).
        \item Se $\hat{y} > 2y$ (erro grave de superestimação), $\mathcal{L}'' < 0$ (Côncava).
    \end{itemize}
    
    A mudança de curvatura quando a predição excede o dobro do alvo indica que a otimização direta em $\hat{y}$ pode ser instável para métodos de segunda ordem (como Newton-Raphson).
\end{proof}

\begin{remark}[Convexidade no Espaço Logarítmico]
    Em Deep Learning e GLMs, contornamos a não-convexidade modelando o logaritmo da média ($z = \log \hat{y}$). Substituindo $\hat{y} = e^z$, a função torna-se $G(z) = y e^{-z} + z$. Sua segunda derivada é $G''(z) = y e^{-z}$, que é estritamente positiva para todo $z$. Portanto, a Gamma Loss é canonicamente usada com uma função de ligação logarítmica.
\end{remark}

\begin{proposition}[Assimetria de Robustez]
    A função Gamma Loss não é Lipschitz-contínua devido à singularidade na origem, mas possui gradientes que tendem a zero no infinito.
\end{proposition}

\begin{proof}
    Analisamos o comportamento do gradiente $\mathcal{L}'(\hat{y}) = \frac{\hat{y} - y}{\hat{y}^2}$ nas bordas do domínio:
    
    \begin{enumerate}
        \item \textbf{Subestimação Extrema ($\hat{y} \to 0^+$):}
        \[ \lim_{\hat{y} \to 0^+} \left( \frac{1}{\hat{y}} - \frac{y}{\hat{y}^2} \right) = -\infty \]
        O gradiente explode negativamente (o termo quadrático no denominador domina), forçando o modelo a corrigir subestimações agressivamente. Não é Lipschitz.
        
        \item \textbf{Superestimação Extrema ($\hat{y} \to \infty$):}
        \[ \lim_{\hat{y} \to \infty} \frac{\hat{y} - y}{\hat{y}^2} \approx \lim_{\hat{y} \to \infty} \frac{1}{\hat{y}} = 0 \]
        Para predições muito altas, o gradiente desaparece.
    \end{enumerate}
    
    Conclui-se que a função penaliza severamente erros onde $\hat{y} \ll y$, mas é leniente (robusta) com erros onde $\hat{y} \gg y$.
\end{proof}

\subsection{Perda de Tweedie (Tweedie loss)}

\begin{definition}
    Seja $y \ge 0$ o alvo e $\hat{y} > 0$ a predição. A família de perdas Tweedie é indexada pelo parâmetro de potência de variância $p$. Para o caso de interesse em dados com excesso de zeros (Composta Poisson-Gamma), onde $1 < p < 2$, a função é definida por:
    \begin{equation}
        \mathcal{L}_p(y, \hat{y}) = -y \frac{\hat{y}^{1-p}}{1-p} + \frac{\hat{y}^{2-p}}{2-p}
    \end{equation}
    Esta fórmula deriva da \textit{Deviance} dos Modelos Lineares Generalizados (GLM) sob a suposição $\text{Var}(Y) \propto \mu^p$.
\end{definition}

\begin{proposition}[Regularidade]
    Para $p \in (1, 2)$, a função Tweedie Loss pertence à classe $C^\infty$ no domínio $\hat{y} \in (0, \infty)$.
\end{proposition}

\begin{proof}
    A função é uma combinação linear de funções potência ($\hat{y}^\alpha$). Como as potências $1-p$ e $2-p$ são números reais e o domínio restringe $\hat{y}$ a valores estritamente positivos, não ocorrem singularidades de divisão por zero ou raízes complexas.
    
    Calculamos a primeira derivada em relação a $\hat{y}$:
    \begin{align*}
        \mathcal{L}'(\hat{y}) &= -y \frac{(1-p)\hat{y}^{-p}}{1-p} + \frac{(2-p)\hat{y}^{1-p}}{2-p} \\
        &= -y\hat{y}^{-p} + \hat{y}^{1-p} \\
        &= \frac{\hat{y} - y}{\hat{y}^p}
    \end{align*}
    
    Observa-se que a derivada tem uma forma canônica elegante: o resíduo simples normalizado pela variância esperada ($\hat{y}^p$). Como $\hat{y} \neq 0$, todas as derivadas de ordem superior existem e são contínuas.
\end{proof}

\begin{proposition}[Não-Convexidade Global]
    Para $1 < p < 2$, a função Tweedie Loss \textbf{não é globalmente convexa} em relação a $\hat{y}$.
\end{proposition}

\begin{proof}
    Analisamos a segunda derivada:
    \[ \mathcal{L}''(\hat{y}) = \frac{d}{d\hat{y}} \left( \hat{y}^{1-p} - y\hat{y}^{-p} \right) \]
    \[ \mathcal{L}''(\hat{y}) = (1-p)\hat{y}^{-p} - (-p)y\hat{y}^{-p-1} \]
    \[ \mathcal{L}''(\hat{y}) = \hat{y}^{-p-1} \left[ (1-p)\hat{y} + py \right] \]
    
    Para garantir convexidade, precisamos que $\mathcal{L}''(\hat{y}) \ge 0$. Analisando o termo entre colchetes (já que $\hat{y}^{-p-1} > 0$):
    \[ py - (p-1)\hat{y} \ge 0 \implies \hat{y} \le \frac{p}{p-1}y \]
    
    Note que como $1 < p < 2$, o termo $(p-1)$ é positivo. A desigualdade mostra que a função é convexa apenas localmente (quando a predição não está muito longe do alvo). Se o modelo superestimar grosseiramente o valor ($\hat{y} > \frac{p}{p-1}y$), a função entra em uma região côncava.
\end{proof}

\begin{remark}
    Assim como na Perda Gamma, a não-convexidade é resolvida na prática modelando o logaritmo da média. A Tweedie Loss é estritamente convexa em relação a $z = \log(\hat{y})$.
\end{remark}

\begin{proposition}[Robustez Assimétrica]
    A função não é Lipschitz-contínua na origem, mas apresenta gradientes evanescentes no infinito.
\end{proposition}

\begin{proof}
    A robustez é determinada pelos limites da magnitude do gradiente $|\mathcal{L}'(\hat{y})| = |\hat{y}^{1-p} - y\hat{y}^{-p}|$.
    
    \begin{enumerate}
        \item \textbf{Limite na Origem ($\hat{y} \to 0^+$):}
        O termo dominante é $y\hat{y}^{-p}$. Como $p > 1$, $\hat{y}^{-p} \to \infty$.
        \[ \lim_{\hat{y} \to 0^+} |\mathcal{L}'(\hat{y})| = \infty \quad (\text{se } y > 0) \]
        Isso indica extrema sensibilidade a subestimativas (prever zero quando existe valor).
        
        \item \textbf{Limite no Infinito ($\hat{y} \to \infty$):}
        O termo dominante é $\hat{y}^{1-p}$. Como $1 < p < 2$, o expoente $1-p$ é negativo (entre $-1$ e $0$).
        \[ \lim_{\hat{y} \to \infty} |\mathcal{L}'(\hat{y})| = 0 \]
    \end{enumerate}
    
    A função exibe o comportamento clássico de perdas geradas por famílias exponenciais: pune severamente o erro "impossível" (prever 0 para alvo positivo) e é robusta/indulgente com erros de magnitude na cauda direita.
\end{proof}

\section{Perdas para classificação}