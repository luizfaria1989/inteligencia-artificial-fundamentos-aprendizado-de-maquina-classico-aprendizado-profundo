\chapter{Propriedades Analíticas das Funções de Perda}%
\label{apendice:propriedades-analiticas-funcoes-de-perda}

\section{Definições Preliminares e Notação}%
\label{sec:definicoes}

Para assegurar a precisão matemática necessária, harmonizamos as definições clássicas de Análise Real com a notação moderna de Aprendizado Profundo.

\subsection{Classes de Diferenciabilidade}

Embora a literatura clássica, como \textcite{rudin1976principles}, utilize a notação $\mathscr{C}'$ e $\mathscr{C}''$ para denotar espaços de funções continuamente diferenciáveis, adotamos a notação moderna $C^k$, comum em otimização, para facilitar a leitura.

\begin{definition}[Classes $C^k$]
    Baseado em Rudin [Definições 9.20 e 9.39], dizemos que uma função $f: U \subset \mathbb{R}^n \to \mathbb{R}$ é de classe $C^k$ se suas derivadas parciais até a ordem $k$ existem e são contínuas em $U$.
    \begin{itemize}
        \item $C^0$: Funções contínuas.
        \item $C^1$: Funções com gradiente contínuo (Rudin $\mathscr{C}'$).
        \item $C^2$: Funções com Hessiana contínua (Rudin $\mathscr{C}''$).
        \item $C^\infty$: Funções infinitamente diferenciáveis (Suaves).
    \end{itemize}
\end{definition}

\subsection{Robustez e Continuidade de Lipschitz}

Em Deep Learning, a estabilidade do treinamento está ligada à limitação da taxa de variação da função de perda. Adotamos a definição vetorial encontrada em \textcite{DeepLearningBook}, adaptando a notação da constante para evitar ambiguidade com a função de perda $\Loss$.

\begin{definition}[Continuidade de Lipschitz]
    Uma função $f: \mathbb{R}^n \to \mathbb{R}$ é dita \textbf{Lipschitz-contínua} se existe uma constante $K \in \mathbb{R}^+$ (chamada constante de Lipschitz) tal que, para todos $\boldsymbol{x}, \boldsymbol{y}$ no domínio:
    \begin{equation}
        |f(\boldsymbol{x}) - f(\boldsymbol{y})| \le K \|\boldsymbol{x} - \boldsymbol{y}\|_2
    \end{equation}
    Onde $\|\cdot\|_2$ denota a norma Euclidiana ($L^2$).
\end{definition}

\begin{remark}[Interpretação Escalar]
    Nas demonstrações deste apêndice, onde analisamos o erro $e \in \mathbb{R}$ (escalar), a norma Euclidiana reduz-se ao valor absoluto: $\|\boldsymbol{x} - \boldsymbol{y}\|_2 \to |x - y|$. A condição torna-se $|f(x) - f(y)| \le K |x - y|$.
\end{remark}

\subsection{Critérios de Verificação de Convexidade}

Para demonstrar a convexidade das funções de perda sem recorrer exaustivamente à definição geométrica, utilizaremos três teoremas fundamentais de operações que preservam a convexidade, conforme estabelecido em \textcite{boyd2004convex}\footnote{Apresentamos aqui as versões escalares dos teoremas de convexidade de \textcite{boyd2004convex}, simplificadas para o contexto de funções de perda unidimensionais. Para a formulação geral vetorial envolvendo Matrizes Hessianas, consulte o texto original, Capítulo 3.}.

\begin{theorem}[Condição de Segunda Ordem]
    Seja $f$ uma função duas vezes diferenciável. $f$ é convexa se, e somente se, sua segunda derivada for não-negativa em todo o domínio:
    \[ f''(x) \ge 0, \quad \forall x \in \text{dom } f \]
\end{theorem}

\begin{theorem}[Preservação por Composição Afim]
    Se $f: \mathbb{R} \to \mathbb{R}$ é convexa, então a composição $g(x) = f(ax + b)$ é convexa para quaisquer escalares $a, b$. Isso garante que provar a convexidade em relação ao erro $e$ implica convexidade em relação à predição $\hat{y}$.
\end{theorem}

\begin{theorem}[Convexidade de Normas e Máximos]
    \begin{enumerate}
        \item Se $f$ e $g$ são funções convexas, então $h(x) = \max(f(x), g(x))$ é convexa.
        \item Qualquer norma $\|\cdot\|$ é convexa, consequência direta da Desigualdade Triangular: $\|x+y\| \le \|x\| + \|y\|$.
    \end{enumerate}
\end{theorem}

\section{Perdas para regressão}

\subsection{Erro quadrático médio (MSE)}%
\label{ap:deducoes-mse}

\begin{definition}
    Seja $e = \hat{y} - y$ o resíduo entre a predição e o alvo. A função de perda erro quadrático médio é definida por $\Loss(e) = e^2$.
\end{definition}

\begin{proposition}[Regularidade]
    A função erro quadrático médio pertence à classe $C^{\infty}$ (é infinitamente diferenciável).
\end{proposition}

\begin{proof}
    Para provar que $\Loss \in C^{\infty}$ devemos demonstrar que existem derivadas contínuas de todas as ordens $k \in \mathbb{N}$.

    Calculamos as derivadas sucessivas de $\Loss(e)$ em relação a $e$:

    \begin{align*}
        \Loss'(e) &= \frac{d}{de} (e^2) = 2e \\
        \Loss''(e) &= \frac{d}{de} (2e) = 2 \\
        \Loss^{(k)}(e) &= 0, \quad \forall k \ge 3
    \end{align*}

    Analisamos a continuidade das funções resultantes:

    \begin{itemize}
        \item $\Loss'(e) = 2e$, é polinômio linear, contínuo em todo $\mathbb{R}$.
        \item $\Loss''(e) = 2$ e $\mathcal{L}^{(k)}(e) = 0$ são funções constantes, contínuas em todo $\mathbb{R}$.
    \end{itemize}

    Como $\Loss^{(k)}$ existe e é contínua para todo $k \ge 1$, conclui-se que $\mathcal{L} \in C^{\infty}$.
\end{proof}

\begin{proposition}[Convexidade]
    A função erro quadrático médio é estritamente convexa.
\end{proposition}

\begin{proof}
    Utilizamos o critério da segunda derivada. Uma função duas vezes diferenciável é estritamente convexa se, e somente se, sua segunda derivada for estritamente positiva em todo o domínio.
    
    Da proposição anterior, temos:
    \[ \Loss''(e) = 2 \]

    Observamos que $2 > 0$ para todo $e \in \mathbb{R}$. Portanto, a função possui curvatura positiva constante, satisfazendo a condição de convexidade estrita.
\end{proof}

\begin{proposition}[Robustez]
    A função erro quadrático médio não é Lipschitz-contínua (não é robusta a \textit{outliers})
\end{proposition}

\begin{proof}
    Uma função diferenciável é Lipschitz-contínua se existir uma constante $K \in \mathbb{R}$ tal que $\sup_{e} |\mathcal{L}'(e)| \le K$.
    
    Analisamos o comportamento assintótico da magnitude do gradiente:
    \[ \lim_{e \to \infty} |\Loss'(e)| = \lim_{e \to \infty} |2e| = \infty \]

    Como o gradiente cresce indefinidamente à medida que o erro aumenta, não existe uma constante $K$ que limite a derivada superiormente. Logo, a função não é Lipschitz-contínua.
\end{proof}

\subsection{Erro absoluto médio (MAE)}%
\label{ap:deducoes-mae}

\begin{definition}
    Seja $e = \hat{y} - y$ o resíduo entre a predição e o alvo. A função de perda erro absoluto médio é definida por $\Loss(e) = |e|$.
\end{definition}

\begin{proposition}[Regularidade]
    A função erro absoluto médio pertence à classe $C^0$ (contínua), mas não pertence à classe $C^1$.
\end{proposition}

\begin{proof}
    A função módulo é contínua por definição, logo $\Loss \in C^0$. Para verificar se é $C^1$, analisamos a derivada no ponto $e=0$.
    Calculamos os limites laterais da derivada:
    \begin{align*}
        \lim_{e \to 0^+} \frac{d}{de}(e) &= 1 \\
        \lim_{e \to 0^-} \frac{d}{de}(-e) &= -1
    \end{align*}
    Como os limites laterais são distintos ($1 \neq -1$), a derivada clássica não existe em $e=0$. Portanto, a função não é continuamente diferenciável em todo o domínio ($\mathcal{L} \notin C^1$).
\end{proof}

\begin{proposition}[Convexidade]
    A função Erro Absoluto Médio é convexa.
\end{proposition}

\begin{proof}
    A função é definida por $\mathcal{L}(e) = |e|$, que constitui a norma $L_1$ no espaço unidimensional ($\mathbb{R}$).
    
    Conforme estabelecido no \textbf{Teorema de Convexidade de Normas} (Seção \ref{sec:definicoes}), toda norma é necessariamente uma função convexa. Portanto, a convexidade do MAE segue diretamente dessa propriedade, sem necessidade de demonstração algébrica adicional.
\end{proof}

\begin{proposition}[Robustez]
    A função erro absoluto médio é Lipschitz-contínua (é robusta a \textit{outliers}).
\end{proposition}

\begin{proof}
    Como a função não é diferenciável na origem, analisamos o seu subdiferencial $\partial \mathcal{L}(e)$. O subgradiente do valor absoluto é dado por:
    \[
    \partial |e| = \begin{cases} 
    \{-1\} & \text{se } e < 0 \\
    [-1, 1] & \text{se } e = 0 \\
    \{1\} & \text{se } e > 0 
    \end{cases}
    \]
    Para provar a robustez (Lipschitz), verificamos se o subgradiente é limitado. Observamos que para qualquer $g \in \partial \mathcal{L}(e)$:
    \[ |g| \le 1, \quad \forall e \in \mathbb{R} \]
    Como a magnitude da ``inclinação'' generalizada nunca excede 1, a função é 1-Lipschitz contínua.
\end{proof}

\subsection{Perda de Huber (Huber loss)}%
\label{ap:deducoes-huber-loss}

\begin{definition}
    Seja $\delta > 0$ um hiperparâmetro de transição. A função de perda de Huber é definida por partes:
    \begin{equation}
        \mathcal{L}_\delta(e) = \begin{cases} 
        \frac{1}{2}e^2 & \text{se } |e| \le \delta \\
        \delta(|e| - \frac{1}{2}\delta) & \text{se } |e| > \delta 
        \end{cases}
    \end{equation}
\end{definition}

\begin{proposition}[Regularidade e Colagem Suave]
    A função de Huber pertence à classe $C^1$ (continuamente diferenciável), mas não pertence à classe $C^2$.
\end{proposition}

\begin{proof}
    A função é composta por polinômios ($C^\infty$) em cada região aberta. A regularidade global depende do comportamento nos pontos de transição $e = \pm \delta$.
    
    Calculamos a primeira derivada:
    \[
    \mathcal{L}'(e) = \begin{cases} 
    e & \text{se } |e| \le \delta \\
    \delta \cdot \text{sgn}(e) & \text{se } |e| > \delta 
    \end{cases}
    \]
    Verificamos a continuidade da derivada em $e = \delta$:
    \begin{align*}
        \lim_{e \to \delta^-} \mathcal{L}'(e) &= \delta \\
        \lim_{e \to \delta^+} \mathcal{L}'(e) &= \delta \cdot 1 = \delta
    \end{align*}
    Como os limites coincidem, $\mathcal{L}'$ é contínua em $\delta$ (e analogamente em $-\delta$). Logo, $\mathcal{L} \in C^1$.
    
    Para a segunda derivada:
    \[
    \mathcal{L}''(e) = \begin{cases} 
    1 & \text{se } |e| < \delta \\
    0 & \text{se } |e| > \delta 
    \end{cases}
    \]
    Observa-se uma descontinuidade de salto nos pontos $\pm \delta$ (de 1 para 0). Portanto, a segunda derivada não é contínua, implicando $\mathcal{L} \notin C^2$.
\end{proof}

\begin{proposition}[Convexidade]
    A função de Huber é convexa em todo o domínio $\mathbb{R}$.
\end{proposition}

\begin{proof}
    Utilizamos o \textbf{Critério de Segunda Ordem} (Teorema \ref{thm:segunda_ordem}). Analisamos o sinal da segunda derivada calculada anteriormente:
    \begin{itemize}
        \item Na região quadrática ($|e| < \delta$): $\mathcal{L}''(e) = 1 > 0$.
        \item Na região linear ($|e| > \delta$): $\mathcal{L}''(e) = 0 \ge 0$.
    \end{itemize}
    Como $\mathcal{L}''(e) \ge 0$ em todos os pontos onde é definida (e a função é $C^1$), conclui-se que a função possui curvatura não-negativa globalmente, satisfazendo a condição de convexidade.
\end{proof}

\begin{proposition}[Robustez]
    A função de Huber é Lipschitz-contínua com constante $K=\delta$, sendo robusta a \textit{outliers}.
\end{proposition}

\begin{proof}
    Analisamos a magnitude do gradiente globalmente. A derivada primeira é dada pela função de corte (\textit{clip}):
    \[ \mathcal{L}'(e) = \max(-\delta, \min(e, \delta)) \]
    
    Portanto, o valor absoluto da derivada é limitado superiormente pelo hiperparâmetro $\delta$:
    \[ |\mathcal{L}'(e)| \le \delta, \quad \forall e \in \mathbb{R} \]
    
    Pelo critério do gradiente limitado (Seção \ref{sec:definicoes}), a função é Lipschitz-contínua. Diferente do MSE, onde o gradiente cresce indefinidamente, a Huber Loss satura a força da penalidade em $\delta$, garantindo estabilidade frente a grandes erros.
\end{proof}

\subsection{Perda log-cosh (log-cosh loss)}%
\label{ap:deducoes-log-cosh}

\begin{definition}
    Seja $e = \hat{y} - y$ o resíduo. A função de perda Log-Cosh é definida pelo logaritmo natural do cosseno hiperbólico do erro:
    \begin{equation}
        \mathcal{L}(e) = \log(\cosh(e))
    \end{equation}
\end{definition}

\begin{proposition}[Regularidade e Suavidade]
    A função Log-Cosh pertence à classe $C^\infty$ (é infinitamente diferenciável em todo o domínio).
\end{proposition}

\begin{proof}
    A função é uma composição de funções elementares: $f(x) = \log(x)$ e $g(e) = \cosh(e)$.
    Sabemos que $\cosh(e)$ é $C^\infty$ em $\mathbb{R}$ e que $\cosh(e) \ge 1$ para todo $e$. Como o logaritmo é $C^\infty$ em $(0, \infty)$, a composição $\log(\cosh(e))$ preserva a suavidade.
    
    Verificamos as primeiras derivadas:
    \begin{align*}
        \mathcal{L}'(e) &= \frac{d}{de} \log(\cosh(e)) = \frac{1}{\cosh(e)} \cdot \sinh(e) = \tanh(e) \\
        \mathcal{L}''(e) &= \frac{d}{de} \tanh(e) = \text{sech}^2(e)
    \end{align*}
    Como $\tanh(e)$ e $\text{sech}(e)$ são funções suaves compostas por exponenciais, todas as derivadas de ordem superior existem e são contínuas. Portanto, $\mathcal{L} \in C^\infty$.
\end{proof}

\begin{proposition}[Convexidade]
    A função Log-Cosh é estritamente convexa.
\end{proposition}

\begin{proof}
    Utilizamos o critério da segunda derivada. Da demonstração anterior, temos:
    \[ \Loss''(e) = \text{sech}^2(e) = \frac{1}{\cosh^2(e)} \]
    
    Sabemos que $\cosh(e) \ge 1$ para todo número real, o que implica $1 \le \cosh^2(e) < \infty$. Consequentemente, o inverso quadrado é estritamente positivo:
    \[ 0 < \frac{1}{\cosh^2(e)} \le 1 \]
    
    Como $\mathcal{L}''(e) > 0$ em todo $\mathbb{R}$, a função é estritamente convexa.
\end{proof}

\begin{proposition}[Robustez]
    A função Log-Cosh é Lipschitz-contínua e robusta a \textit{outliers}.
\end{proposition}

\begin{proof}
    Analisamos a magnitude do gradiente $\mathcal{L}'(e) = \tanh(e)$.
    Sabemos pelas propriedades da tangente hiperbólica que sua imagem é o intervalo aberto $(-1, 1)$. Portanto:
    \[ |\mathcal{L}'(e)| = |\tanh(e)| < 1, \quad \forall e \in \mathbb{R} \]
    
    Como a derivada é limitada globalmente por $K=1$, a função é 1-Lipschitz contínua. 
    
    \textit{Observação:} Note que assintoticamente, quando $|e| \to \infty$, a derivada se aproxima de $\pm 1$ (comportamento do MAE), mas na origem ela se aproxima de $0$ linearmente (comportamento do MSE), combinando as melhores propriedades de ambas sem perder a diferenciabilidade.
\end{proof}

\subsection{Erro quadrático logarítmico médio (MSLE)}
\label{ap:deducoes-msle}

\begin{definition}
    Seja $y \ge 0$ o valor alvo e $\hat{y} \ge 0$ a predição (com a restrição de não-negatividade usual para dados log-normais). A função MSLE é definida por:
    \begin{equation}
        \mathcal{L}(y, \hat{y}) = (\log(1 + y) - \log(1 + \hat{y}))^2
    \end{equation}
    Para simplificar a notação nas demonstrações, definimos o termo constante $C = \log(1+y)$ e analisamos a função em relação à variável $\hat{y}$.
\end{definition}

\begin{proposition}[Regularidade]
    A função MSLE pertence à classe $C^{\infty}$ no domínio $\hat{y} > -1$.
\end{proposition}

\begin{proof}
    A função é uma composição de funções elementares: a função logarítmica (suave em $\mathbb{R}^+$), a função afim $1+\hat{y}$ e a função quadrática.
    
    Calculamos as derivadas em relação a $\hat{y}$:
    \begin{align*}
        \mathcal{L}'(\hat{y}) &= 2(\log(1+y) - \log(1+\hat{y})) \cdot \frac{d}{d\hat{y}}(-\log(1+\hat{y})) \\
        &= -2 \frac{\log(1+y) - \log(1+\hat{y})}{1+\hat{y}}
    \end{align*}
    
    Como o denominador $(1+\hat{y})$ nunca é zero para $\hat{y} \ge 0$ e o logaritmo é infinitamente diferenciável nesse domínio, todas as derivadas de ordem superior existem e são contínuas.
\end{proof}

\begin{proposition}[Convexidade no Espaço Logarítmico]
    A função MSLE é convexa em relação à predição transformada $z = \log(1+\hat{y})$, mas não é globalmente convexa em relação a $\hat{y}$.
\end{proposition}

\begin{proof}
    A análise de convexidade direta em $\hat{y}$ revela que $\mathcal{L}''(\hat{y})$ pode assumir valores negativos para $\hat{y}$ muito grandes, violando a convexidade global no espaço original.
    
    No entanto, se considerarmos a transformação do espaço de saída para a escala logarítmica, definindo $z = \log(1+\hat{y})$ e $c = \log(1+y)$, a função se torna:
    \[ G(z) = (c - z)^2 \]
    
    A derivada segunda em relação a $z$ é:
    \[ G''(z) = 2 > 0 \]
    
    Portanto, a MSLE é estritamente convexa \textit{no espaço logarítmico}. Isso justifica seu uso em problemas onde a magnitude das variáveis varia exponencialmente, pois a otimização se comporta como um MSE sobre os expoentes.
\end{proof}

\begin{proposition}[Robustez e Continuidade de Lipschitz]
    A função MSLE é Lipschitz-contínua (com constante $K > 0$) e seu gradiente tende a zero para erros extremos.
\end{proposition}

\begin{proof}
    Para provar que a função é Lipschitz, precisamos mostrar que a magnitude da derivada é limitada superiormente em todo o domínio ($\sup |\mathcal{L}'| < \infty$).
    
    A derivada é dada por:
    \[ \mathcal{L}'(\hat{y}) = -2 \frac{\log(1+y) - \log(1+\hat{y})}{1+\hat{y}} \]
    
    Observamos que:
    \begin{enumerate}
        \item A função $\mathcal{L}'$ é contínua no intervalo $[0, \infty)$.
        \item No limite $\hat{y} \to \infty$, a derivada tende a 0.
        \item No ponto $\hat{y} = y$ (erro zero), a derivada é 0.
    \end{enumerate}
    
    Pelo Teorema de Weierstrass generalizado, uma função contínua que tende a zero no infinito é necessariamente limitada (possui um máximo e um mínimo globais finitos). 
    
    Portanto, existe uma constante máxima $K$ (o pico da derivada) tal que $|\mathcal{L}'(\hat{y})| \le K$ para todo $\hat{y}$. Isso classifica a MSLE como Lipschitz-contínua. Diferente do MAE (onde o gradiente trava em 1), na MSLE o gradiente diminui para erros muito grandes, o que pode ser visto como uma "super-robustez" ou um risco de o modelo parar de aprender se o erro inicial for absurdo.
\end{proof}

\subsection{Perda quantílica (quantile loss)}
\label{ap:deducoes-quantile-loss}

\begin{definition}
    Seja $e = \hat{y} - y$ o resíduo e $\tau \in (0, 1)$ o parâmetro do quantil desejado. A função de perda quantílica é definida como o máximo entre duas funções lineares:
    \begin{equation}
        \mathcal{L}_\tau(e) = \max(\tau e, (\tau - 1)e)
    \end{equation}
    Ou, de forma explícita por partes:
    \[
    \mathcal{L}_\tau(e) = \begin{cases} 
    \tau e & \text{se } e \ge 0 \\
    (\tau - 1)e & \text{se } e < 0 
    \end{cases}
    \]
\end{definition}

\begin{proposition}[Singularidade]
    A função Perda Quantílica pertence à classe $C^0$ (contínua), mas não pertence à classe $C^1$.
\end{proposition}

\begin{proof}
    A continuidade ($C^0$) segue da composição de funções contínuas (funções lineares e máximo). Para a diferenciabilidade ($C^1$), analisamos a derivada no ponto de inflexão $e=0$.
    
    Calculamos os limites laterais da derivada:
    \begin{align*}
        \lim_{e \to 0^+} \mathcal{L}'_\tau(e) &= \frac{d}{de}(\tau e) = \tau \\
        \lim_{e \to 0^-} \mathcal{L}'_\tau(e) &= \frac{d}{de}((\tau - 1)e) = \tau - 1
    \end{align*}
    
    Como $\tau \in (0, 1)$, temos que $\tau \neq \tau - 1$. Sendo os limites laterais distintos, a derivada não existe na origem. Logo, $\mathcal{L}_\tau \notin C^1$.
\end{proof}

\begin{proposition}[Convexidade]
    A função Perda Quantílica é convexa.
\end{proposition}

\begin{proof}
    Utilizamos a propriedade de preservação de convexidade em operações de máximo (Teorema \ref{thm:convex_tools}).
    
    Definimos duas funções auxiliares:
    \[ f_1(e) = \tau e \quad \text{e} \quad f_2(e) = (\tau - 1)e \]
    
    Ambas $f_1$ e $f_2$ são funções lineares (afins) e, portanto, convexas. Como a função de perda é definida por $\mathcal{L}_\tau(e) = \max(f_1(e), f_2(e))$, a convexidade é preservada.
\end{proof}

\begin{proposition}[Robustez via Subgradiente]
    A função Perda Quantílica é Lipschitz-contínua e robusta a \textit{outliers}.
\end{proposition}

\begin{proof}
    Analisamos o \textbf{Subdiferencial} na origem e as derivadas nas caudas. O subgradiente é dado por:
    \[
    \partial \mathcal{L}_\tau(e) = \begin{cases} 
    \{\tau - 1\} & \text{se } e < 0 \\
    [\tau - 1, \tau] & \text{se } e = 0 \\
    \{\tau\} & \text{se } e > 0 
    \end{cases}
    \]
    
    Para verificar a condição de Lipschitz, buscamos um limite superior para a magnitude da inclinação. Definimos $K = \max(\tau, 1-\tau)$. Como $\tau \in (0,1)$, $K < 1$.
    Observamos que:
    \[ |g| \le K < 1, \quad \forall g \in \partial \mathcal{L}_\tau(e) \]
    
    Como a inclinação é globalmente limitada, a função é Lipschitz-contínua. Isso garante robustez similar ao MAE, penalizando erros linearmente, mas com pesos assimétricos para superestimação e subestimação.
\end{proof}

\subsection{Perda epsilon-insensível (epsilon-insensitive)}

\subsection{Perda de Poisson (Poisson loss)}

\subsection{Perda Gamma (Gamma loss)}

\subsection{Perda de Tweedie (Tweedie loss)}

\section{Perdas para classificação}