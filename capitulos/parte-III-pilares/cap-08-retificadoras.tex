% ===================================================================
% Arquivo: capitulos/parte-III-pilares/cap-08-retificadoras.tex
% ===================================================================

\chapter{Funções de Ativação Retificadoras}
\label{cap:ativacao-retificadoras}

% ===================================================================
% Resumo do capítulo
% ===================================================================

\section{Exemplo Ilustrativo}

% ===================================================================
% ReLU
% ===================================================================

\section{Rectified Linear Unit e Revolução Retificadora}

\begin{equacaodestaque}{Rectified Linear Unit (ReLU)}
    \text{ReLU}(z_i) = \begin{cases}z_i, & \text{se } z_i > 0 \\0, & \text{se } z_i \leq 0\end{cases}
    \label{eq:relu}
\end{equacaodestaque}

\begin{equacaodestaque}{Derivada Rectified Linear Unit (ReLU)}
    \frac{d}{dz_i} [ReLU](z_i) = \begin{cases}1, & \text{se } z_i > 0 \\0, & \text{se } z_i \leqslant 0 \end{cases}
    \label{eq:relu-derivada}
\end{equacaodestaque}

\subsection{Implementação em Python}

\begin{codelisting}{Classe completa do função de ativação Rectified Linear Unit}{gd_class}
import numpy as np
from layers.base import Layer

class ReLU(Layer):
    def __init__(self):
        super().__init__()
        self.input = None

    def forward(self, input_data):
        self.input = input_data
        return np.maximum(0, self.input)

    def backward(self, grad_output):
        relu_grad = (self.input > 0)

        # Apply the chain rule
        return grad_output * relu_grad, None
\end{codelisting}

\section{Dying ReLUs Problem}

\section{Corrigindo o Dying ReLUs Problem: As Variantes com Vazamento}

\subsection{Leaky ReLU}

\begin{equacaodestaque}{Leaky ReLU (LReLU)}
    \text{LReLU}(z_i) = \begin{cases}z_i, & \text{se } z_i \ge 0 \\ \alpha \cdot z_i, & \text{se } z_i < 0\end{cases}
    \label{eq:leaky-relu}
\end{equacaodestaque}

\begin{equacaodestaque}{Derivada Leaky ReLU (LReLU)}
    \frac{d}{dz_i} [LReLU](z_i) = \begin{cases}1, & \text{se } z_i > 0 \\ \alpha, & \text{se } z_i \leqslant  0 \end{cases}
    \label{eq:leaky-relu-derivada}
\end{equacaodestaque}

\subsubsection{Implementação em Python}

\begin{codelisting}{Classe completa do função de ativação Leaky ReLU}{gd_class}
import numpy as np
from layers.base import Layer


class LeakyReLU(Layer):
    def __init__(self, alpha=0.01):
        super().__init__()
        self.input = None
        self.alpha = alpha

    def forward(self, input_data):
        self.input = input_data
        return np.maximum(self.input * self.alpha, self.input)

    def backward(self, grad_output):
        leaky_relu_grad = np.where(self.input > 0, 1, self.alpha)
        return grad_output * leaky_relu_grad, None
\end{codelisting}

\subsection{Parametric ReLU}

\begin{equacaodestaque}{Parametric ReLU (PReLU)}
    \text{PReLU}(z_i) = \begin{cases}z_i, & \text{se } z_i \ge 0 \\ \alpha_i \cdot z_i, & \text{se } z_i < 0\end{cases}
    \label{eq:prelu}
\end{equacaodestaque}

\begin{equacaodestaque}{Derivada Parametric ReLU (PReLU)}
    \frac{d}{dz_i} [PReLU](z_i) = \begin{cases}1, & \text{se } z_i > 0 \\ \alpha_i, & \text{se } z_i < 0 \\ \nexists, & \text{se } z_i = 0\end{cases}
    \label{eq:prelu-derivada}
\end{equacaodestaque}

\subsection{Randomized Leaky ReLU}

\begin{equacaodestaque}{Randomized Leaky ReLU (RReLU)}
    \text{RReLU}(z_i) = \begin{cases} z_i, & \text{se } z_i > 0 \\ \alpha_i z_i, & \text{se } z_i \leq 0 \end{cases}
    \label{eq:rrelu}
\end{equacaodestaque}

\begin{equacaodestaque}{Derivada Randomized Leaky ReLU (RReLU)}
    \frac{d}{dz_i} [RReLU](z_i) = \begin{cases}1, & \text{se } z_i > 0 \\ \alpha_i, & \text{se } z_i \leqslant  0 \end{cases}
    \label{eq:rrelu-derivada}
\end{equacaodestaque}

\section{Em Busca da Suavidade}

\subsection{Exponential Linear Unit}

\begin{equacaodestaque}{Exponential Linear Unit (ELU)}
    \text{ELU}(z_i) = \begin{cases}z_i, & \text{se } z_i \ge 0 \\ \alpha \cdot (e^{z_i} - 1), & \text{se } z_i < 0\end{cases}
    \label{eq:elu}
\end{equacaodestaque}

\begin{equacaodestaque}{Derivada Exponential Linear Unit (ELU)}
    \frac{d}{dz_i} [ELU](z_i) = \begin{cases}1, & \text{se } z_i > 0 \\ \alpha \cdot e^{z_i}, & \text{se } z_i \le 0 \end{cases}
    \label{eq:elu-derivada}
\end{equacaodestaque}

\subsection{Scaled Exponential Linear Unit}

\begin{equacaodestaque}{Scaled Exponential Linear Unit (ELU)}
    \text{SELU}(z_i) = \lambda \begin{cases}z_i, & \text{se } z_i > 0 \\ \alpha \cdot (e^{z_i} - 1), & \text{se } z_i \le 0\end{cases}
    \label{eq:selu}
\end{equacaodestaque}

\begin{equacaodestaque}{Derivada Scaled Exponential Linear Unit (ELU)}
    \frac{d}{dz_i} [SELU](z_i) = \lambda \begin{cases}1, & \text{se } z_i > 0 \\ \alpha \cdot e^{z_i}, & \text{se } z_i \le 0\end{cases}
    \label{eq:selu-derivada}
\end{equacaodestaque}

\subsection{Noisy ReLU}

\begin{equacaodestaque}{Noisy ReLU (NReLU)}
    \text{NReLU}(z_i) = \begin{cases} 
    0 & \text{se } z_i \le 0 \\ 
    z_i + \mathcal{N} (0, \sigma(z_i)) & \text{se } z_i > 0 
    \end{cases}
    \label{eq:nrelu}
\end{equacaodestaque}

\begin{equacaodestaque}{Derivada Noisy ReLU (NReLU)}
    \frac{d}{dz_i}[\text{NReLU}](z_i) = \begin{cases} 
    0 & \text{se } z_i \le 0 \\ 
    1 & \text{se } z_i > 0 
    \end{cases}
    \label{eq:nrelu-derivada}
\end{equacaodestaque}

\section{O Problema dos Gradientes Explosivos}

\section{Comparativo de Desempenho das Funções Retificadoras}