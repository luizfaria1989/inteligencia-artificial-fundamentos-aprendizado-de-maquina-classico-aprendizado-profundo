% ===================================================================
% Arquivo: capitulos/parte-III-pilares/cap-10-perda-binaria.tex
% ===================================================================

\chapter{Funções de perda para regressão}%
\label{cap:perda-regressao}

Agora agora vimos o funcionamento da retropropagação, e como ela faz uso dos otimizadores, os quais funcionam como um barco, percorrendo as ondas em busca dos pontos de mínimo. Além disso, foram vistas em seguida diversas funções de ativação, começando pelas sigmoidais. Depois pelas retificadoras, e por fim uma coletânea de diferentes funções. Contudo, está na hora de entender um outro lado da retropropagação: as funções de perda, as quais são justamente as ondas que os otimizadores percorrem.

Esse capítulo explica alguns dos diferentes tipos de funções de perda, mais precisamente as funções de perda para tarefas de regressão. Portanto, o capítulo pode ser dividido em quatro grandes partes: funções de perda para propósitos gerais, funções de perda para medir o erro relativo, funções que vão além do cálculo da média dos erros. Por último, funções que são utilizadas para problemas que seguem outros tipos de distribuição (como as distribuições de Poisson e Gamma).

Para explicar cada uma das funções é apresentado as suas equações, os gráficos (contendo as vistas em duas e três dimensões), as derivadas parciais junto com os seus respectivos gráficos. Além disso, ao final de cada explicação dessas funções, é selecionado uma série de artigos que exploram o uso dessas funções para resolver problemas variados em aprendizado de máquina. Já no final do capítulo, está uma tabela resumo, explicando as principais características das funções e seus usos. Também é dedicada uma seção que apresenta um diagrama, que serve de guia para escolher a função de perda ideal para um problema de regressão.

\section{Exemplo ilustrativo: jogando dardos}

Pense que você está jogando dardos com seus amigos, e, quer decidir quem está com mais pontos. Mas, você não está satisfeito em considerar as marcações que estão no jogo, e, decidiu inovar. Assim, você pegou uma régua e passou a medir a distância que os dardos que você e seus amigos haviam jogado no centro. Quem chegasse mais próximo do centro, ganhava o jogo.

Essa ideia de medir o quão próximo você está do resultado desejado utilizando a distância entre esses dois pontos como parâmetro, é o motivador pela criação das funções de perda para regressão. Essas funções utilizam diferentes fórmulas, com todas com o mesmo intuito, medir a distância em que a predição dado pelo modelo está do ponto real (desejado).

\section{Características das funções de perda}

Antes de discutir as funções de perda para tarefas de regressão, é importante citar as principais propriedades que uma função de perda, seja ela para tarefas de regressão ou para outros tipos de tarefa. Em \textit{Loss Functions and Metrics in Deep Learning}, \textcite{LossesArticle} explicam algumas características desse grupo de funções. Os autores argumentam que algumas propriedades das funções de perda são: \textbf{convexidade}, \textbf{diferenciabilidade}, \textbf{robustez}, \textbf{suavidade}, \textbf{esparsidade}, \textbf{monotonicidade} \parencite{LossesArticle}. Todas essas propriedades devem ser consideradas quando estiver escolhendo a função de perda ideal para resolver um determinado tipo de problema.

\subsubsection*{Convexidade}

Uma função convexa é uma função na qual qual ponto de mínimo local é também o ponto de mínimo global \parencite{LossesArticle}. Uma forma fácil de identificar se uma função é convexa ou não, é verificar se ela possui o formato de um funil, ou um formato da letra ``V''. Em \textit{Convex optimization}, \textcite{boyd2004convex} elaboram e discutem uma série de teoremas que buscam avaliar se uma função estudada é convexa ou não. Alguns desses teoremas são utilizados no Apêndice~\ref{apendice:propriedades-analiticas-funcoes-de-perda} (Propriedades Analíticas das Funções de Perda) para provar a convexidade das funções de perda analisadas durante este e outros capítulos.

Funções convexas são ideais para serem utilizadas como funções de perda, porque facilitam a otimização utilizando métodos baseados em gradiente. Como foi visto no Capítulo~\ref{cap:retropropagacao-gradiente}, é bem mais intuitivo para o modelo encontrar pontos de mínimo em uma função que apresenta o formato de uma tigela do que em uma função não-convexa, cheia de ondas e com vários pontos de mínimo locais.

\subsubsection*{Diferenciabilidade}

A questão da diferenciabilidade está relacionada também com a utilização de otimizadores baseados em gradiente. Ao utilizar a retropropagação para fazer o ajuste de parâmetros, o primeiro cálculo do gradiente será pelas derivadas parciais da função de perda. Essas derivadas formam juntas o vetor gradiente que será propagado por toda a rede, das camadas mais próximas da saída até as camadas mais próximas da entrada. De tal maneira que vai sendo ajustado os valores dos pesos e vieses do modelo. 

Utilizar uma função que não seja diferenciável ou que possuam muitos pontos de descontinuidade certamente irá afetar negativamente o algoritmo da retropropagação, atrapalhando o aprendizado com o uso do gradiente\footnote{Vale dizer também que mesmo que uma função apresente um ou outro ponto de descontinuidade, pode ser que isso não atrapalhe a otimização por gradiente, um exemplo disso é a função erro absoluto médio, que a apresenta um ponto de descontinuidade. Mas, é possível contornar esse ``problema'' utilizando o cálculo do subgradiente.}.

Em \textit{Principles of Mathematical Analysis}, \textcite{rudin1976principles} discute a notação de classes de continuidade, chamadas neste livro por $C^k$, para descrever o comportamento de funções matemáticas. Uma função de classe $C^0$ é uma função contínua em todo o seu domínio, mas não é suave, de fato que sua derivada não será contínua. Uma função de classe $C^1$ é contínua e suave em todo o seu domínio, mas a derivada de sua derivada não é contínua. Por fim, uma função de classe $C^{\infty}$ (infinitamente diferenciável), tem todas as suas derivadas contínuas.

A notação de classes de continuidade nos ajuda a entender de forma mais fácil as propriedades de uma função. Neste livro, para cada uma das funções de perda analisadas, é discutida a sua classe de continuidade. Suas provas matemáticas são encontradas no Apêndice~\ref{apendice:propriedades-analiticas-funcoes-de-perda}.

\subsubsection*{Robustez}

As funções de perda devem ser capazes de lidar com \textit{outliers} e não serem afetadas por um pequeno número de valores extremos \parencite{LossesArticle}. Os \textit{outliers} são valores que estão fora da padrão dos dados gerais, eles podem ter valores consideravelmente maiores ou menores que a distribuição geral. Eles são um dos principais problemas ao utilizar uma função de perda, pois, caso esta seja sensível a \textit{outliers}, esses dados irão enviesar com o cálculo da perda. Consequentemente, irão afetar negativamente o aprendizado por gradiente. 

A definição de robustez está intrinsecamente ligada com o conceito de continuidade de Lipschitz.

\begin{definition}[Continuidade de Lipschitz]
    Uma função $f: \mathbb{R}^n \to \mathbb{R}$ é dita \textbf{Lipschitz-contínua} se existe uma constante $K \in \mathbb{R}^+$ (chamada constante de Lipschitz) tal que, para todos $\boldsymbol{x}, \boldsymbol{y}$ no domínio:
    \begin{equation}
        |f(\boldsymbol{x}) - f(\boldsymbol{y})| \le K \|\boldsymbol{x} - \boldsymbol{y}\|_2
    \end{equation}
    Onde $\|\cdot\|_2$ denota a norma Euclidiana ($L^2$).
\end{definition}

A continuidade de Lipschitz garante que se uma função segue essa propriedade, caso o modelo cometa um erro absurdamente grande, o resultado da função de perda não irá ``explodir'' para o infinito. Isso é importante, pois também se relaciona com o problema dos gradientes explosivos. Utilizar uma função que não seja robusta, pode aumentar as chances desse fenômeno durante o treino.

Assim como as classes de continuidade e a convexidade, para cada função apresentada neste capítulo é provado se ela é robusta a \textit{outliers} ou não. Esses demonstrações estão no Apêndice~\ref{apendice:propriedades-analiticas-funcoes-de-perda}, onde é verificada a continuidade de Lipschitz das funções de perda. 

\subsubsection*{Suavidade}

A suavidade se relaciona com a questão da continuidade da função. Uma função que não é suave, que apresenta pontos de descontinuidade, como ``bicos'' ou transições bruscas entre um pedaço e outro da função afeta diretamente o cálculo da derivada, e, portanto, do gradiente. Esses pontos de descontinuidade, são um empecilho para o cálculo da derivada. Pois uma função não pode ser derivada em um ponto no qual não é contínua. Considerar se uma função é suave ou não é ideal, uma vez que também irá refletir no cálculo do gradiente dessa função de perda.

\subsubsection*{Esparsidade}

Uma função de perda que promova a esparsidade deve incentivar o modelo a produzir uma saída esparsa \parencite{LossesArticle}. \textit{LossesArticle} explicam que essa propriedade é útil ao trabalhar com dados de alta dimensão e quando o número de características importantes é pequeno.

\subsubsection*{Monotonicidade}

Uma função de perda é monotônica se seu valor diminui à medida que a saída prevista se aproxima da saída verdadeira \parencite{LossesArticle}. Ao escolher uma função de perda que seja monotônica é possível garantir que o processo de otimização do modelo esteja caminhando em direção da solução correta.

\section{Funções de perda para regressão para propósitos gerais}

A primeira seção desse capítulo que explora as funções de perda busca focar nas funções de perda mais comuns, que geralmente são umas das primeiras alternativas para serem utilizadas ao construir um modelo de regressão. Entre elas, vale destacar a dupla de funções erro quadrático médio (também conhecido como perda L2) e o erro absoluto médio (que também recebe o nome de perda L1). Contudo, essa seção também apresenta outras alternativas, como a perda de Huber, uma função que tem como objetivo unir as principais características do \textit{MSE} e do \textit{MAE}. Além disso, também é apresentada uma alternativa para a perda de Huber: a perda log-cosh, que possui propriedades parecidas com essa outra função, mas que resolve os problemas de descontinuidade.

\subsection{Erro quadrático médio (MSE)}%
\index{Funções de Perda!Erro quadrático médio (MSE)}%
\label{sec:mse-loss}

O \textbf{\gls{mse-loss}}, também conhecido como \textbf{\textit{mean squared error}} (\textbf{\textit{MSE}}) ou \textbf{perda L2}, tem suas origens no século XIX. Nessa época, estava em alta o estudo da astronomia, com os pesquisadores buscando entender o comportamento das estrelas e dos outros planetas. O \textit{MSE} surge nesse contexto, mais precisamente no trabalho \textit{Nouvelles méthodes pour la détermination des orbites des comètes} (Novos métodos para determinar as órbitas dos cometas). Nele, \textcite{Legendre1805} introduz o método dos mínimos quadrados, o qual tem como objetivo minimizar a soma dos quadrados dos erros. Ou seja, minimizar o erro quadrático médio.

O trabalho de Legendre não foi o único responsável por popularizar o método dos mínimos quadrados. Em \textit{Theoria Motus Corporum Coelestium in Sectionibus Conicis Solem Ambientium} (Teoria do Movimento dos Corpos Celestes em Seções Cônicas ao Redor do Sol), \textcite{Gauss1809}, discute um problema que se inicia com um sistema de equações lineares com mais equações que incógnitas derivadas de observações astronômicas que possuem erros. Seu objetivo é encontrar o valor mais provável para essas incógnitas.

É definido por Gauss que, o valor mais provável de uma quantidade de medidas de igual precisão é dado pela média aritmética dessas medidas \parencite{Gauss1809}. \textcite{Gauss1809} questiona qual deve ser a lei de probabilidade dos erros para que a média aritmética seja sempre a estimativa mais provável. Utilizando o princípio da máxima-verossimilhança das probabilidades de todos os erros, o matemático consegue ser capaz de demonstrar matematicamente que a única função que satisfaz o seu postulado da média aritmética é a própria distribuição Normal.

Sabendo disso, Gauss inverte a lógica: se a probabilidade de um conjunto de erros é maximizada quando a soma dos seus quadrados é minimizada, então o método dos mínimos quadrados é o método que dá a solução mais provável para a suposição de que os erros de medição são normalmente distribuídos \parencite{Gauss1809}. O matemático adicionou mais embasamento teórico na técnica de Legendre, aumentando a confiabilidade do método dos mínimos quadrados, e também do \textit{MSE}.

Passados quase 200 anos, o erro quadrático médio passa a ser uma das principais funções a ser utilizada para calcular o erro dos modelos. Caso você tenha lido o Capítulo~\ref{cap:retropropagacao-gradiente}, pode ter notado que ele foi uma das funções, junto com a sigmoide logística e a equação do neurônio, a ser utilizada para deduzir os cálculos das atualizações de pesos para o algoritmo da retropropagação.

Voltando para analogia do jogo dos dardos. Imagine que no jogo de dardos todos os jogadores começam com 1.000 pontos, e, vão perdendo conforme vão errando os lançamentos. Além disso, você definiu uma regra que diz que o erro (ou débito dos pontos totais) será dado pelo quadrado da distância entre o centro até o dardo jogado. Jogadores que acertam dardos mais próximos do centro, perdem poucos pontos. Entretanto, aqueles jogadores que são mais desleixados, que acertam longe do centro perdem muitos pontos. Porque além de já perderem muitos pontos por estarem longe do centro, essa distância ainda será elevada ao quadrado.

Essa ideia de elevar o erro ao quadrado é o grande motivador para entender o cálculo do \textit{MSE}. A sua fórmula Equação~\ref{eq:mse}. O erro quadrático médio calcula a diferença do valor real $y_j$ com o valor predito pelo modelo $\hat{y}_j$, ou seja, o erro daquela predição. Tendo analisado todos os erros, o \textit{MSE} calcula a média dos erros, retornando o valor final da perda.

\begin{equacaodestaque}{Erro quadrático médio}
    \Loss_{\text{MSE}} (y_j, \hat{y}_j) = \frac{1}{N} \sum_{j=1}^{N} (y_j -\hat{y}_j)^2%
    \label{eq:mse}
\end{equacaodestaque}

Em que:

\begin{itemize}
    \item $y_j$ representa o valor real da j-ésima amostra;
    \item $\hat{y}_j$ representa o valor predito pelo modelo;
    \item $N$ representa o número de amostras.
\end{itemize}

A Figura \ref{fig:mse} ilustra o comportamento do erro quadrático médio. O gráfico da esquerda mostra a vista em duas dimensões do \textit{MSE} com foco no erro, enquanto o gráfico da direita apresenta a superfície no espaço dessa função de perda. As funções de perda neste capítulo estão em sua maioria representadas dessa forma, uma vez que apresentar a vista bidimensional com relação ao erro ilustra como o erro cresce. Enquanto a vista em três dimensões garante uma melhor visualização de como a função de perda é representada de fato.

\begin{figure}[h!]
    \centering 

    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \begin{tikzpicture}
            \begin{axis}[
                width=\linewidth,  
                height=7cm,
                xlabel={Erro},
                ylabel={Perda},
                axis lines=middle,
                grid=major,
                grid style={dashed, gray!40},
                xmin=-3.5, xmax=3.5,
                ymin=-0.5, ymax=9.5,
                legend pos=north west,
                title style={font=\bfseries\small},
                label style={font=\small},
                tick label style={font=\scriptsize}
            ]
                \addplot[
                    domain=-3:3, 
                    samples=100, 
                    color=blue, 
                    very thick
                ] {x^2};

            \end{axis}
        \end{tikzpicture}
        \caption{Vista da função de perda em relação ao erro.}%
        \label{fig:mse-2d}
    \end{subfigure}
    \hfill 
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \begin{tikzpicture}
            \begin{axis}[
                width=\linewidth,
                height=7cm,
                xlabel={$y$ (Real)},
                ylabel={$\hat{y}$ (Previsto)},
                zlabel={Perda},
                grid=major,
                view={150}{45}, 
                zmin=0, zmax=35, 
                title style={font=\bfseries\small},
                label style={font=\small},
                tick label style={font=\scriptsize}
            ]
                \addplot3[
                    mesh,          
                    color=blue,
                    shader=interp,  
                    domain=-5:5,    
                    domain y=-5:5,  
                    samples=15      
                ] { (x - y)^2 };
            \end{axis}
        \end{tikzpicture}
        \caption{Superfície da função de perda.}%
        \label{fig:mse-3d}
    \end{subfigure}

    \caption{Visualizações do erro quadrático médio em duas e em três dimensões.}%
    \label{fig:mse}
    \fonte{O autor (2025).}
\end{figure}

\medskip
\begin{center}
 * * *
\end{center}
\medskip

\subsubsection*{Características do erro quadrático médio}

\begin{itemize}
    \item \textbf{Continuidade, suavidade e diferenciabilidade:} O erro quadrático médio é uma função de classe $C^{\infty}$, é infinitamente diferenciável (ver Apêndice~\ref{ap:deducoes-mse}). Como consequência, não são encontrados problemas para derivar essa função e também o cálculo do vetor gradiente. Isso torna o \textit{MSE} uma boa função para ser utilizada em conjunto com otimizadores baseados em gradiente.
    \item \textbf{Sensibilidade a \textit{outliers} (não robusto):} O \textit{MSE} é uma função que não é Lipschitz-contínua (ver Apêndice~\ref{ap:deducoes-mse}). Caso o erro do modelo treinado cresça exponencialmente, o valor numérico da perda será consideravelmente grande. Não somente isso, ele é sensível a \textit{outliers}, se um grupo de dados fora do padrão estiver presente no conjunto de dados, eles podem enviesar o cálculo da perda, e também na otimização em busca de pontos de mínimo.
    \item \textbf{Convexidade:} O \textit{MSE} é uma função estritamente convexa (ver Apêndice~\ref{ap:deducoes-mse}), representado pelos gráficos da Figura~\ref{fig:mse}, que apresentam uma função com formato de tigela. Dessa forma, essa característica torna-se uma vantagem se estiver sendo utilizados otimizadores baseados em gradiente. Contudo, devido às transformações não-lineares, \textcite{LossesArticle} argumentam que essa propriedade pode deixar de ser verdade em algoritmos de aprendizado profundo.
    \item \textbf{Não-negatividade:} Como gráficos da Figura~\ref{fig:mse} ilustram, o \textit{MSE} é retorna apenas valores positivos. Isso se dá devido à diferença das entradas estar sendo calculada, e, em seguida, elevada ao quadrado, impendido que valores negativos ocorram na saída;
    \item \textbf{Baixa interpretabilidade:} O \textit{MSE} eleva o erro ao quadrado. Ao retornar o valor da perda, não é visto diretamente a distância entre os pontos reais e os pontos preditos pelo modelo. Essa cálculo dificulta saber, de forma rápida, quão bem ou mal o modelo está performando. Funções como a \textit{MAE}, em que o erro é dado pelo módulo da distância entre os dois pontos são mais diretas em mostrar a performance do modelo;
\end{itemize}

\medskip
\begin{center}
 * * *
\end{center}
\medskip

\subsubsection*{Gradiente do erro quadrático médio}

A derivada do erro quadrático médio está na Equação~\ref{eq:mse-derivada}. As derivadas parciais de uma função de perda são de extrema importância para uma rede neural. É partir delas que é calculado o primeiro vetor gradiente, que passa a ser retropropagado por toda a rede no sentido inverso. Indo primeiro das últimas camadas para as camadas de entrada\footnote{Portanto, as derivadas das funções de perda serão discutidas sempre que possível para explicar as características de uma função neste capítulo.}.

\begin{equacaodestaque}{Derivada parcial do erro quadrático médio em relação à predição}
    \frac{\partial\Loss_{\text{MSE}}}{\partial\hat{y}_j} (y_j, \hat{y}_j) = \frac{2}{N} (\hat{y}_j -y_j)%
    \label{eq:mse-derivada}
\end{equacaodestaque}

A Equação~\ref{fig:mse-derivada} mostra a derivada parcial do erro quadrático médio em relação à predição feita pelo modelo $\hat{y}_j$. Mas existe também a derivada parcial em relação ao valor real $y_j$. Juntas, tem-se a Equação~\ref{eq:vetor-gradiente-mse}, que representa o vetor gradiente do \textit{MSE}. Neste livro será dado um foco maior em apresentar apenas uma das derivadas. Porque, elas muitas vezes apresentarão grandes semelhanças.

\begin{equation}
    \nabla (y_j, \hat{y}_j) = \left( \frac{\partial \Loss_{\text{MSE}}}{\partial y_j}, \frac{\partial \Loss_{\text{MSE}}}{\partial \hat{y}_j} \right)%
    \label{eq:vetor-gradiente-mse}
\end{equation}

Os gráficos da derivada parcial do erro quadrático médio estão ilustrados na Figura~\ref{fig:mse-derivada}. A Figura~\ref{fig:mse-derivada-2d} mostra que o gradiente do \textit{MSE} é proporcional ao erro do modelo. Um erro alto, gera um gradiente alto, e vice-versa. Quando o modelo está nas primeiras épocas do treinamento, e ainda não se ajustou aos dados, seu erro será alto. Se nesse treinamento estiver sendo utilizado um algoritmo de otimização baseado em gradiente, o ajuste dos parâmetros do modelo será mais brusco. Dado que a variação dos valores dos parâmetros de uma iteração para outra é diretamente proporcional ao vetor gradiente (como explica a Equação~\ref{eq:gradiente-do-erro-em-relacao-a-um-peso-de-um-neuronio-regressao}). Contudo, quando o modelo já tiver melhor se ajustado aos dados, o erro será menor, o gradiente também. Por conseguinte, a retropropagação irá garantir um ajuste fino nos pesos e vieses do modelo.

\begin{figure}[h!]
    \centering 
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \begin{tikzpicture}
            \begin{axis}[
                width=\linewidth,  
                height=7cm,
                xlabel={Erro ($e = \hat{y} - y$)},
                ylabel={Gradiente ($\frac{\partial L}{\partial \hat{y}}$)},
                axis lines=middle,
                grid=major,
                grid style={dashed, gray!40},
                xmin=-3.5, xmax=3.5,
                ymin=-6.5, ymax=6.5,
                legend pos=north west,
                title style={font=\bfseries\small},
                label style={font=\small},
                tick label style={font=\scriptsize}
            ]
                \addplot[
                    domain=-3:3, 
                    samples=100, 
                    color=red, 
                    very thick
                ] {2*x};
                \addlegendentry{$\frac{\partial L}{\partial \hat{y}} = \frac{2}{N} \cdot e$}
            \end{axis}
        \end{tikzpicture}
        \caption{Relação linear entre erro e gradiente da perda.}%
        \label{fig:mse-derivada-2d}
    \end{subfigure}
    \hfill 
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \begin{tikzpicture}
            \begin{axis}[
                width=\linewidth,
                height=7cm,
                xlabel={$y$ (Real)},
                ylabel={$\hat{y}$ (Previsto)},
                zlabel={Gradiente ($\frac{\partial L}{\partial \hat{y}}$)},
                grid=major,
                view={150}{45},
                zmin=-20.5, zmax=20.5,
                title style={font=\bfseries\small},
                label style={font=\small},
                tick label style={font=\scriptsize}
            ]
                \addplot3[
                    mesh,           
                    color=red,      
                    shader=interp,  
                    domain=-5:5,    
                    domain y=-5:5,  
                    samples=15      
                ] { 2 * (y - x) };
            \end{axis}
        \end{tikzpicture}
        \caption{Superfície do gradiente no espaço.}%
        \label{fig:mse-derivada-3d}
    \end{subfigure}

    \caption{Visualizações do gradiente do erro quadrático médio em relação à predição.}%
    \label{fig:mse-derivada}
    \fonte{O autor (2025).}
\end{figure}

\begin{equation}
    \frac{\partial E}{\partial w_{ji}} = \frac{\partial E}{\partial x_j} \cdot y_i \quad \text{ou} \quad \frac{\partial E}{\partial w_{ji}} = \frac{\partial E}{\partial y_j} \cdot \sigma'(x_j) \cdot y_i
    \label{eq:gradiente-do-erro-em-relacao-a-um-peso-de-um-neuronio-perda-regressao}
\end{equation}

\medskip
\begin{center}
 * * *
\end{center}
\medskip

\subsubsection*{Raiz do erro quadrático médio}

Além disso, cabe discutir a \textbf{raiz do erro quadrático médio}, também chamada de \textbf{\textit{root mean squared error}} (\textbf{\textit{RMSE}}). Ela está na Equação~\ref{eq:rmse-metric}. Ela é uma função derivada do \textit{MSE}, cuja diferença está no cálculo de uma raiz quadrada a partir do resultado do \textit{MSE}.

\begin{equacaodestaque}{Raiz do erro quadrático médio}
    {RMSE} (y_j, \hat{y}_j) = \sqrt{\frac{1}{N} \sum_{j=1}^{N} (y_j -\hat{y}_j)^2}%
    \label{eq:rmse-metric}
\end{equacaodestaque}

A \textit{RMSE} serve de métrica para avaliar os modelos de regressão. O \textit{MSE} funciona como uma função de perda bem como uma métrica avaliativa. Mas, é difícil interpretar a primeira vista os seus resultados, uma vez que ele eleva ao quadrado o cálculo do erro. Com a \textit{RMSE} é resolvido esse ``problema'' da interpretabilidade, gerando um resultado mais tangível para entender como o modelo está performando.

\medskip
\begin{center}
 * * *
\end{center}
\medskip

\subsubsection*{Aplicações do erro quadrático médio em aprendizado de máquina}%
\index{Aplicações práticas! Erro quadrático médio (MSE)}

Além de estar presente nas deduções do algoritmo da retropropagação como uma função de perda, o erro quadrático médio é uma função bem versátil para ser aplicada em problemas de regressão. Seu uso não se restringe a somente uma função de perda, servindo como guia para a otimização do modelo. O \textit{MSE} também é uma ótima métrica para avaliar o desempenho de um modelo de regressão. Junto do \textit{MSE} está a \textit{RMSE}, essas duas funções tornam-se ótimas escolhas para medir a performance de um modelo.

Algumas aplicações do \textit{MSE} e da \textit{RMSE} são:

\begin{itemize}
    \item \textbf{Estimação de custos médicos (saúde):} Em \textit{Medical Costs Estimation Using Linear Regression Method}, \textcite{MedicalCostsEstimationUsingLR} utilizam técnicas de regressão linear como a regressão linear múltipla para fazer previsões de custos médicos. Para avaliar os modelos de regressão criados, os autores utilizam o erro quadrático médio, mas, também aplicam outras métricas, como o erro absoluto médio (tópico principal da Seção xx), e também a métrica $R^2$ \parencite{MedicalCostsEstimationUsingLR};
    \item \textbf{Estimação de preços de imóveis (mercado imobiliário):} No artigo \textit{An Optimal House Price Prediction Algorithm: XGBoost}, \textcite{OptimalHousePricePrediction} aplicam diferentes modelos de regressão (como regressão linear, florestas aleatórias e XGBoost) com intuito de criar um modelo ideal para a prever valores de casas. Uma das técnicas para avaliar o desempenho dos algoritmos criados foi o uso do \textit{MSE} e da \textit{RMSE}.\parencite{OptimalHousePricePrediction};
    \item \textbf{Previsão da produção agrícola (agronomia):} No trabalho \textit{Coupling Machine Learning and Crop Modeling Improves Crop Yield Prediction in the US Corn Belt}, \textcite{CouplingMachineLearningAndCropModeling} estudavam formas de combinar técnicas com modelagem de culturas para prever a produção das plantações na região do cinturão do milho nos Estados Unidos. No artigo, os pesquisadores não utilizam diretamente o \textit{MSE}, ao invés disso, utilizam a raiz do erro quadrático médio como uma das diferentes métricas para avaliação dos modelos de previsão desenvolvidos ao longo do projeto \parencite{CouplingMachineLearningAndCropModeling};
    \item \textbf{Previsão de demanda de energia (gestão energética):} No texto \textit{Optimizing Federated Learning for Scalable Power-demand Forecasting in Microgrids} ocorre uma situação diferente ao aplicar o erro quadrático médio, \textcite{OptimizingFL} fazem uma adaptação nessa função transformando-a no \textit{Exponentially Weighted Mean Squared Error} (\textit{EW-RSM}). Essa adaptação é justificada pelos autores como uma forma de enfatizar a acurácia em previsões de longo prazo atribuindo pesos exponencialmente crescentes aos erros em etapas de tempo posteriores \parencite{OptimizingFL}.
\end{itemize}

\medskip
\begin{center}
 * * *
\end{center}
\medskip

O erro quadrático médio é ideal para ser utilizado em cenários que deve-se garantir uma maior penalização dos erros do modelo que durante seu treinamento. Isso acontece devido à sua sensibilidade a \textit{outliers}. Entretanto, vão existir cenários em que essa propriedade pode ser uma desvantagem, enviesando o treinamento de um modelo de regressão. Diante disso, alternativas como o erro absoluto médio tornam-se mais viáveis.

\subsection{Erro absoluto médio (MAE)}%
\index{Funções de Perda!Erro absoluto médio (MAE)}%
\label{sec:mae-loss}

O \textbf{\gls{mae-loss}}, também chamado de \textbf{perda L1}, é uma função que tem o mesmo propósito do erro quadrático médio, ser utilizada em tarefas de regressão. Ele não possui uma origem definida como o \textit{MSE}, esse conceito de minimizar a diferença de um resultado pelo seu valor real já havia sendo utilizado há bastante tempo. 

Trabalhos como \textit{Greedy function approximation: A gradient boosting machine} de \textcite{GreedyFunctionApproximation} utilizam essa função de perda para resolver problemas de aprendizado de máquina. No texto, o autor desenvolve um algoritmo de \textit{boosting} específico para o \textit{MAE} (o qual é apresentado com outro nome, \textbf{\textit{Least-Absolute-Deviation}} (\textbf{\textit{LAD}})), sendo responsável por dar nome ao algoritmo criado, o \texttt{LAD\_TreeBoost} \parencite{GreedyFunctionApproximation}. \textcite{GreedyFunctionApproximation} explica que esse algoritmo utiliza uma árvore de regressão com a perda, para prever a pseudo-resposta que é o sinal dos resíduos atuais. Como consequência, o modelo é atualizado adicionando em cada nó terminal da nova árvore criada a mediana dos resíduos daquela região específica.

Um trabalho mais recente que explora o uso dessa função é o \textit{Image-to-Image Translation with Conditional Adversarial Networks} \parencite{ImageToImage}. Nele, \textcite{ImageToImage} argumentam que preferiram trabalhar com a distância L1 (um dos diferentes nomes utilizados para se referir ao \textit{MAE}) devido à ela gerar imagens menos borradas. A comparação das funções de perda utilizadas na pesquisa está nas imagens da Figura~\ref{fig:comparativo-perdas-image-to-image}. A perda que apresenta os resultados mais consistentes com a realidade é a \textit{L1 + cGAN}, a qual possuí o \textit{MAE} em sua composição.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.65\linewidth]{../imagens/perda-regressao/image-to-image-perdas-comparativo.png}
    
    \caption[Perdas diferentes induzem qualidades de resultados diferentes. Cada coluna mostra resultados treinados sob uma perda diferente de aprendizado no dataset MNIST]{%
        Perdas diferentes induzem qualidades de resultados diferentes. Cada coluna mostra resultados treinados sob uma perda diferente de aprendizado no dataset MNIST
        \newline
        \small Fonte: \parencite{ImageToImage}.
    }%
    \label{fig:comparativo-perdas-image-to-image}
\end{figure}

Voltando para a analogia dos dardos. Consideramos que será feito um novo jogo com dez participantes, no jogo, cada um tem direito de jogar apenas um dardo. No final, os dardos serão somados e será dado o resultado da equipe. Se tivéssemos uma equipe muito ruim utilizando como métrica o \textit{MSE} para avaliar os pontos, o resultado seria péssimo. Os dardos estariam muito longes do centro, e, como o erro é elevado ao quadrado, isso geraria uma pontuação ainda menor. 

Contudo, se a métrica escolhida para avaliar o jogo fosse o \textit{MAE} o resultado seria melhor. Visto que, o erro cresce de forma linear. No contexto de aprendizado de máquina, os jogadores ruins se comparam com os \textit{outliers}, eles estão presentes nos dados e vão enviesar o aprendizado do modelo. Entretanto, utilizar uma função que não seja tão sensível na medição desses valores extremos pode ser uma boa alternativa para cenários em que essa configuração será comum.

O \textit{MAE} está definido na Equação~\ref{eq:mae}. Essa função de perda avalia a diferença entre dois pontos: o valor real $y_j$ e o valor predito $\hat{y}_j$, para todos os $N$ casos analisados. A partir disso, calcula a média dos resultados, retornando o valor final para a perda do modelo.

\begin{equacaodestaque}{Erro absoluto médio}
    \Loss_{\text{MAE}} (y_j, \hat{y}_j) = \frac{1}{N} \sum_{j=1}^{N} |y_j -\hat{y}_j|%
    \label{eq:mae}
\end{equacaodestaque}

Em que:

\begin{itemize}
    \item $y_j$ representa o valor real da j-ésima amostra;
    \item $\hat{y}_j$ representa o valor predito pelo modelo;
    \item $N$ representa o número de amostras.
\end{itemize}

 Diferente do erro quadrático médio, o textit{MAE} cresce de forma linear, como é mostrado na Figura~\ref{fig:mae}. Isso garante para essa função de perda uma melhor capacidade de lidar com \textit{outliers}, dado que o erro não irá aumentar de forma exacerbada como acontecia no \textit{MSE}. O \textit{MAE} também apresenta uma ``quina'', nos pontos em que o cálculo do erro é nulo. Dessa forma, essa função não pode ser classificada como suave em todo o seu domínio.

\begin{figure}[h!]
    \centering
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \begin{tikzpicture}
            \begin{axis}[
                width=\linewidth,  
                height=7cm,
                xlabel={Erro ($e = y - \hat{y}$)},
                ylabel={Perda Calculada},
                axis lines=middle,
                grid=major,
                grid style={dashed, gray!40},
                xmin=-4.5, xmax=4.5,
                ymin=-0.5, ymax=4.5,
                legend pos=north west,
                title style={font=\bfseries\small},
                label style={font=\small},
                tick label style={font=\scriptsize}
            ]
                \addplot[
                    domain=-4:4, 
                    samples=100, 
                    color=blue,  
                    very thick
                ] {abs(x)};
                
                \addlegendentry{$L = |e|$}
            \end{axis}
        \end{tikzpicture}
        \caption{Vista da função de perda em relação ao erro.}%
        \label{fig:mae-2d}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \begin{tikzpicture}
            \begin{axis}[
                width=\linewidth,
                height=7cm,
                xlabel={$y$ (Real)},
                ylabel={$\hat{y}$ (Previsto)},
                zlabel={Perda},
                grid=major,
                view={150}{45}, 
                zmin=0, zmax=10.5, 
                title style={font=\bfseries\small},
                label style={font=\small},
                tick label style={font=\scriptsize}
            ]
                \addplot3[
                    mesh,         
                    color=blue,
                    shader=interp,
                    domain=-5:5,   
                    domain y=-5:5,  
                    samples=15     
                ] { abs(x - y) };
            \end{axis}
        \end{tikzpicture}
        \caption{Superfície da função de perda.}%
        \label{fig:mae-3d}
    \end{subfigure}

    \caption{Visualizações da função de perda erro absoluto médio em duas e em três dimensões.}%
    \label{fig:mae}
    \fonte{O autor (2025).}
\end{figure}

\medskip
\begin{center}
 * * *
\end{center}
\medskip

\subsubsection*{Características do erro absoluto médio}

\begin{itemize}
    \item \textbf{Continuidade, suavidade e diferenciabilidade:} O \textit{MAE} é uma função de classe $C^0$ (ver Apêndice~\ref{ap:deducoes-mae}). Ele é contínuo em todo o seu domínio, mas, devido ao ``bico'' quando o erro é zero, não pode ser totalmente derivado. Essa característica não impede que o erro absoluto médio seja utilizado em conjunto com otimizadores baseados em gradiente. Como será visto, é possível contornar esse problema com o cálculo do subgradiente.
    \item \textbf{Convexidade:} O \textit{MAE} é convexo (ver Apêndice~\ref{ap:deducoes-mae}). Como o teorema de \textbf{Convexidade de Normas e Máximos} afirma, toda função modular é convexa como consequência da desigualdade triangular. Isso também está explicitado nos gráficos da Figura~\ref{fig:mae}, com o formato em ``V'' da função de perda.
    \item \textbf{Robustez para \textit{outliers}:} O \textit{MAE} é uma função Lipschitz-contínua (ver Apêndice~\ref{ap:deducoes-mae}). Mais precisamente, o maior valor que sua derivada pode assumir é 1. Como resultado, independente do tamanho do erro que o modelo possa cometer, isso se traduz para um valor limitado. Dessa forma, o \textit{MAE} e robusto a \textit{outliers} além de conseguir lidar com o problema dos gradientes explosivos. 
    \item \textbf{Não-negatividade:} Como é apontado na Figura~\ref{fig:mae}, o erro absoluto médio não retorna valores negativos para nenhum valor de entrada. A sua saída saída será sempre positiva ou zero, independente da entrada. Isso se dá, devido à propriedade do módulo, que não admite números negativos para sua saída;
    \item \textbf{Não-derivável em zero:} Esse ponto acontece devido à forma que a função módulo é representa graficamente, ela tem a forma de um ``bico'' em zero. Para contornar esse problema, pode ser utilizado o subgradiente.
    \item \textbf{Boa interpretabilidade:} Diferente do erro quadrático médio em que o erro total é elevado ao quadrado, no \textit{MAE} isso não ocorre. O erro é simplesmente a média das diferenças dos pontos. É intuitivo interpretar os resultados que essa função de perda retorna, considerando que não é preciso fazer nenhum cálculo adicional para ter uma noção precisa se as previsões feitas pelo modelo condizentes ou não dos valores reais.
\end{itemize}

\medskip
\begin{center}
 * * *
\end{center}
\medskip

\subsubsection*{Gradiente do erro absoluto médio}

Como destacado anteriormente, o \textit{MAE} não é diferenciável em zero. Assim, pode-se ter a ideia de que devido a isso, essa não seja uma boa função para ser utilizada em conjunto com otimizadores baseados em gradiente. Contudo, existe uma forma de contornar esse problema da não-diferenciabilidade: o subgradiente.

O subgradiente é uma estratégia que generaliza a derivada para funções convexas não-diferenciáveis, um exemplo disso é a função módulo de $x$. Em vez de um único valor em um determinado ponto, o subgradiente retorna um conjunto de valores possíveis para a inclinação da reta tangente.
 
Matematicamente, a derivada da função módulo com o subgradiente é dada por:

\begin{enumerate}
    \item Para $x > 0$: A função comporta-se como $f(x) = x$. A derivada é $1$;
    \item Para $x < 0$: A função comporta-se como $f(x) = -x$. A derivada é $-1$;
    \item Para $x = 0$: O subgradiente é o intervalo de todas as inclinações possíveis entre $-1$ e $1$.
\end{enumerate}

Ao utilizar o subgradiente para calcular a derivada parcial do erro absoluto médio em relação à predição $\hat{y}_j$ é encontrada a Equação~\ref{eq:mae-derivada}.

\begin{equacaodestaque}{Derivada parcial do erro absoluto médio em Relação à predição}
    \frac{\partial\Loss_{\text{MAE}}}{\partial\hat{y}_j} (y_j, \hat{y}_j) = 
    \begin{cases} 
      -1 & \text{se } \hat{y}_j > y_j \\
      +1 & \text{se } \hat{y}_j < y_j \\
      [-1, +1] & \text{se } \hat{y}_j = y_j
    \end{cases}%
    \label{eq:mae-derivada}
\end{equacaodestaque}

 Pelo fato do \textit{MAE} ser uma função que não é diferenciável em todos o seu domínio, isso reflete no jeito com que o gráficos da derivada são representados, os quais estão na Figura~\ref{fig:mae-derivada}. Existem, também na derivada, pontos de descontinuidade, fazendo com que o gráfico dessa função se assemelhe ao gráfico da função de ativação degrau unitário. Esses pontos de descontinuidade, estão exatamente em zero, assim como na função original.

Diferente do \textit{MSE} que é uma parábola, e portanto, ao ser derivado, gera o gráfico de uma reta que cresce de forma linear. O \textit{MAE} gera o gráfico de duas retas constantes, dado que a derivada de uma função linear é uma função constante.

\begin{figure}[h!]
    \centering
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \begin{tikzpicture}
            \begin{axis}[
                width=\linewidth,  
                height=7cm,
                xlabel={Erro ($e = \hat{y} - y$)},
                ylabel={Gradiente ($\frac{\partial L}{\partial \hat{y}}$)},
                axis lines=middle,
                grid=major,
                grid style={dashed, gray!40},
                xmin=-3.5, xmax=3.5,      
                ymin=-1.5, ymax=1.5,   
                ytick={-1, 0, 1},     
                legend pos=north west,
                title style={font=\bfseries\small},
                label style={font=\small},
                tick label style={font=\scriptsize}
            ]
                \addplot[
                    domain=-3:0, 
                    samples=100, 
                    color=red, 
                    very thick
                ] {-1};
                \addplot[
                    domain=0:3, 
                    samples=100, 
                    color=red, 
                    very thick
                ] {1};
                \addplot[only marks, mark=o, color=red, mark size=2pt] coordinates {(0,-1) (0,1)};
            \end{axis}
        \end{tikzpicture}
        \caption{Relação constante por partes entre erro e gradiente da perda.}%
        \label{fig:mae-derivada-2d}
    \end{subfigure}
    \hfill 
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \begin{tikzpicture}
            \begin{axis}[
                width=\linewidth,
                height=7cm,
                xlabel={$y$ (Real)},
                ylabel={$\hat{y}$ (Previsto)},
                zlabel={Gradiente ($\frac{\partial L}{\partial \hat{y}}$)},
                grid=major,
                view={150}{45},
                zmin=-1.5, zmax=1.5, 
                ztick={-1, 0, 1},
                title style={font=\bfseries\small},
                label style={font=\small},
                tick label style={font=\scriptsize}
            ]
                \addplot3[
                    mesh,           
                    color=red,     
                    shader=interp,  
                    domain=-5:5,    
                    domain y=-5:5, 
                    samples=15    
                ] { sign(y - x) };
            \end{axis}
        \end{tikzpicture}
        \caption{Superfície do gradiente no espaço.}%
        \label{fig:mae-derivada-3d}
    \end{subfigure}

    \caption{Visualizações do gradiente do erro absoluto médio em relação à predição.}%
    \label{fig:mae-derivada}
    \fonte{O autor (2025).}
\end{figure}

\medskip
\begin{center}
 * * *
\end{center}
\medskip

\subsubsection*{Aplicações do erro absoluto médio em aprendizado de máquina}%
\index{Aplicações práticas! Erro absoluto médio (MAE)}

Além dos casos discutidos no início da seção: o \texttt{LAD\_TreeBoost} e o artigo \textit{Image-to-Image Translation with Conditional Adversarial Networks}, o \textit{MAE} está presente em uma série de trabalhos. Atuando tanto como função de perda, quanto como métrica de avaliação. 

Algumas aplicações do \textit{MAE} são:

\begin{itemize}
    \item \textbf{Avaliação de idade óssea e estimação do escore de cálcio na artéria coronária (Saúde):} Em \textit{Regression Metric Loss: Learning a Semantic Representation Space for Medical Images}, \textcite{chao2022regressionmetriclosslearning} desenvolvem algoritmos de regressão para estimar escore de cálcio da artéria coronária além de um segundo algoritmo para avaliação da idade óssea. Os autores também apresentam uma nova função de perda, a \textit{RM-Loss} que demonstra ser mais apta para resolver os problemas propostos de regressão \parencite{chao2022regressionmetriclosslearning}. Como forma de avaliar essa nova função criada e também as diferentes outras funções comparadas no artigo, \textcite{chao2022regressionmetriclosslearning} utilizam o erro absoluto médio em uma das métricas;
    \item \textbf{Restauração de imagens (Engenharia):} No artigo \textit{Noise2Noise: Learning Image Restoration without Clean Data}, \textcite{Noise2Noise} estudaram formas de restaurar imagens corrompidas sem a utilização de dados limpos. Em um dos experimentos os autores estavam buscando uma forma ideal de remover textos de imagens, de forma que a perda L1, por ser uma função de perda robusta, conseguiu atingir bons resultados nessa tarefa \parencite{Noise2Noise};
    \item \textbf{Previsão da produção de energia eólica (Setor energético):} Em \textit{Minimum Open Data Subset for Wind Power Prediction}, \textcite{MinimumOpenDataSubsetForWindPowerPrediction} utilizam um modelo de florestas aleatórias com para prever a produção de energia eólica. Na avaliação do modelo de regressão desenvolvido, os autores utilizam como métricas o \textit{MAE} além do \textit{RMSE}. Vale comentar que em testes realizados pelos pesquisadores foi criado um modelo com erro absoluto médio de 0,071, indicando um excelente resultado para para o algoritmo criado \parencite{MinimumOpenDataSubsetForWindPowerPrediction}.
    \item \textbf{Previsão de poluição do ar (Setor ambiental):} No trabalho de \textcite{nedungadi2025aircastimprovingairpollution}, \textit{AirCast: Improving Air Pollution Forecasting Through Multi-Variable Data Alignment}, os autores buscam formas de melhorar a previsão da poluição do ar. Os pesquisadores utilizam uma função inspirada pelo erro absoluto médio, o \textit{Frequency-weighted Mean Absolute Error} (\textit{fMAE}), tendo como principal vantagem lidar com variáveis que apresentam uma distribuição de cauda pesada, como as variáveis PM1, PM2.5 e PM10, que indicam a qualidade do ar \parencite{nedungadi2025aircastimprovingairpollution}.
\end{itemize}

\medskip
\begin{center}
 * * *
\end{center}
\medskip

O erro quadrático médio penaliza em maior grau os \textit{outliers}, enquanto o erro absoluto médio é uma função robusta a \textit{outliers}. Contudo, pode ser necessário ter uma função que combine essas duas características. De forma que ela penalize em maior força os erros pequenos, mas ao mesmo tempo seja robusta a \textit{outliers}, permitindo evitar que o modelo seja enviesado no treinamento. Nesses casos, uma solução é a perda de Huber.

\subsection{Perda de Huber (Huber loss)}%
\index{Funções de Perda!Perda de Huber (Huber loss)}%
\label{sec:huber-loss}

Proposta por Peter J. Huber, a \gls{huber-loss}, também chamada de \textit{\textbf{Huber loss}} foi apresentada no trabalho \textit{Robust Estimation of a Location Parameter} \parencite{HuberLoss}. \textcite{HuberLoss} estudava maneiras de fazer uma estimação robusta de um parâmetro de localização (como a média o mediana de um conjunto de dados) quando a distribuição dos dados é aproximadamente conhecida. Para solucionar esse problema, o autor define um estimador robusto $p$ que segue a Equação~\ref{eq:huber-loss-do-huber} \parencite{HuberLoss}.

\begin{equation}
    \rho(t) = 
    \begin{cases}
        \frac{1}{2} t^2 & \text{se} |t| < k \\
        k |t| - \frac{1}{2} k^2 & \text{se} t \ge k
    \end{cases}
    \label{eq:huber-loss-do-huber}
\end{equation}

Ao desenvolver a Equação~\ref{eq:huber-loss-do-huber}, Huber cria uma função que comportava de forma quadrática, semelhante ao \textit{MSE} para os casos em que $|t| < k$. Já para os casos em que $t \ge k$, a função cresce linearmente, como o \textit{MAE}. Essa função criada pelo pesquisador é a perda de Huber, que, representada com as notações deste livro, está na Equação~\ref{eq:huber-loss}.

\begin{equacaodestaque}{Perda de Huber}
    \Loss_{\text{Huber}} (y, \hat{y}) = 
    \begin{cases} 
      \frac{1}{2} (y -\hat{y})^2 & \text{para } |y -\hat{y}| \le\delta\\
      \delta(|y -\hat{y}| -\frac{1}{2}\delta) & \text{caso contrário}
    \end{cases}%
    \label{eq:huber-loss}
\end{equacaodestaque}

Em que:

\begin{itemize}
    \item $y_j$ representa o valor real da j-ésima amostra;
    \item $\hat{y}_j$ representa o valor predito pelo modelo;
    \item $N$ representa o número de amostras;
    \item $\delta$ representa o parâmetro de sensibilidade.
\end{itemize}

Como a Figura~\ref{fig:huber-loss} evidencia, a perda de Huber é uma função ``híbrida'', apresentando dois comportamentos distintos. Até o valor em que o erro é menor que o parâmetro $\delta$, a perda segue o formato de uma parábola. Quando o erro torna-se maior que $\delta$, a perda passa a ser linear. Pode-se pensar que nessas junções, em que o erro é igual a $\delta$, seriam formados ``bicos'' afetando a suavidade da função. Contudo, a perda de Huber é uma função de classe $\mathbb{C}^1$, o que significa que ela é suave, contínua e diferenciável em todo o seu domínio para a primeira derivada. Isso pode ser um problema para aqueles otimizadores que são de segunda ordem, como o método de Newton-Raphson, neste caso, eles dependem que a função seja de classe $\mathbb{C}^2$ para conseguir encontrar pontos de mínimo.

\begin{figure}[h!]
    \centering
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \begin{tikzpicture}
            \def\delta{1.0} 
            
            \begin{axis}[
                width=\linewidth,  
                height=7cm,
                xlabel={Erro ($e = y - \hat{y}$)},
                ylabel={Perda Calculada},
                axis lines=middle,
                grid=major,
                grid style={dashed, gray!40},
                xmin=-4.5, xmax=4.5,  
                ymin=-0.5, ymax=4.5,      
                legend pos=north west,
                title style={font=\bfseries\small},
                label style={font=\small},
                tick label style={font=\scriptsize}
            ]
                \addplot[
                    domain=-4:4, 
                    samples=201, 
                    color=blue,  
                    very thick
                ] { abs(x) <= \delta ? 0.5*x^2 : \delta*(abs(x) - 0.5*\delta) };

                \draw[dashed, gray] (axis cs:-\delta, 0) -- (axis cs:-\delta, {\delta*(\delta-0.5*\delta)});
                \draw[dashed, gray] (axis cs:\delta, 0) -- (axis cs:\delta, {\delta*(\delta-0.5*\delta)});
            \end{axis}
        \end{tikzpicture}
        \caption{Vista da função de perda em relação ao erro.}%
        \label{fig:huber-2d}
    \end{subfigure}
    \hfill 
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \begin{tikzpicture}

            \def\delta{1.0}
            
            \begin{axis}[
                width=\linewidth,
                height=7cm,
                xlabel={$y$ (Real)},
                ylabel={$\hat{y}$ (Previsto)},
                zlabel={Perda},
                grid=major,
                view={150}{45},
                zmin=0, zmax=10.5,
                title style={font=\bfseries\small},
                label style={font=\small},
                tick label style={font=\scriptsize}
            ]
                \addplot3[
                    mesh,      
                    color=blue,
                    shader=interp,
                    domain=-5:5, 
                    domain y=-5:5,
                    samples=15  
                ] { abs(x-y) <= \delta ? 0.5*(x-y)^2 : \delta*(abs(x-y) - 0.5*\delta) };
            \end{axis}
        \end{tikzpicture}
        \caption{Superfície da função de perda.}%
        \label{fig:huber-3d}
    \end{subfigure}

    \caption{Visualizações da função de perda de Huber (com $\delta=1$) em duas e em três dimensões.}%
    \label{fig:huber-loss}
    \fonte{O autor (2025).}
\end{figure}

O hiperparâmetro $\delta$ serve para ajustar a sensibilidade da perda de Huber. Um valor muito alto, como demonstrado na Figura~\ref{fig:huber-2d-d40}, torna a função muito sensível ao erro e também a \textit{outliers}. Ela também se assemelha com os gráficos do erro quadrático médio, dado que para grande parte de seu domínio, ela calcula o erro de forma quadrática. Um valor muito pequeno para $\delta$, como na Figura~\ref{fig:huber-2d-d05}, faz com que a parábola da perda de Huber seja curta. Com isso, a função apresenta uma melhor robustez a \textit{outliers}, se assemelhando com o erro absoluto médio.

\begin{figure}[h!]
    \centering
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \begin{tikzpicture}
            \def\delta{0.5} 
            \begin{axis}[
                width=\linewidth,
                height=7cm,
                xlabel={Erro ($\hat{y} - y$)},
                ylabel={Perda Calculada},
                axis lines=middle,
                grid=major,
                grid style={dashed, gray!40},
                xmin=-4.5, xmax=4.5,
                ymin=-0.5, ymax=4.5,
                legend pos=north west,
                title style={font=\bfseries\small},
                label style={font=\small},
                tick label style={font=\scriptsize},
            ]
                \addplot[
                    domain=-4:4, 
                    samples=201, 
                    color=blue, 
                    very thick,
                ] {(abs(x) <= \delta) ? (0.5*x^2) : (\delta*(abs(x) - 0.5*\delta))};
            \end{axis}
        \end{tikzpicture}
        \caption{Perda 2D com $\delta = 0.5$.}
        \label{fig:huber-2d-d05}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \begin{tikzpicture}
             \def\delta{0.5}
            \begin{axis}[
                width=\linewidth,
                height=7cm,
                xlabel={$y$ (Real)},
                ylabel={$\hat{y}$ (Previsto)},
                zlabel={Perda},
                grid=major,
                view={150}{45},
                legend pos=north east,
                title style={font=\bfseries\small},
                label style={font=\small},
                tick label style={font=\scriptsize},
            ]
                \addplot3[
                    mesh,         
                    color=blue,
                    shader=interp, 
                    domain=-5:5,  
                    domain y=-5:5, 
                    samples=15    
                ] 
                { (abs(y-x) <= \delta) ? (0.5*(y-x)^2) : (\delta*(abs(y-x) - 0.5*\delta)) };
            \end{axis}
        \end{tikzpicture}
        \caption{Superfície com $\delta = 0.5$.}
        \label{fig:huber-3d-d05}
    \end{subfigure}

    \vspace{0.5cm} 

    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \begin{tikzpicture}
            \def\delta{4.0}
            \begin{axis}[
                width=\linewidth,
                height=7cm,
                xlabel={Erro ($\hat{y} - y$)},
                ylabel={Perda Calculada},
                axis lines=middle,
                grid=major,
                grid style={dashed, gray!40},
                xmin=-4.5, xmax=4.5,
                ymin=-0.5, ymax=8.5,
                legend pos=north west,
                title style={font=\bfseries\small},
                label style={font=\small},
                tick label style={font=\scriptsize},
            ]
                \addplot[
                    domain=-4.5:4.5, 
                    samples=201, 
                    color=blue, 
                    very thick
                ] {(abs(x) <= \delta) ? (0.5*x^2) : (\delta*(abs(x) - 0.5*\delta))};
            \end{axis}
        \end{tikzpicture}
        \caption{Perda com $\delta = 4.0$.}
        \label{fig:huber-2d-d40}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \begin{tikzpicture}
            \def\delta{4.0} 
            \begin{axis}[
                width=\linewidth,
                height=7cm,
                xlabel={$y$ (Real)},
                ylabel={$\hat{y}$ (Previsto)},
                zlabel={Perda},
                grid=major,
                view={150}{45},
                legend pos=north east,
                title style={font=\bfseries\small},
                label style={font=\small},
                tick label style={font=\scriptsize},
            ]
                \addplot3[
                    mesh,         
                    color=blue,
                    shader=interp, 
                    domain=-5:5,   
                    domain y=-5:5,  
                    samples=15     
                ] 
                { (abs(y-x) <= \delta) ? (0.5*(y-x)^2) : (\delta*(abs(y-x) - 0.5*\delta)) };
            \end{axis}
        \end{tikzpicture}
        \caption{Superfície com $\delta = 4.0$.}
        \label{fig:huber-3d-d40}
    \end{subfigure}
    
    \caption{Visualização da Perda de Huber em duas e três dimensões variando o hiperparâmetro $\delta$.}%
    \label{fig:huber-grid-comparacao}
    \fonte{O autor (2025).}
\end{figure}

\medskip
\begin{center}
 * * *
\end{center}
\medskip

\subsubsection*{Características da perda de Huber}

\begin{itemize}
    \item \textbf{Continuidade, suavidade e diferenciabilidade:} Como dito anteriormente a perda de Huber é uma função de classe $C^1$ (ver Apêndice~\ref{ap:deducoes-huber-loss}). Isso implica em facilidades para encontrar as derivadas parciais e o vetor gradiente dessa função de perda.
    \item \textbf{Convexidade} A perda de Huber é uma função convexa (ver Apêndice~\ref{ap:deducoes-huber-loss}). Ela apresenta o formato característico de um funil. Como resultado, é esperado que a busca por pontos de mínimo utilizando métodos baseados em gradiente seja feita sem muitos problemas de convergência dependendo do tamanho do passo escolhido. Assim como as outras funções, essa convexidade pode ser alterada caso seja utilizada em modelos de aprendizado profundo.
    \item \textbf{Robustez a \textit{outliers}:} A perda de Huber é uma função Lipschitz-contínua (ver Apêndice~\ref{ap:deducoes-huber-loss}). Assim como o erro absoluto médio, o maior valor que a derivada da perda de Huber pode alcançar é 1, não existindo risco de ocorrer explosões do gradiente ao calcular o valor inicial da perda.
    \item \textbf{Hiperparâmetro $\delta$}: A perda de Huber adiciona um novo hiperparâmetro para a construção de um modelo de aprendizado. Ela dá a possibilidade de que seja controlado como os erros são calculados, contudo, $\delta$ não é ajustado automaticamente como os pesos e vieses na retropropagação. Um novo hiperparâmetro, significa ainda mais testes para encontrar seu valor ideal, isso costuma levar tempo, o que pode não ser ideal em certos casos.
\end{itemize}

\medskip
\begin{center}
 * * *
\end{center}
\medskip

\subsubsection*{Gradiente da perda de Huber}

Dado que a perda de Huber é uma função por partes, a suas derivadas devem levar isso em consideração. Para os casos em que $|y - \hat{y}|  \le \delta$, a função que antes comportava de forma quadrática, ao ser derivada, torna-se uma reta afim. Já em $|y - \hat{y}|$, o comportamento linear, vira uma reta constante. Nessa situação, a função depende da derivada da função módulo, que é dado pelo termo $\text{sgn}$. 

A derivada da parcial da perda de Huber em relação à predição é dada pela Equação~\ref{eq:huber-loss-derivada}.

\begin{equacaodestaque}{Derivada parcial da perda de Huber em relação à predição}
    \frac{\partial\Loss_{\delta}}{\partial\hat{y}} (y_j, \hat{y}_j) = 
    \begin{cases} 
        \hat{y} -y & \text{se } | y -\hat{y} | \le\delta\\
        \delta\cdot \text{sgn} (\hat{y} -y) & \text{se } | y -\hat{y} | > \delta\end{cases}%
    \label{eq:huber-loss-derivada}
\end{equacaodestaque}

Uma vantagem da derivada da perda de Huber é que ela permite naturalmente um \textit{gradient clipping} para evitar que o gradiente exploda. Isso acontece, porque a parte que antes era linear, na derivada é constante. O que significa, que independentemente do valor de entrada, se ele for maior que $\delta$, ele não irá crescer. Ele será dado por $\delta \text{sgn} (\hat{y} - y)$, que é uma reta constante.

O comportamento híbrido da perda de Huber está presente em sua derivada, como é apontado na Figura~\ref{eq:huber-loss-derivada}. A derivada da perda de Huber deixa de ser suave, sendo composta de quatro retas, duas afim, e duas retas constantes. 

\begin{figure}[h!]
    \centering 

    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \begin{tikzpicture}
            \begin{axis}[
                % Dimensões ajustadas para caber lado a lado
                width=\linewidth, 
                height=7cm,
                xlabel={Erro ($e = \hat{y} - y$)},
                ylabel={Gradiente ($\frac{\partial L}{\partial \hat{y}}$)},
                axis lines=middle,
                grid=major,
                grid style={dashed, gray!40},
                xmin=-3.5, xmax=3.5,      % Limites do gráfico (além de delta)
                ymin=-1.5, ymax=1.5,      % Limites (um pouco além de -delta e +delta)
                ytick={-1, 0, 1},         % Ticks em -delta, 0, +delta
                xtick={-3, -2, -1, 0, 1, 2, 3}, % Ticks incluindo -delta e +delta
                legend pos=north west,
                title style={font=\bfseries\small},
                label style={font=\small},
                tick label style={font=\scriptsize}
            ]
                % Parte constante negativa (e < -delta)
                \addplot[
                    domain=-3:-1, 
                    samples=10, 
                    color=red, 
                    very thick
                ] {-1};
                \addplot[
                    domain=-1:1, 
                    samples=10, 
                    color=red, 
                    very thick
                ] {x}; % Valor e
                \addplot[
                    domain=1:3, 
                    samples=10, 
                    color=red, 
                    very thick
                ] {1}; 
                \addplot[only marks, mark=*, color=red, mark size=2pt] coordinates {(-1,-1) (1,1)};
            \end{axis}
        \end{tikzpicture}
        \caption{Relação híbrida entre erro e gradiente da perda.}%
        \label{fig:huber-derivada-2d}
    \end{subfigure}
    \hfill 
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \begin{tikzpicture}
            \begin{axis}[
                % Dimensões consistentes com o gráfico (a)
                width=\linewidth,
                height=7cm,
                xlabel={$y$ (Real)},
                ylabel={$\hat{y}$ (Previsto)},
                zlabel={Gradiente ($\frac{\partial L}{\partial \hat{y}}$)},
                grid=major,
                view={150}{45}, % Mesmo ângulo de visão do seu template
                zmin=-1.5, zmax=1.5, % Espelha o eixo Y do gráfico 2D
                ztick={-1, 0, 1}, % Consistente com o 2D
                title style={font=\bfseries\small},
                label style={font=\small},
                tick label style={font=\scriptsize}
            ]
                \addplot3[
                    mesh, 
                    color=red,    % Cor consistente com o gráfico 2D
                    shader=interp, 
                    domain=-5:5,    % Mesmo domínio do seu template
                    domain y=-5:5,  % Mesmo domínio do seu template
                    samples=20,     % Aumentei um pouco os samples para definir melhor a parte linear
                ] { ( abs(y-x) <= 1 ? (y-x) : (1 * sign(y-x)) ) }; 
            \end{axis}
        \end{tikzpicture}
        \caption{Superfície do gradiente no espaço.}%
        \label{fig:huber-derivada-3d}
    \end{subfigure}

    \caption{Visualizações da derivada (gradiente) da função de perda Huber (com $\delta = 1.0$).}%
    \label{fig:huber-derivada}
    \fonte{O autor (2025).}
\end{figure}

\medskip
\begin{center}
 * * *
\end{center}
\medskip

\subsubsection*{Aplicações da perda de Huber em aprendizado de máquina}%
\index{Aplicações práticas! Perda de Huber}

Criada por Huber como intuito de ser utilizada para fazer uma estimação robusta de parâmetros, a perda de Huber provou ser muito mais que uma solução isolada para resolver um problema específico. Com o passar do tempo, foi tornando-se uma excelente alternativa para ser utilizada ao construir modelos de aprendizado de máquina, servindo como uma função de perda para poder medir o erro de um modelo.

Algumas aplicações da perda de Huber são:

\begin{itemize}
    \item \textbf{Previsão de custos (Saúde):} Em \textit{A Huber loss-based super learner with applications to healthcare expenditures}, \textcite{HuberLossSuperLearner} desenvolvem um algoritmo de \textit{ensemble} chamado de \textit{Super Leaner} que tem como um dos objetivos prever de custos para a área da saúde. Os autores usam uma série de outros métodos para compor o \textit{Super Leaner}, como máquinas de vetores de suporte e florestas aleatórias, além disso, para calcular a perda é utilizada a perda de Huber \parencite{HuberLossSuperLearner};
    \item \textbf{Visão computacional de veículos autônomos (Automotiva):} No trabalho \textit{Robust Aleatoric Modeling for Future Vehicle Localization},\textcite{RobustAleatoricModelingVehicleLocalization} apresentam uma rede \textit{feedforward} para previsão robusta para fazer a localização de objetos com intuito de ser utilizada em veículos autônomos. Os autores adotam a perda de Huber como função de perda para o modelo, como justificativa, eles explicam que ela apresenta a capacidade de treinar modelos de forma robusta contra caixas delimitadoras de referência (\textit{ground-truth}) anormais ou discrepantes;
    \item \textbf{Filtragem de tendências (Área):} No artigo \textit{RobustTrend: A Huber Loss with a Combined First and Second Order Difference Regularization for Time Series Trend Filtering}, \textcite{RobustTrendHuberLoss} discutem um novo algoritmo de filtragem de tendências em séries temporais, o objetivo é extrair o sinal de tendência de uma série temporal mesmo quando houver \textit{outliers} ou variações abruptas na tendência. Os autores utilizam a perda de Huber como a função de perda escolhida para ser otimizada \parencite{RobustTrendHuberLoss}. A escolha da perda de Huber é ideal para esse tipo de problema, dado que como os autores explicam, existe a presença de \textit{outliers} nos dados que estão sendo analisados;
    \item \textbf{Aplicação 4 (Área):} No texto \textit{Adaptive Huber Regression} dos pesquisadores \textcite{AdaptiveHuberRegression}, nele, é proposta uma mudança no parâmetro de robustez da perda de Huber, o qual antes era fixo e agora passa a ser variável, considerando o tamanho da amostra, dimensão dos dados e outros parâmetros. Para testar essa nova forma de lidar com a perda de Huber, os autores utilizam o \textit{dataset} NCI-60, o qual possui 60 linhas de células de câncer humano \parencite{AdaptiveHuberRegression}.
\end{itemize}

\medskip
\begin{center}
 * * *
\end{center}
\medskip

A perda de Huber consegue unir o melhor dos dois mundos quando trata-se das perdas \textit{MSE} e \textit{MAE}. É uma função que atua de forma quadrática até certo ponto e depois, de forma linear. Ela garante ser robusta a \textit{outliers}, uma excelente vantagem para essa função. Entretanto, ao utilizar um parâmetro $\delta$ para definir o comportamento da função, a perda de Huber aumenta a complexidade do modelo criado. Esse fato pode ser um empecilho, gerando mais hiperparâmetro para ser otimizado empiricamente, levando mais tempo de pesquisa. Uma solução para manter o comportamento híbrido da perda, mas, ainda diminuir o hiperparâmetro de robustez, é a perda log-cosh.

\subsection{Perda log-cosh (log-cosh loss)}%
\index{Funções de Perda!Perda log-cosh (log-cosh loss)}%
\label{sec:log-cosh-loss}

A \textbf{perda log-cosh} é uma função de perda que vem ganhando popularidade entre os desenvolvedores. Em \textit{Statistical Properties of the log-cosh Loss Function Used in Machine Learning}, \textcite{StatisticalPropetiesLogCosh} explicam que ela aparece em cenários de \textit{autoencoders} variacionais, detecção de câncer, algoritmos de aprendizado baseados em árvores (como o XGBoost) e também em regressão quantílica (\textit{quantile regression}).

A fórmula da perda log-cosh está na Equação~\ref{eq:log-cosh-loss}. Se considerarmos o erro como sendo a diferença do valor real $y_j$ com o valor predito $\hat{y}_j$, a perda log-cosh é dada então pelo logaritmo natural do cosseno hiperbólico do erro.

\begin{equacaodestaque}{Perda log-cosh}
    \Loss_{\text{Log-Cosh}} (y_j, \hat{y}_j) = \sum_{j=1}^{N} \log(\cosh(y_j -\hat{y}_j))%
    \label{eq:log-cosh-loss}
\end{equacaodestaque}

Em que:

\begin{itemize}
    \item $y_j$ representa o valor real da j-ésima amostra;
    \item $\hat{y}_j$ representa o valor predito pelo modelo;
    \item $N$ representa o número de amostras.
\end{itemize}

A elegância da perda log-cosh está na interação dessas duas funções. Para erros pequenos,  $\cosh (e) \approx 1 + e^2/2$, como $\log (1 +x) \approx x$, a função se aproxima de $e^2/2$. Ou seja, para erros pequenos, a função tem um comportamento quadrático, semelhante ao que acontece do erro quadrático médio. Já para erros grandes, $\cosh (e) \approx e^{\|e\|}/2$, ao ser aplicado o logaritmo, o resultado se simplifica para uma função linear, $\log e^{\|e\|} \approx e$. Isso garante uma função robusta a \textit{outliers} assim como o \textit{MAE}. Dessa forma, a perda log-cosh apresenta a mesma natureza híbrida, quadrática-linear, da perda de Huber, sendo uma alternativa que não necessita de hiperparâmetros adicionais. 

Esse comportamento variável para calcular os erros também é evidenciado nas suas representações gráficas, como a Figura~\ref{fig:log-cosh-loss} aponta. Em testes realizados por \textcite{StatisticalPropetiesLogCosh}, a perda log-cosh foi comparada com a perda de Huber, e foi verificado que as estimativas dessas funções bem como os erros padrões apresentam resultados similares.

\begin{figure}[h!]
    \centering 

    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \begin{tikzpicture}
            \begin{axis}[
                % Dimensões ajustadas para caber lado a lado
                width=\linewidth,  
                height=7cm,
                xlabel={Erro ($e = y - \hat{y}$)},
                ylabel={Perda Calculada},
                axis lines=middle,
                grid=major,
                grid style={dashed, gray!40},
                xmin=-4.5, xmax=4.5,        % Limites do seu gráfico
                ymin=-0.5, ymax=4.5,         % Limites do seu gráfico
                legend pos=north west,
                title style={font=\bfseries\small},
                label style={font=\small},
                tick label style={font=\scriptsize}
            ]
                % Gráfico da função ln(cosh(x))
                \addplot[
                    domain=-4:4, 
                    samples=101,
                    color=blue,  
                    very thick
                ] {ln(cosh(x))};
                
                \addlegendentry{$L = \log(\cosh(e))$}
            \end{axis}
        \end{tikzpicture}
        \caption{Vista da função de perda em relação ao erro.}%
        \label{fig:log-cosh-2d}
    \end{subfigure}
    \hfill 
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \begin{tikzpicture}
            \begin{axis}[
                % Dimensões consistentes com o gráfico (a)
                width=\linewidth,
                height=7cm,
                xlabel={$y$ (Real)},
                ylabel={$\hat{y}$ (Previsto)},
                zlabel={Perda},
                grid=major,
                view={150}{45}, % Mesmo ângulo de visão do seu template
                zmin=0, zmax=10.5, % Limite Z ajustado para Log-Cosh com domínio -5:5
                title style={font=\bfseries\small},
                label style={font=\small},
                tick label style={font=\scriptsize}
            ]
                % Gráfico da superfície da Log-Cosh Loss
                \addplot3[
                    mesh,           % Tipo de gráfico: superfície
                    color=blue,
                    shader=interp,  % Suaviza as cores
                    domain=-5:5,    % Domínio de x (y_real)
                    domain y=-5:5,  % Domínio de y (y_previsto)
                    samples=15      % Resolução da malha
                ] { ln(cosh(x - y)) }; % A função Log-Cosh 3D
            \end{axis}
        \end{tikzpicture}
        \caption{Superfície da função de perda.}%
        \label{fig:log-cosh-3d}
    \end{subfigure}

    % --- Legenda e Fonte da Figura Principal ---
    \caption{Visualizações da função de perda log-cosh em duas e em três dimensões.}%
    \label{fig:log-cosh-loss}
    \fonte{O autor (2025).}
\end{figure}

\medskip
\begin{center}
 * * *
\end{center}
\medskip

\subsubsection*{Características da perda log-cosh}

\begin{itemize}
    \item \textbf{Continuidade, suavidade e diferenciabilidade:} A perda log-cosh é uma função de classe $C^{\infty}$, infinitamente diferenciável, (ver Apêndice~\ref{ap:deducoes-log-cosh}). Isso é uma vantagem quando comparada com a perda de Huber, que é de classe $C^1$. Como consequência, a derivada da perda log-cosh é contínua em todo seu domínio. Esse fato pode ser útil caso essa função seja trabalhada em métodos de otimização de segunda ordem, que dependem do cálculo da matriz hessiana.
    \item \textbf{Convexidade:} A perda log-cosh é uma função estritamente convexa (ver Apêndice~\ref{ap:deducoes-log-cosh}). Isso é evidenciado ao calcular a segunda derivada dessa função, que tem como resultado $\frac{1}{\cosh^2(e)}$ que é estritamente positivo. Essa convexidade pode ser afetada ao trabalhar com modelos de aprendizado profundo, bem como as outras funções de perda.
    \item \textbf{Robustez a \textit{outliers}:} A perda log-cosh é Lipschitz-contínua (ver Apêndice~\ref{ap:deducoes-log-cosh}). Isso decorre de sua derivada, $\tanh(x)$, que tem limite em 1. Assim, também é dito que a perda log-cosh é 1-Lipschitz-contínua. Como consequência, essa função é robusta a \textit{outliers} e portanto, ajuda a minimizar problemas de gradientes explosivos.
    \item \textbf{Alternativa para a perda de Huber:} Como dito anteriormente, a perda log-cosh é uma função híbrida assim como a perda de Huber. Ao escolher a perda log-cosh ao invés da perda de Huber para criar um algoritmo estamos priorizando a sua simplicidade, evitando o uso de hiperparâmetros que podem ser desnecessários e de difícil ajuste. Contudo, a perda de Huber ainda é essencial para aqueles casos em que queremos o controle total de como o erro será avaliado pela função de perda. Assim, chega-se em um dilema entre escolher a \textbf{simplicidade (perda log-cosh)} ou o \textbf{controle (perda de Huber)}.
\end{itemize}

\medskip
\begin{center}
 * * *
\end{center}
\medskip

\subsubsection*{Gradiente da perda de log-cosh}

A elegância da perda log-cosh não se restringe à sua função, estando presente também na sua derivada, a qual é dada pela tangente hiperbólica. Para encontrá-la, basta calcular a regra da cadeia dessa função de perda, de modo que seu resultado é

\[
    \frac{d}{de} \log(\cosh(e)) = \frac{1}{\cosh(e)} \cdot \sinh (e) 
\]

que se simplifica para o cálculo da tangente hiperbólica.

Assim, expandindo o cálculo da derivada para o cenário em duas dimensões, chega-se na Equação~\ref{eq:log-cosh-derivada}.

\begin{equacaodestaque}{Derivada parcial da perda log-cosh em relação à predição}
    \frac{\partial\Loss_{\text{log-cosh}}}{\partial\hat{y}_j} = \tanh(\hat{y}_j - y_j)%
    \label{eq:log-cosh-derivada}
\end{equacaodestaque}

Caso você tenha lido o Capítulo~\ref{cap:ativacao-sigmoidais}, pode ter notado que a tangente hiperbólica passa a assumir um novo papel além de uma função de ativação. Neste caso, ela serve para calcular o gradiente inicial.

Os gráficos da derivada parcial da perda log-cosh, vistos na Figura~\ref{eq:log-cosh-derivada}, servem de referência para entender o porquê desta função de perda ser robusta a \textit{outliers}. Como a tangente hiperbólica é limitada em um intervalo $\pm 1$, mesmo que o erro do modelo seja muito grande em termos de valores, a ``correção'' (representada pelo gradiente) não irá aumentar drasticamente. Vale destacar também que como a tangente hiperbólica retorna tanto valores negativos quanto positivos, isso também ajuda a garantir uma convergência mais rápida para os pontos de mínimo\footnote{Esse fato está evidenciado no Capítulo~\ref{cap:ativacao-sigmoidais}.}.

\begin{figure}[h!]
    \centering
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \begin{tikzpicture}
            \begin{axis}[
                width=\linewidth,  
                height=7cm,
                xlabel={Erro ($e = \hat{y} - y$)},
                ylabel={Gradiente ($\frac{\partial L}{\partial \hat{y}}$)},
                axis lines=middle,
                grid=major,
                grid style={dashed, gray!40},
                xmin=-4.5, xmax=4.5,   
                ymin=-1.5, ymax=1.5,       
                ytick={-1, -0.5, 0, 0.5, 1},
                legend pos=south east,
                title style={font=\bfseries\small},
                label style={font=\small},
                tick label style={font=\scriptsize}
            ]
                \addplot[
                    domain=-4:4, 
                    samples=101,
                    color=red, 
                    very thick
                ] {tanh(x)};
                
                \addlegendentry{$L' = \tanh(e)$}
            \end{axis}
        \end{tikzpicture}
        \caption{Relação harmônica entre erro e gradiente da perda.}%
        \label{fig:log-cosh-derivada-2d}
    \end{subfigure}
    \hfill 
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \begin{tikzpicture}
            \begin{axis}[
                width=\linewidth,
                height=7cm,
                xlabel={$y$ (Real)},
                ylabel={$\hat{y}$ (Previsto)},
                zlabel={Gradiente ($\frac{\partial L}{\partial \hat{y}}$)},
                grid=major,
                view={150}{45}, 
                zmin=-1.5, zmax=1.5, 
                ztick={-1, 0, 1},
                title style={font=\bfseries\small},
                label style={font=\small},
                tick label style={font=\scriptsize}
            ]
                \addplot3[
                    mesh,           
                    color=red,    
                    shader=interp,  
                    domain=-5:5,  
                    domain y=-5:5, 
                    samples=15   
                ] { tanh(y - x) };
            \end{axis}
        \end{tikzpicture}
        \caption{Superfície do gradiente no espaço.}%
        \label{fig:log-cosh-derivada-3d}
    \end{subfigure}

    \caption{Visualizações do gradiente da perda log-cosh em relação à predição.}%
    \label{fig:log-cosh-derivada}
    \fonte{O autor (2025).}
\end{figure}

\medskip
\begin{center}
 * * *
\end{center}
\medskip

\subsubsection*{Aplicações da perda log-cosh em aprendizado de máquina}%
\index{Aplicações práticas! Perda log-cosh}

Além das aplicações discutidas no início da seção sobre a perda Log-cosh, ela também está presente em várias outras pesquisas.

Algumas aplicações da perda log-cosh são:

\begin{itemize}
    \item \textbf{Previsão de índices do mercado financeiro (Economia):} Em \textit{Financial Market Forecasting using RNN, LSTM, BiLSTM, GRU and Transformer-Based Deep Learning Algorithms}, \textcite{FinantialMarketForecastingUsingRNN} criam algoritmos de aprendizado de máquina prever índices de ações globais (como o FTSE 100, S\&P 500 e HSI), construindo diferentes modelos, como uma rede neural recorrente. Eles utilizam diversas funções como métricas para avaliar os modelos construídos, Log-Cosh é uma dessas funções, junto com o erro absoluto médio (\textit{MAE}), erro quadrático médio (\textit{MSE}), a raiz do erro quadrático médio (\textit{RMSE}) e também a perda de Huber \parencite{FinantialMarketForecastingUsingRNN};
    \item \textbf{Predição de bioatividade em moléculas (Farmácia):} Já em \textit{Siamese Recurrent Neural Network with a Self-Attention Mechanism for Bioactivity Prediction} \textcite{SiameseRecurrentNeuralNetwork} apresentam uma rede neural recorrente (\textit{RNN}) siamesa com o intuito de fazer a predição da bioatividade de pequenas moléculas, um procedimento muito útil na descoberta de remédios. Os autores argumentam que a perda Log-Cosh foi uma função de perda ideal para ser aplicada nesse problema, pois apresentou o melhor desempenho das perdas que foram testadas: perda constrativa (\textit{constractive loss}), perda de Huber, perda L1 e perda L2 \parencite{SiameseRecurrentNeuralNetwork};
    \item \textbf{Aplicação 3 (Área):} No trabalho \textit{An Effective Method for Detecting Unknown Types of Attacks Based on Log-Cosh Variational Autoencoder}, \textcite{AnEffectiveMethodForDetectingUnknowTypes} propõem um modelo de \textit{deep learning} chamado de \textit{LVAE (Log-Cosh Variational Autoencoder)} com intuito detectar tipos desconhecidos de ataques em redes. A perda log-cosh é utilizada no modelo como a função de perda de reconstrução dentro do modelo \textit{variational autoencoder} \parencite{AnEffectiveMethodForDetectingUnknowTypes}.
\end{itemize}

\medskip
\begin{center}
 * * *
\end{center}
\medskip

Todas as funções de perda vistas até o momento calculam o erro de forma absoluta. Elas não lidam com o erro de forma relativa. Como consequência, um erro de R\$ $50,00$ em uma conta de padaria tem o mesmo grau que um erro da mesma magnitude para validar o aluguel de uma casa. Para os casos em que deve-se considerar o erro de forma relativa, cabe analisar outras funções, como o erro quadrático logarítmico médio ou a perda quantílica.

\section{Lidando com a escala: foco no erro relativo}

\subsubsection*{O problema dos imóveis}

Considere que existem duas situações que está sendo previsto os valores de imóveis:

\begin{itemize}
    \item Cenário A: o modelo previu que uma casa vale 50.000 R\$. Enquanto no rótulo está que ela vale 100.000 R\$;
    \item Cenário B: o modelo previu que uma casa vale 950.000 R\$. Enquanto no rótulo está que ela vale 1.000.000 R\$.
\end{itemize}

Utilizando o \textit{MSE} para calcular a perda dessas funções encontramos que o erro será de $50.000$ e a perda de $250.00.000$, independentemente dos cenários. Para o cenário B, a previsão do modelo não foi tão ruim, foi uma previsão realista. Contudo, o modelo do cenário A previu que a casa valia apenas a metade do seu valor real, ele fez uma previsão subestimada.

Neste caso, avaliar os modelos com uma função de perda que calcula o erro de forma absoluta (como o \textit{MSE} e o \textit{MAE}), pode não ser ideal. Para evitar esses problemas de subestimação, uma solução é o erro quadrático médio logarítmico.

\subsection{Erro quadrático médio logarítmico (MSLE)}%
\index{Funções de Perda!Erro quadrático médio logarítimico (MSLE)}%

O \textbf{erro quadrático médio logarítmico}, também chamado de \textbf{\textit{mean squared logarithmic error}} (\textbf{\textit{MSLE}}), é calculado pela Equação~\ref{eq:msle-loss}. Ele calcula o erro como sendo a diferença dos logaritmos naturais do valor real $y_j$ com a predição $\hat{y}_j$ e depois faz a média dos erros das diferentes previsões.

\begin{equacaodestaque}{Erro quadrático médio logarítmico}
    \Loss_{\text{MSLE}} (y_j, \hat{y}_j) = \frac{1}{N} \sum_{j=1}^{N} (\ln(y_j + 1) -\ln(\hat{y}_j + 1))^2%
    \label{eq:msle-loss}
\end{equacaodestaque}

Em que:

\begin{itemize}
    \item $y_j$ representa o valor real da j-ésima amostra;
    \item $\hat{y}_j$ representa o valor predito pelo modelo;
    \item $N$ representa o número de amostras.
\end{itemize}

O valor $1$ que é somado junto com os valores de entrada no logaritmo serve como forma de evitar que seja calculado o logaritmo de zero, que não existe no conjunto dos reais. 

A Figura~\ref{fig:msle-loss} evidencia a natureza assimétrica do erro quadrático logarítmico. Para as situações em que a previsão $\hat{y}_j$ tem um valor muito pequeno quando comparada ao valor real $y_j$ a distância entre esses dois pontos é consideravelmente maior devido ao comportamento dos logaritmos. Dessa forma, para subestimações, o \textit{MSLE} gera erros maiores. Isso é útil para resolver o problema das contas de padaria e também de preços de imóveis vistos anteriormente. 

\begin{figure}[h!]
    \centering 

    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \begin{tikzpicture}
            \begin{axis}[
                % Dimensões ajustadas para caber lado a lado
                width=\linewidth,  
                height=7cm,
                xlabel={Valor Previsto ($\hat{y}$)},
                ylabel={Perda Calculada},
                axis lines=middle,
                grid=major,
                grid style={dashed, gray!40},
                xmin=-1, xmax=25,        % Limites do seu gráfico
                ymin=-0.5, ymax=6,         % Limites do seu gráfico
                legend pos=north west,
                title style={font=\bfseries\small},
                label style={font=\small},
                tick label style={font=\scriptsize}
            ]
                % Gráfico da função (ln(11) - ln(x+1))^2
                \addplot[
                    domain=0:25, 
                    samples=101,
                    color=blue,  
                    very thick
                ] {(ln(10+1) - ln(x+1))^2};
                
                \addlegendentry{$L(\hat{y} | y=10)$}

                % Linha vertical para marcar o valor real
                \draw[dashed, gray] (axis cs:10, 0) -- (axis cs:10, 6);
                \node[above, gray!80, font=\tiny] at (axis cs:10, 6) {Valor Real ($y=10$)};
            \end{axis}
        \end{tikzpicture}
        \caption{Vista da função de perda em relação ao erro.}%
        \label{fig:msle-2d}
    \end{subfigure}
    \hfill 
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \begin{tikzpicture}
            \begin{axis}[
                % Dimensões consistentes com o gráfico (a)
                width=\linewidth,
                height=7cm,
                xlabel={$y$ (Real)},
                ylabel={$\hat{y}$ (Previsto)},
                zlabel={Perda},
                grid=major,
                view={150}{45}, % Mesmo ângulo de visão do seu template
                zmin=0, zmax=12, % Ajustado para (ln(26)-ln(1))^2
                title style={font=\bfseries\small},
                label style={font=\small},
                tick label style={font=\scriptsize}
            ]
                % Gráfico da superfície da MSLE
                \addplot3[
                    mesh,           % Tipo de gráfico: superfície
                    color=blue,
                    shader=interp,  % Suaviza as cores
                    domain=-0:25,    % Domínio de x (y_real)
                    domain y=-0:25,  % Domínio de y (y_previsto)
                    samples=15      % Resolução da malha
                ] { (ln(x+1) - ln(y+1))^2 }; % A função MSLE 3D
            \end{axis}
        \end{tikzpicture}
        \caption{Superfície da função de perda.}%
        \label{fig:msle-3d}
    \end{subfigure}

    \caption{Visualizações da função de perda erro quadrático médio logarítmico.}%
    \label{fig:msle-loss}
    \fonte{O autor (2025).}
\end{figure}

Essa propriedade assimétrica dos logaritmos também é explorada ao descrever o comportamento da entropia cruzada binária e funções semelhantes (vistas com mais detalhes do Capítulo~\ref{cap:perda-classificacao}). Neste caso, a forma com que a função utiliza os logaritmos faz com que sejam punidas em maior peso as previsões confiantes que estão erradas.

\medskip
\begin{center}
 * * *
\end{center}
\medskip

\subsubsection*{Características do erro quadrático médio logarítmico}

\begin{itemize}
    \item \textbf{Continuidade, suavidade e diferenciabilidade:} O \textit{MSLE} é uma função de classe $C^{\infty}$ (infinitamente diferenciável) (ver Apêndice~\ref{ap:deducoes-msle}). Isso contribui para uma função de perda que pode ser facilmente utilizada em conjunto com otimizadores baseados em gradiente, dado que não serão encontrados problemas de continuidade ao derivar essa função.
    \item \textbf{Convexidade:} A função \textit{MSLE} é convexa em relação à predição transformada $z = \log(1+\hat{y})$, mas não é globalmente convexa em relação a $\hat{y}$ (ver Apêndice~\ref{ap:deducoes-msle}). 
    \item \textbf{Robustez:} A função \textit{MSLE} é Lipschitz-contínua, portanto é robusta a \textit{outliers} (ver Apêndice~\ref{ap:deducoes-msle}). Diferente do erro absoluto médio, que quando o valor do erro ``explode'' para infinito o seu gradiente será 1, no \textit{MSLE} o seu gradiente tende a 0. Como resultado, para erros muito grandes o erro logarítmico médio gera uma correção irrisória, o que pode não ser útil em certos casos.
    \item \textbf{Foco no erro relativo (percentual):} Quando o \textit{MSLE} é utilizado como função de perda, diferente das outras vistas até agora que medem o erro absoluto, o erro quadrático logarítmico médio mede o erro percentual/relativo. Isso acontece por conta dos uso dos logaritmos dessa função, que são responsáveis por comprimir a escala dos dados. Portanto, erros em que o valor predito é 50\% menor que o real são mais penalizados em que cenários nos quais o valor predito é 5\% menor;
    \item \textbf{Tendência de penalizar subestimações:} Como aponta a Figura~\ref{fig:msle-loss}, o \textit{MSLE} não forma uma curva convexa simétrica verticalmente, conforme os valores vão diminuindo, a perda aumenta consideravelmente. Enquanto isso, conforme os valores aumentam, a perda também aumenta, mas não de forma tão agressiva quanto no sentido inverso. Isso significa que quanto maior for a diferença entre o valor previsto $\hat{y}_j$ e o valor real $y_j$, existe uma tendência de que a perda será maior;
    \item \textbf{Restrição de domínio (não-negatividade):} Como os logaritmos são indeterminados para valores nulos ou negativos, o \textit{MSLE} exige que os valores de entrada sejam positivos ($y_j > 0$ e $\hat{y}_j > 0$). Caso os conjunto de dados contenha valores negativos (como lucro/prejuízo ou temperaturas), o \textit{MSLE} irá causar erros matemáticos. Para contornar esse problema, deve-se aplicar uma transformação nos dados, como somar uma constante para deslocar os valores negativos para positivos, ou ser utilizada outra função de perda.
\end{itemize}

\medskip
\begin{center}
 * * *
\end{center}
\medskip

\subsubsection*{Gradiente do erro quadrático médio logarítmico}

Na equação~\ref{eq:msle-derivada} está a derivada do erro quadrático médio logarítmico em relação os valores preditos pelo modelo $\hat{y}_j$.

\begin{equacaodestaque}{Derivada parcial do erro quadrático médio logarítmico em relação à predição}
    \frac{\partial\Loss_{\text{MSLE}}}{\partial\hat{y}_j} (y_j, \hat{y}_j) = -\frac{2}{N} \cdot \frac{\ln(y_j + 1) -\ln(\hat{y}_j + 1)}{\hat{y}_j + 1}%
    \label{eq:msle-derivada}
\end{equacaodestaque}
 
O comportamento assimétrico do \textit{MSLE} também está presente nos gráficos da sua derivada vistos na Figura~\ref{fig:msle-derivada}. Eles mostram de forma intuitiva o tamanho da correção que será feita conforme o modelo vai errando durante suas predições. O canto esquerdo do gráfico da superfície da derivada mostra uma situação em que o modelo subestima os valores reais. Nesta parte, os valores do gradiente começam a subir de uma forma que lembra uma curva exponencial. 

\begin{figure}[h!]
    \centering 

    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \begin{tikzpicture}
            \begin{axis}[
                width=\linewidth,  
                height=7cm,
                xlabel={Valor Previsto ($\hat{y}$)},
                ylabel={Gradiente ($\frac{\partial L}{\partial \hat{y}}$)},
                axis lines=middle,
                grid=major,
                grid style={dashed, gray!40},
                xmin=-1, xmax=25, 
                ymin=-1, ymax=5,      
                legend pos=north east,
                title style={font=\bfseries\small},
                label style={font=\small},
                tick label style={font=\scriptsize}
            ]
                
                \addplot[
                    domain=0:25, 
                    samples=101,
                    color=red, 
                    very thick
                ] {2 * (ln(10+1) - ln(x+1)) / (x+1)}; 
                
                \addlegendentry{$L' \text{ para } y=10$}

                \draw[dashed, gray] (axis cs:10, -1) -- (axis cs:10, 5);
                \node[above, gray!80, font=\tiny] at (axis cs:10, 5) {Valor Real};
            \end{axis}
        \end{tikzpicture}
        \caption{Relação assimétrica entre erro e gradiente da perda.}%
        \label{fig:msle-derivada-2d}
    \end{subfigure}
    \hfill 
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \begin{tikzpicture}
            \begin{axis}[
                width=\linewidth,
                height=7cm,
                xlabel={$y$ (Real)},
                ylabel={$\hat{y}$ (Previsto)},
                zlabel={Gradiente ($\frac{\partial L}{\partial \hat{y}}$)},
                grid=major,
                view={150}{45},
                zmin=-1.5, zmax=6.5,
                title style={font=\bfseries\small},
                label style={font=\small},
                tick label style={font=\scriptsize}
            ]
                \addplot3[
                    mesh,           
                    color=red,  
                    shader=interp,  
                    domain=0:25,  
                    domain y=0:25,  
                    samples=15  
                ] { 2 * (ln(x+1) - ln(y+1)) / (y+1) };
            \end{axis}
        \end{tikzpicture}
        \caption{Superfície do gradiente no espaço.}%
        \label{fig:msle-derivada-3d}
    \end{subfigure}

    \caption{Visualizações do gradiente da perda log-cosh em relação à predição.}%
    \label{fig:msle-derivada}
    \fonte{O autor (2025).}
\end{figure}

\medskip
\begin{center}
 * * *
\end{center}
\medskip

\subsubsection*{O problema dos imóveis com o MSLE}

Voltando para o exemplo anterior dos modelos prevendo preços de imóveis. Cabe discutir como o \textit{MSLE} se compara ao erro quadrático médio, e como ele pode ser uma alternativa para função de perda em cenários em que é preciso garantir que as subestimações sejam mais penalizadas.

Com esse intuito, a Tabela~\ref{tab:comparativo-mse-msle} compara os dois cenários vistos na seção anterior, e mostra qual foi o valor do erro calculado para essas situações.

\begin{table}[htbp]
    \centering
    \begin{threeparttable}
        \caption{Comparativo das funções de perda \textit{MSE} e \textit{MSLE}}%
        \label{tab:comparativo-mse-msle}
        \begin{tabular}{l c c c c }
            \toprule
            Cenário & $y_j$ (real) & $\hat{y}_j$ (predito) & \textit{MSE} & \textit{MSLE} \\
            \midrule
            Cenário A & $100.000$ & $50.000$ & $2.500.000.000$ & $0,4804$ \\
            Cenário B & $1.000.000$ & $950.000$ & $2.500.000.000$ & $0,0026$ \\
            \bottomrule
        \end{tabular}
        
        \begin{tablenotes}[para]
            \small
            \item[] Fonte: O autor (2025).
        \end{tablenotes}

    \end{threeparttable}
\end{table}

O erro quadrático médio considera a grandeza dos valores, nos dois cenários o erro é o mesmo. Consequentemente a atualização do gradiente, para o modelo os erros são de mesma magnitude. Contudo, olhando agora para os valores do \textit{MSLE}, o cenário A, em que o modelo subestimou o valor do imóvel, é cerca de 185 vezes pior que o do cenário B. Neste caso, como no cenário A o erro foi maior, as atualizações nos parâmetros do modelo também serão maiores, enquanto no cenário B ainda irá ocorrer atualizações, mas como apontado, os valores dos parâmetros irão ter uma variação menor de uma época para a próxima.

\medskip
\begin{center}
 * * *
\end{center}
\medskip

Do \textit{MSLE} é possível derivar a métrica raiz do erro quadrático médio logarítmico (\textit{RMSLE}) que também serve para avaliar problemas de predição. Neste caso, o \textit{RMSLE} é dado pelo cálculo da raiz quadrada da função de perda \textit{RMSE}.

\begin{equacaodestaque}{Raiz do erro quadrático médio logarítmico}
    \textit{RMSLE} (y_j, \hat{y}_j) = \sqrt{\frac{1}{N} \sum_{j=1}^{N} (\log(y_j + 1) -\log(\hat{y}_j + 1))^2}%
    \label{eq:rmsle-metric}
\end{equacaodestaque}

Em que:

\begin{itemize}
    \item $y_j$ representa o valor real da j-ésima amostra;
    \item $\hat{y}_j$ representa o valor predito pelo modelo;
    \item $N$ representa o número de amostras.
\end{itemize}

Ao calcular a raiz quadrada do erro quadrático médio logarítmico tem-se uma noção melhor da magnitude dos erros, dado que eles são mascarados ao serem elevados ao quadrado pela expressão do \textit{RMSE}. Diante disso, a equação da raiz do erro quadrático médio logarítmico consegue melhorar a interpretabilidade dos erros, servindo como uma métrica que pode ser aplicada para problemas em que as subestimações devem ser mais penalizadas.

\medskip
\begin{center}
 * * *
\end{center}
\medskip

\subsubsection*{Aplicações do erro quadrático logarítmico médio em aprendizado de máquina}%
\index{Aplicações práticas! Erro quadrático logarítimico médio}

Visto o erro quadrático logarítmico médio e também o \textit{RMSLE}, agora será discutido alguns cenários em que essa função pode ser aplicada para resolver problemas de regressão. Essas duas funções podem aparecer tanto como métricas, avaliando o desempenho do modelo, mas também podem atuar como uma função de perda, servindo de guia para o cálculo do gradiente e consequentemente o ajuste dos parâmetros da rede.

Dito isso, vale destacar os trabalhos:

\begin{itemize}
    \item \textbf{Predição do preço de casas (Mercado imobiliário):} Em \textit{A Net Over Your Head: A Neural Network Approach to Home Price Predictions}, \textcite{SunKim-NetOverYourHead} discutem formas de prever o preço de casas, com esse objetivo, eles desenvolvem uma rede neural, indo contra a tendência de utilizar técnicas mais tradicionais, como regressão linear ou florestas aleatórias. Eles utilizam como métrica o \textit{RMSLE}, como justificativa, os autores explicam que o \textit{dataset} que está sendo utilizado apresenta casas que têm ordem de magnitude de diferença de preço, e com isso uma previsão ruim para uma casa de alto valor terá mais peso que uma previsão ruim para uma casa de baixo valor; ao usar o \textit{RMSLE} é possível capturar as ordens de magnitude dos preços das casas, algo que não acontece com o \textit{RMSE} \parencite{SunKim-NetOverYourHead};
    \item \textbf{Predição no tempo de internação de pacientes (Saúde):} Além disso, no trabalho \textit{Predicting Hospital Length of Stay of Patients Leaving the Emergency Department}, \textcite{Winter2023PredictingLOS} analisam maneiras de prever o tempo de internação de um paciente qualquer após receber alta no pronto-socorro e sua transferência para a próxima unidade hospitalar. Com esse objetivo em mente, os autores usam um modelo de regressão de \textit{gradient boosting} com arquitetura \textit{CatBoost}, para a função de perda, os autores escolhem duas, a primeira sendo o \textit{RMSE} e a segunda o \textit{RMSLE}, como justificativa, os autores apontam que o \textit{RMSLE} penaliza erros proporcionais e é menos afetado por \textit{outliers};
    \item \textbf{Previsão de séries temporais (Área):} Por fim, no texto \textit{A Comprehensive Survey of Regression Based Loss Functions for Time Series Forecasting}, \textcite{Jadon2022ComprehensiveSurvey} analisam uma série de funções de perda que são comumente utilizadas na previsão de séries temporais, sendo uma delas o \textit{MSLE}. No trabalho, os autores apontam que o erro quadrático logaritmo médio reduz o efeito punitivo de diferenças significativas em grandes valores previsos; sendo apropriado quando o modelo prevê quantidades não escalonadas diretamente \parencite{Jadon2022ComprehensiveSurvey}. Além disso, \textcite{Jadon2022ComprehensiveSurvey} analisam também o \textit{RMSLE}, explicando que ele pode ser uma função de perda para ser utilizada em cenários em que a subestimação dos valores reais não é aceitável, mas a superestimações do modelo não são consideradas um problema.
\end{itemize}

\medskip
\begin{center}
 * * *
\end{center}
\medskip

O erro quadrático médio logarítmico médio penaliza fortemente as subestimações feitas pelo modelo. Entretanto, pode ser necessário uma função que tenha a característica contrária: penalizar em maior grau as superestimações. Dessa forma, uma solução é utilizar a perda quantílica.

\section{Mudando o objetivo da previsão: além da média}

Até agora todas as funções de perda vistas calculavam o erro para um conjunto $m$ de instâncias, e a partir disso aplicavam o cálculo da média dos erros. Ao calcular a média dos erros, está sendo presumido que a tendência central é a característica mais informativa daquela distribuição de dados, seguindo a clássica distribuição Gaussiana. Contudo, e quando isso não for a realidade? E quando a caraterística mais comum estiver deslocada mais para à esquerda (como na Figura~\ref{fig:dist-skew-esquerda}), ou mais para à direita da distribuição (como na Figura~\ref{fig:dist-skew-direita})?

\begin{figure}[h!]
    \centering 

    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \begin{tikzpicture}
            \begin{axis}[
                width=\linewidth,
                height=7cm,
                xlabel={Valor},
                ylabel={Densidade},
                axis lines=middle,
                grid=major,
                grid style={dashed, gray!40},
                xmin=-5, xmax=10,
                ymin=0, ymax=0.5,
                legend pos=north west,
                title style={font=\bfseries\small},
                label style={font=\small},
                tick label style={font=\scriptsize},
                title={Distribuição Assimétrica à Direita}
            ]
                
                % Curva "desenhada" com smooth
                \addplot[
                    blue, thick, fill=blue!10,
                    smooth, tension=0.7 % 'tension' controla a suavidade
                ] coordinates {
                    (-4, 0)
                    (0, 0.1)
                    (1.5, 0.4)  % Pico (Moda)
                    (3, 0.25)
                    (5, 0.1)
                    (8, 0.02)
                    (10, 0)
                };

                % Mediana (mais perto do pico)
                \pgfmathsetmacro{\medianpos}{2.2} 
                \addplot[dashed, color=green, thick] coordinates {(\medianpos,0) (\medianpos,0.38)};
                \node[above, font=\tiny, green] at (axis cs:\medianpos,0.38) {Mediana};

                % Média (puxada pela cauda)
                \pgfmathsetmacro{\meanpos}{3.1} 
                \addplot[dashed, color=red, thick] coordinates {(\meanpos,0) (\meanpos,0.38)};
                \node[above, font=\tiny, red] at (axis cs:\meanpos,0.38) {Média};

            \end{axis}
        \end{tikzpicture}
        \caption{Assimétrica à direita (skew positivo).}%
        \label{fig:dist-skew-direita}
    \end{subfigure}
    \hfill 
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \begin{tikzpicture}
            \begin{axis}[
                width=\linewidth,
                height=7cm,
                xlabel={Valor},
                ylabel={Densidade},
                axis lines=middle,
                grid=major,
                grid style={dashed, gray!40},
                xmin=-10, xmax=5,
                ymin=0, ymax=0.5,
                legend pos=north east,
                title style={font=\bfseries\small},
                label style={font=\small},
                tick label style={font=\scriptsize},
                title={Distribuição Assimétrica à Esquerda}
            ]
                
                % Curva "desenhada" com smooth
                \addplot[
                    blue, thick, fill=blue!10,
                    smooth, tension=0.7
                ] coordinates {
                    (-10, 0)
                    (-8, 0.02)
                    (-5, 0.1)
                    (-3, 0.25)
                    (-1.5, 0.4) % Pico (Moda)
                    (0, 0.1)
                    (4, 0)
                };

                % Mediana (mais perto do pico)
                \pgfmathsetmacro{\medianneg}{-2.2} 
                \addplot[dashed, color=green, thick] coordinates {(\medianneg,0) (\medianneg,0.38)};
                \node[above, font=\tiny, green] at (axis cs:\medianneg,0.38) {Mediana};

                % Média (puxada pela cauda)
                \pgfmathsetmacro{\meannneg}{-3.1} 
                \addplot[dashed, color=red, thick] coordinates {(\meannneg,0) (\meannneg,0.38)};
                \node[above, font=\tiny, red] at (axis cs:\meannneg,0.38) {Média};

            \end{axis}
        \end{tikzpicture}
        \caption{Assimétrica à esquerda (skew negativo).}%
        \label{fig:dist-skew-esquerda}
    \end{subfigure}

    \caption{Exemplos de distribuições assimétricas (desenhadas) onde a média (vermelho) e a mediana (verde) divergem, justificando o uso da Perda Quantílica.}%
    \label{fig:distribuicoes-assimetricas}
    \fonte{O autor (2025).}
\end{figure}

A perda quantílica consegue lidar com as situações em que a média e a medina são divergentes. Ela foca em estudar como a perda acontece em diferentes quantis da distribuição. Ao utilizá-la, é definido o quantil que será analisado, escolhendo se ela irá funcionar de forma assimétrica, ou de forma parecida com o \textit{MAE}.

A $\epsilon$-insensível também é outra função de perda que não faz o cálculo da média dos erros. Ela é uma função essencial para a tarefas de regressão para máquinas de vetores de suporte. Ao utilizá-la, é definido uma ``margem de tolerância'' em que caso o erro esteja dentro dessa média, a perda não irá penalizar o modelo.

\subsection{Perda quantílica (quantile loss)}%
\index{Funções de Perda!Perda quantílica (\textit{quantile loss})}

No artigo \textit{Regression Quantiles} dos autores \textcite{regression-quantiles} analisaram uma nova técnica de regressão que se baseava nos quantis de uma distribuição. Como motivação, os autores discutem a ideia de que ``todos acreditam a lei de Gauss para os erros, porque eles acreditam que é um teorema matemático, e os matemáticos acreditam porque é um fato experimental'' \parencite{regression-quantiles}. Contudo, isso não é uma verdade absoluta, existem casos, como os vistos na introdução dessa seção, que fogem da distribuição normal. Como consequência, técnicas mais conhecidas, como o \textit{MSE}, para o cálculo dos erros com base na média, não são ideais para servir de guia para a otimização do modelo.

Tendo isso em mente, os pesquisadores apresentam uma forma de minimizar a soma de perdas assimétricas, eles então definem a Equação~\ref{eq:soma-das-perdas-assimetricas}\footnote{Caso você leitor decida ler o artigo original dos pesquisadores, você verá um conjunto diferente de notações utilizadas, isso acontece pois neste livro elas foram adaptadas para condizer com as notações que já estavam sendo utilizadas anteriormente nos outros capítulos}.

\begin{equation}
    \Loss(w; \tau, y, X) = \sum_{j \text{ t.q. } y_j > \hat{y}_j} \tau |y_j -\hat{y}_j| + \sum_{j \text{ t.q. } y_j < \hat{y}_j} (1 -\tau) |y_j -\hat{y}_j|
    \label{eq:soma-das-perdas-assimetricas}
\end{equation}

Em que:

\begin{itemize}
    \item $\tau$ representa o quantil;
    \item $w$ representa os parâmetros do modelo, neste caso os pesos;
    \item $\hat{y}_j$ representa a predição do modelo.
\end{itemize}

A partir da Equação~\ref{eq:soma-das-perdas-assimetricas}, é possível derivá-la em dois casos distintos. O caso em que o valor real é maior que a previsão $(y_j > \hat{y}_j)$, ou seja, ocorre uma subestimação, é gerado um erro positivo. Assim, a perda é proporcional a $\tau$. Quando o valor real é menor que a previsão $(y_j < \hat{y}_j)$, o modelo superestimou o valor real, é aplicado o peso $(1 - \tau)$. Desses dois cenários, deriva-se a definição por partes da \textbf{perda quantílica}, também conhecida como \textbf{\textit{quantile loss}}, dada pela Equação~\ref{eq:quantile-loss}. Neste caso, está sendo considerado que a situação $\hat{y} = y$ faz parte do primeiro caso estudado.

\begin{equacaodestaque}{Perda quantílica}
    \Loss_{\tau} (y_j, \hat{y}_j) = 
    \begin{cases} 
        \tau(y_j -\hat{y}_j) & \text{se } y_j \ge \hat{y}_j\\
        (1 -\tau) (\hat{y}_j -y_j) & \text{se } y_j < \hat{y}_j\end{cases}%
    \label{eq:quantile-loss}
\end{equacaodestaque}

Em que:

\begin{itemize}
    \item $y_j$ representa o valor real da j-ésima amostra;
    \item $\hat{y}_j$ representa o valor predito pelo modelo;
    \item $\tau$ representa o quantil.
\end{itemize}

Existem também uma versão compacta da perda quantílica, presente na Equação~\ref{eq:quantile-loss-com-max}, ela utiliza a função $\max$ para definir a função por partes em apenas uma linha.

\begin{equacaodestaque}{Perda quantílica com $\max$}
    \Loss_{\tau} (e)= \max(\tau e, (\tau-1 )e)%
    \label{eq:quantile-loss-com-max}
\end{equacaodestaque}

Considerando que $e = y - \hat{y}$, ou seja, o cálculo do erro.

A perda quantílica com $\tau = 0,5$ é retratada na Figura~\ref{fig:quantile-loss-tau05}. Os gráficos são semelhantes ao erro absoluto médio, porque o \textit{MAE} é uma função que analisa justamente o quantil 0,5. Assim, não faz muito sentido utilizar a perda quantílica para esse cenário em específico, considerando que já existe uma função com essa finalidade.

% --- FIGURA 1: FOCO NO TAU = 0.5 (MAE) ---
\begin{figure}[h!]
    \centering 

    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \begin{tikzpicture}
            \begin{axis}[
                width=\linewidth,  
                height=7cm,
                xlabel={Erro ($e = y - \hat{y}$)},
                ylabel={Perda Calculada},
                axis lines=middle,
                grid=major,
                grid style={dashed, gray!40},
                xmin=-4.5, xmax=4.5,
                ymin=-0.5, ymax=3.5,
                legend pos=north west,
                title style={font=\bfseries\small},
                label style={font=\small},
                tick label style={font=\scriptsize},
            ]
                \addplot[domain=-4:4, samples=5, color=blue, very thick] {0.5*abs(x)};
            \end{axis}
        \end{tikzpicture}
        \caption{Vista da função de perda em relação ao erro.}%
        \label{fig:quantile-2d-tau05}
    \end{subfigure}
    \hfill 
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \begin{tikzpicture}
            \begin{axis}[
                width=\linewidth,
                height=7cm,
                xlabel={$y$ (Real)},
                ylabel={$\hat{y}$ (Previsto)},
                zlabel={Perda},
                grid=major,
                view={150}{45},
                zmin=0, zmax=10.5,
                legend pos=north east,
                title style={font=\bfseries\small},
                label style={font=\small},
                tick label style={font=\scriptsize},
            ]
                \addplot3[
                    mesh,           % Tipo de gráfico: superfície
                    color=blue,
                    shader=interp,  % Suaviza as cores
                    domain=-5:5,    % Domínio de x (y_real)
                    domain y=-5:5,  % Domínio de y (y_previsto)
                    samples=15      % Resolução da malha
                ] { 0.5*abs(x - y) };
            \end{axis}
        \end{tikzpicture}
        \caption{Superfície da função de perda.}%
        \label{fig:quantile-3d-tau05}
    \end{subfigure}

    \caption{Visualizações da função de perda quantílica em duas e em três dimensões (com $\tau=0.5$ (\textit{MAE})).}%
    \label{fig:quantile-loss-tau05}
    \fonte{O autor (2025).}
\end{figure}

O diferencial dessa função é quando ela precisa ser utilizada em cenários em que a distribuição de dados é irregular, e não segue a distribuição normal. Essa função tem a propriedade de se ajustar para penalizar de forma assimétrica os erros que um modelo comete. 

A perda quantílica com hiperparâmetro $\tau = 0,9$, presente na Figura~\ref{fig:quantile-loss-grid}, penaliza fortemente as subestimações feitas pelo modelo. Tem se essa noção inicial devido à sua assimetria, como os gráficos apontam. Quando está sendo analisado o 90º percentil, significa que queremos uma previsão $\hat{y}_j$ que seja maior que 90\% dos valores reais $y_j$. Se o modelo subestima, ou seja $y_j > \hat{y}_j$, ele está falhando o seu objetivo, e terá um erro bem maior.

Um cenário contrário, em que a perda é ajustada para penalizar as superestimações está na Figura~\ref{fig:quantile-loss-grid}. Neste caso, $\tau = 0,1$, ou seja, queremos uma previsão que esteja abaixo dos 10\% dos valores reais. Essa situação é ideal para treinar o modelo a não superestimar os valores, dado que ele será fortemente penalizado se ele fizer uma predição em que $y_j < \hat{y}_j$.

\begin{figure}[h!]
    \centering

    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \begin{tikzpicture}
            \begin{axis}[
                width=\linewidth,  
                height=7cm,
                xlabel={Erro ($e = y - \hat{y}$)},
                ylabel={Perda Calculada},
                axis lines=middle,
                grid=major,
                grid style={dashed, gray!40},
                xmin=-4.5, xmax=4.5,
                ymin=-0.5, ymax=3.5,
                legend pos=north west,
                title style={font=\bfseries\small},
                label style={font=\small},
                tick label style={font=\scriptsize},
            ]
                \addplot[domain=-4:4, samples=5, color=blue, very thick] {(x >= 0) ? (0.9*x) : ((1-0.9)*(-x))};
            \end{axis}
        \end{tikzpicture}
        \caption{Vista da função de perda em relação ao erro ($\tau=0.9$).}%
        \label{fig:quantile-2d-tau09}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \begin{tikzpicture}
            \begin{axis}[
                width=\linewidth,
                height=7cm,
                xlabel={$y$ (Real)},
                ylabel={$\hat{y}$ (Previsto)},
                zlabel={Perda},
                grid=major,
                view={150}{45},
                zmin=0, zmax=10.5,
                legend pos=north east,
                title style={font=\bfseries\small},
                label style={font=\small},
                tick label style={font=\scriptsize},
            ]
                \addplot3[
                    mesh,          
                    color=blue,
                    shader=interp, 
                    domain=-5:5,   
                    domain y=-5:5,  
                    samples=15     
                ] { (x - y >= 0) ? (0.9*(x - y)) : ((1-0.9)*(-(x - y))) };
            \end{axis}
        \end{tikzpicture}
        \caption{Superfície da função de perda ($\tau=0.9$).}%
        \label{fig:quantile-3d-tau09}
    \end{subfigure}

    \vspace{0.5cm} 

    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \begin{tikzpicture}
            \begin{axis}[
                width=\linewidth,  
                height=7cm,
                xlabel={Erro ($e = y - \hat{y}$)},
                ylabel={Perda Calculada},
                axis lines=middle,
                grid=major,
                grid style={dashed, gray!40},
                xmin=-4.5, xmax=4.5,
                ymin=-0.5, ymax=3.5,
                legend pos=north west,
                title style={font=\bfseries\small},
                label style={font=\small},
                tick label style={font=\scriptsize},
            ]
                \addplot[domain=-4:4, samples=5, color=blue, very thick] {(x >= 0) ? (0.1*x) : ((1-0.1)*(-x))};
            \end{axis}
        \end{tikzpicture}
        \caption{Vista da função de perda em relação ao erro ($\tau=0.1$).}%
        \label{fig:quantile-2d-tau01}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \begin{tikzpicture}
            \begin{axis}[
                width=\linewidth,
                height=7cm,
                xlabel={$y$ (Real)},
                ylabel={$\hat{y}$ (Previsto)},
                zlabel={Perda},
                grid=major,
                view={150}{45},
                zmin=0, zmax=10.5,
                legend pos=north east,
                title style={font=\bfseries\small},
                label style={font=\small},
                tick label style={font=\scriptsize},
            ]
                \addplot3[
                    mesh,          
                    color=blue,
                    shader=interp,  
                    domain=-5:5,   
                    domain y=-5:5,  
                    samples=15     
                ] { (x - y >= 0) ? (0.1*(x - y)) : ((1-0.1)*(-(x - y))) };
            \end{axis}
        \end{tikzpicture}
        \caption{Superfície da função de perda ($\tau=0.1$).}%
        \label{fig:quantile-3d-tau01}
    \end{subfigure}

    \caption{Visualizações das perdas quantílicas para os percentis 90 ($\tau=0.9$) e 10 ($\tau=0.1$).}%
    \label{fig:quantile-loss-grid}
    \fonte{O autor (2025).}
\end{figure}

\medskip
\begin{center}
 * * *
\end{center}
\medskip

\subsubsection*{Características da perda quantílica} 

\begin{itemize}
    \item \textbf{Continuidade, suavidade e diferenciabilidade:} A perda quantílica é de classe $C^0$, contínua, (ver Apêndice~\ref{ap:deducoes-quantile-loss}). Isso acontece pelo mesmo motivo que o \textit{MAE} também é de classe $C^0$, essas duas funções apresentam um ``bico'' quando o erro é zero. Como resultado, deve-se analisar os limites laterais dessa função nesse ponto, chegando a conclusão que seus valores divergem. Ainda assim, o uso de subgradiente ajuda a contornar esse problema de suavidade.
    \item \textbf{Convexidade:} A perda quantílica é convexa (ver Apêndice~\ref{ap:deducoes-quantile-loss}). Isso é evidenciado também pelos seus gráficos, os quais apontam que essa função apresenta um formato semelhante a um funil. Consequentemente, essa propriedade ajuda os otimizadores baseados em gradiente. Vale destacar que otimizadores baseados em momento vão se comportar diferente conforme forem os valores de $\tau$. Inclinações diferentes podem resultar em otimizações com iterações diferentes.
    \item \textbf{Robustez:} A perda quantílica é Lipschitz-contínua, e portanto, robusta a \textit{outliers} (ver Apêndice~\ref{ap:deducoes-quantile-loss}). As suas derivadas parciais quando o erro tende ao infinito são limitadas, garantindo que o gradiente não ``exploda'' quando ocorrem erros grandes.
    \item \textbf{Hiperparâmetro $\tau$ (penalização assimétrica):} A perda quantílica é uma função assimétrica, exceto no caso em que $\tau = 0.5$. Como o parâmetro $\tau$ é definido ao treinar ao modelo, cabe a quem estiver treinando o modelo definir se quer que a função penalize mais as subestimações ou as superestimações, a depender do problema que está sendo considerado.
\end{itemize}

\medskip
\begin{center}
 * * *
\end{center}
\medskip

\subsubsection*{Gradiente da perda quantílica}

Para calcular a derivada da perda quantílica, deve-se derivar as duas expressões da Equação~\ref{eq:quantile-loss} e escrever o seu resultado em uma função por partes. Neste caso, está sendo considerado que não existe um valor para o cenário em que a predição é igual ao valor real, ou seja, $\hat{y}_j = y_j$, dado que este é um ponto de descontinuidade dessa função.

Dessa forma, a derivada parcial da perda quantílica em relação à predição feita pelo modelo é dada pela Equação~\ref{eq:quantile-loss-derivada}. Além disso, está sendo indicado os cenários de superestimação e subestimação, de forma a simplificar como os erros do modelo serão corrigidos ao retropropagar o gradiente para as camadas.

\begin{equacaodestaque}{Derivada parcial da perda quantílica em relação à predição}
    \frac{\partial\Loss_{\tau}}{\partial\hat{y}_j} = 
    \begin{cases} 
        (1 - \tau) & \text{se } y_j < \hat{y}_j \text{ (superestimação)}\\
        -\tau & \text{se } y_j > \hat{y}_j \text{ (subestimação)}
    \end{cases}%
    \label{eq:quantile-loss-derivada}
\end{equacaodestaque}

Já o gráfico da derivada da perda quantílica está na Figura~\ref{fig:quantile-loss-derivada}, neste caso está sendo apresentado somente a sua visualização em duas dimensões. O gráfico em que $\tau = 0.5$ é igual o gráfico da derivada do erro absoluto médio.

\begin{figure}[h!]
    \centering
    \begin{tikzpicture}
        \begin{axis}[
            xlabel={Erro ($y - \hat{y}$)},
            ylabel={Gradiente da Perda ($\frac{\partial L}{\partial (\text{erro})}$)},
            axis lines=middle,
            grid=major,
            grid style={dashed, gray!40},
            xmin=-4.5, xmax=4.5,
            ymin=-1.1, ymax=1.1,
            ytick={-0.9, -0.5, -0.1, 0, 0.1, 0.5, 0.9},
            legend pos=south east,
            width=12cm,
            height=9cm,
            title style={font=\bfseries},
            label style={font=\small},
            tick label style={font=\scriptsize}
        ]
            % Linhas para Tau = 0.9
            \addplot[const plot, color=blue, very thick] coordinates {(-4, 0.9-1) (0, 0.9-1)};
            \addplot[const plot, color=blue, very thick] coordinates {(0, 0.9) (4, 0.9)};
            \addlegendentry{$\tau=0.9$}
            
            % Linhas para Tau = 0.5
            \addplot[const plot, color=gray, thick] coordinates {(-4, 0.5-1) (0, 0.5-1)};
            \addplot[const plot, color=gray, thick] coordinates {(0, 0.5) (4, 0.5)};
            \addlegendentry{$\tau=0.5$}

            % Linhas para Tau = 0.1
            \addplot[const plot, color=red, very thick] coordinates {(-4, 0.1-1) (0, 0.1-1)};
            \addplot[const plot, color=red, very thick] coordinates {(0, 0.1) (4, 0.1)};
            \addlegendentry{$\tau=0.1$}

            % Círculos abertos para a descontinuidade
            \addplot[only marks, mark=o, color=blue, mark size=2pt] coordinates {(0, -0.1) (0, 0.9)};
            \addplot[only marks, mark=o, color=gray, mark size=2pt] coordinates {(0, -0.5) (0, 0.5)};
            \addplot[only marks, mark=o, color=red, mark size=2pt] coordinates {(0, -0.9) (0, 0.1)};
        \end{axis}
    \end{tikzpicture}
    \caption{Gráfico da derivada da Perda Quantílica (em Relação ao erro).}%
    \label{fig:quantile-loss-derivada}
    \fonte{O autor (2025).}
\end{figure}

\medskip
\begin{center}
 * * *
\end{center}
\medskip

\subsubsection*{Aplicações da perda quantílica em aprendizado de máquina}%
\index{Aplicações práticas! Perda quantílica}

Uma situação que a perda quantílica está muito presente é na análise do valor em risco ou \textit{value at risk} (\textit{VaR}). O \textit{VaR} é uma medida de risco que estima a perda potencial máxima que pode acontecer um determinado portfólio durante um período de tempo definido, isso é estimado dentro de um nível de confiança especificado. Por exemplo, caso queira ser descoberto o \textit{VaR} a 99\%, é preciso calcular o 1º quantil da distribuição de perdas e ganhos. Esse é um dos cenários que está presente em alguns dos trabalhos apresentados nessa seção, em que os autores optam pela perda quantílica para resolver esse problema.

Dito isso, vale citar os trabalhos: 

\begin{itemize}
    \item \textbf{Aplicação 1 (Área):} Em \textit{Mixed–frequency quantile regressions to forecast Value–at–Risk and Expected Shortfall}, \textcite{candila2023mixedfrequencyquantileregressionsforecast} utilizam da regressão quantílica como uma forma de prever o \textit{VaR} e o \textit{expected shortfall} (\textit{ES}), para isso, eles criam utilizam de uma técnica diferente, em que é feito o uso de regressões quantílicas de frequência mista. Para fazer isso, os autores combinam dados macroêconomicos de baixa frequência (que são mensais), com dados de mercado de alta frequência (os quais são diários) \parencite{candila2023mixedfrequencyquantileregressionsforecast}. Além disso, \textcite{candila2023mixedfrequencyquantileregressionsforecast} exploram esse novo modelo criado com dados reais, utilizando duas commodities energéticas chamadas Crude Oil and Gasoline futures;
    \item \textbf{Aplicação 2 (Área):} Outro trabalho que também faz uso da regressão quantílica é o \textit{CAViaR: Conditional Autoregressive Value at Risk by Regression Quantiles}, nele, \textcite{Engle2004CAViaR} apresenta o \textit{Conditional Autoregressive Value at Risk} ou \textit{CAViaR}. O \textit{CAViaR}, como os autores explicam no texto, propõe uma alternativa diferente para calcular o \textit{VaR}, ao invés de ser modelado toda a distribuição, com o \textit{CAViaR} é calculado apenas o quantil desejado, isso pode ser calculado com o uso da regressão quantílica, e consequentemente com a função de perda quantílica \parencite{Engle2004CAViaR};
    \item \textbf{Aplicação 3 (Área):} Além disso, vale citar também o artigo \textit{Reforming health care: Evidence from quantile regressions for counts} de \textcite{WINKELMANN2006131}, nele, a regressão quantílica para contagens é utilizada para estimar o efeito de uma reforma na saúde sobre a frequência de consultas médicas individuais. O autor justifica a escolha de ser feita uma regressão quantílica, e não uma técnica mais comum, como a dos mínimos quadrados, pois ele considera que esse é um problema em que os efeitos da reforma são diferentes em cada uma das partes da distribuição \parencite{WINKELMANN2006131}. Neste cenário, em que a distribuição de dados foge da distribuição normal, uma alternativa para lidar com isso é utilizar a perda quantílica\footnote{Além disso, caso você leitor esteja procurando funções de perdas para lidar com distribuições que não seguem a distribuição normal, vale a pena dar uma olhada na Seção~\ref{sec:perdas-baseadas-em-distribuicoes-de-dados}. Nela, são explicadas duas funções de perda que podem ser utilizadas nesse cenário: a perda de Poisson e a perda de Tweedie.}.
\end{itemize}

\medskip
\begin{center}
 * * *
\end{center}
\medskip

Além da perda quantílica, que é usada nos problemas de regressão quantílica, e que faz o cálculo dos erros considerando uma distribuição irregular dos dados, e por isso não faz o uso da média dos erros para encontrar a perda, existem outras funções que possuem essa mesma característica. Uma dessas funções é a epsilon-insensível, que é essencial para resolver os problemas de regressão para modelos de máquinas de vetores de suporte (\textit{SVMs}). Ela será explicada em seguida.

\subsection{Perda epsilon-insensível (epsilon-insensitive loss)}%
\index{Funções de Perda!Perda Epsilon-Insensível}

Para explicar a perda $\epsilon$-insensível é preciso antes entender o cenário em que essa função surgiu, e como ela está relacionada com os modelos de regressão para as \textit{SVMs}. Um do artigos que explica a criação desse modelo de aprendizado de máquina para a tarefa de regressão é o \textit{Support Vector Regression Machines} dos pesquisadores \textcite{SupportVectorRegressionMachines}, nele, os autores detalham um novo uso das máquinas de vetores desenvolvidas por Vapnik, as quais antes eram utilizadas para resolver problemas de classificação.

\textcite{SupportVectorRegressionMachines} começam o artigo explicando um problema, eles possuem uma função $G(x)$ (chamada de verdade), sendo uma função que recebe um vetor $x$ (chamado de espaço de entradas), esse vetor $x$ possui $d$ componentes, sendo do tipo $x^t = [x_1, x_2, ..., x_d]$. Além disso, existe também uma família de funções $F(x, w)$ parametrizadas por $w$ \parencite{SupportVectorRegressionMachines}. O Objeto dos autores é encontrar um parâmetro $\hat{w}$ que é uma estimativa de $w$ a partir de observar um conjunto de $N$ instâncias de treinamento \parencite{SupportVectorRegressionMachines}.

Uma das aproximações para resolver esse problema é dada Equação xx como explicam \textcite{SupportVectorRegressionMachines}.

\begin{equation}
    F = (x, \hat{[w]}) = \sum_{i = 1}^N (\alpha_i^*- \alpha_i)(v_i^tx + 1)^p + b
    \label{eq:svms-para-regressao}
\end{equation}

Além disso, eles possuem também a função objetiva que querem minimizar, ela é dada pela Equação xx

\begin{equation}
    U \sum_{j = 1}^N L [y_j- F(v_j, \hat{w})] + ||\hat{w}||^2
\end{equation}

Note que essa função objetiva faz o uso de uma função de perda em sua fórmula, essa função é dada pela $\epsilon$-insensível, a qual pode ser vista na Equação~\ref{eq:epsilon-insensitive-loss}

\begin{equacaodestaque}{Perda epsilon-insensível}
    \Loss_{\epsilon}(y,\hat{y}) = 
    \begin{cases} 
        0 & \text{se } |y -\hat{y}| < \epsilon\\
        |y -\hat{y}| -\epsilon& \text{se } |y -\hat{y}| \ge\epsilon\end{cases}%
    \label{eq:epsilon-insensitive-loss}
\end{equacaodestaque}

Em que:

\begin{itemize}
    \item $y_j$ representa o valor real da j-ésima amostra;
    \item $\hat{y}_j$ representa o valor predito pelo modelo;
    \item $\epsilon$ ...
\end{itemize}

O objetivo de utilizar essa função de perda em máquinas de vetores de suporte para problemas de regressão é criar uma margem em que o erro do modelo não seja penalizado. O tamanho dessa margem é dado pelo termo $\epsilon$, além disso, fora desse intervalo o erro é penalizado de forma linear, semelhante ao erro absoluto médio. Isso pode ser visto de forma mais intuitiva nos gráficos da Figura~\ref{fig:epsilon-insensitive-loss}, no gráfico da esquerda (a Figura~\ref{fig:epsilon-2d}) é possível ver a representação em duas dimensões da perda, enquanto no gráfico da direta (a Figura~\ref{fig:epsilon-3d}) está representado a superfície dessa função no espaço.

\begin{figure}[h!]
    \centering 

    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \begin{tikzpicture}
            % Define o valor de epsilon
            \def\epsilon{1.0}
            
            \begin{axis}[
                % Dimensões ajustadas para caber lado a lado
                width=\linewidth,  
                height=7cm,
                xlabel={Erro ($e = y - \hat{y}$)},
                ylabel={Perda Calculada},
                axis lines=middle,
                grid=major,
                grid style={dashed, gray!40},
                xmin=-4.5, xmax=4.5,        % Limites do seu gráfico
                ymin=-0.5, ymax=3.5,         % Limites do seu gráfico
                legend pos=north west,
                title style={font=\bfseries\small},
                label style={font=\small},
                tick label style={font=\scriptsize}
            ]
                % Gráfico da função
                \addplot[
                    domain=-4:4, 
                    samples=101,
                    color=blue, 
                    very thick
                ] {max(0, abs(x) - \epsilon)};
                
                \addlegendentry{$L_{\epsilon=1}(e)$}

                % Linhas tracejadas para marcar a margem epsilon
                \draw[dashed, gray] (axis cs:-\epsilon, -0.5) -- (axis cs:-\epsilon, 3.5);
                \draw[dashed, gray] (axis cs:\epsilon, -0.5) -- (axis cs:\epsilon, 3.5);
                \node[above, gray!80, font=\tiny] at (axis cs:0, 2) {Zona de Perda Zero};
            \end{axis}
        \end{tikzpicture}
        \caption{Visão 2D (Perda vs. Erro).}%
        \label{fig:epsilon-2d}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \begin{tikzpicture}
            % Define o valor de epsilon
            \def\epsilon{1.0}
            
            \begin{axis}[
                % Dimensões consistentes com o gráfico (a)
                width=\linewidth,
                height=7cm,
                xlabel={$y$ (Real)},
                ylabel={$\hat{y}$ (Previsto)},
                zlabel={Perda},
                grid=major,
                view={150}{45}, % Mesmo ângulo de visão do seu template
                zmin=0, zmax=10.5, % Ajustado para max(0, abs(10) - 1) = 9
                title style={font=\bfseries\small},
                label style={font=\small},
                tick label style={font=\scriptsize}
            ]
                % Gráfico da superfície da Epsilon-Insensitive Loss
                \addplot3[
                    mesh,           % Tipo de gráfico: superfície
                    color=blue,
                    shader=interp,  % Suaviza as cores
                    domain=-5:5,    % Domínio de x (y_real)
                    domain y=-5:5,  % Domínio de y (y_previsto)
                    samples=15      % Resolução da malha
                ] { max(0, abs(x - y) - \epsilon) }; % A função Epsilon-Insensitive 3D
            \end{axis}
        \end{tikzpicture}
        \caption{Superfície 3D completa.}%
        \label{fig:epsilon-3d}
    \end{subfigure}

    \caption{Visualizações da função de perda epsilon-insensível (com $\epsilon=1$) em duas e em três dimensões.}%
    \label{fig:epsilon-insensitive-loss}
    \fonte{O autor (2025).}
\end{figure}

\medskip
\begin{center}
 * * *
\end{center}
\medskip

\subsubsection*{Características da perda epsilon-insensível}

\begin{itemize}
    \item \textbf{Robustez para \textit{outliers}:} Uma das características da perda $\epsilon$-insensível é a sua robustez para \textit{outliers}, isso acontece devido ao jeito que ela lida com esse tipo de dado. Diferente do erro quadrático médio, que penaliza fortemente as predições do modelo $\hat{y}$ que estão muito distantes do valor real $y$ e por isso, ao ser aplicada um conjunto que possui muitos \textit{outliers} o modelo que faz uso dessa perda não irá performar tão bem. Na perda $\epsilon$-insensível isso não acontece de forma tão drástica, uma vez que ela penaliza os erros de forma linear.
    \item \textbf{Não-diferenciabilidade em erro $=$ $\epsilon$:} Note no gráfico dessa função de perda que existem duas ``quinas'' nos pontos em que o erro é igual a $\epsilon$, isso não impede a função de ser contínua nesse ponto, mas não permite que ela seja diferenciada, algo que pode atrapalhar ao ser combinada com otimizadores que fazem uso do cálculo do gradiente.
    \item \textbf{Zona de perda zero:} Perceba pelo gráfico da perda $\epsilon$-insensível que diferente das outras perdas em que elas possuíam um único ponto de mínimo no qual a perda era zero, aqui isso não acontece, existe uma reta com tamanho definido por $\epsilon$ que mostra que a perda é zero, qualquer ponto que estiver dentro dessa reta estará no ponto de mínimo da função. Isso é propriedade interessante, pois significa que existem mais locais para pontos de mínimos, o que em teoria pode fazer com que a função seja mais fácil para ser otimizada.
\end{itemize}

\medskip
\begin{center}
 * * *
\end{center}
\medskip

\subsubsection*{Gradiente da perda epsilon-insensível}

Cabe agora discutir também o cálculo da derivada dessa função, que será útil para criação do vetor gradiente e da retropropagação dos erros. Para calculá-la é preciso derivar as duas expressões da Equação~\ref{eq:epsilon-insensitive-loss}, contudo, assim como no erro absoluto médio que foi preciso fazer o cálculo da derivada com auxílio de subgradientes por conta dos pontos de não-diferenciabilidade, aqui o processo é semelhante. Com isso, é possível chegar na expressão da Equação~\ref{eq:epsilon-insensitive-derivada} para a derivada parcial em Relação à predição do modelo $\hat{y}$ para a perda $\epsilon$-insensível.

\begin{equacaodestaque}{Derivada da perda epsilon-insensível}
    \frac{\partial\Loss_{\epsilon}}{\partial\hat{y}} = 
    \begin{cases} 
        -1 & \text{se } \hat{y} -y > \epsilon\\
        0 & \text{se } |\hat{y} -y| \le\epsilon\\
        1 & \text{se } \hat{y} -y < -\epsilon\end{cases}%
    \label{eq:epsilon-insensitive-derivada}
\end{equacaodestaque}

Tendo sua derivada calculada, o próximo passo é fazer a plotagem do seu gráfico, o qual pode ser visto na Figura~\ref{fig:epsilon-derivada-completa}. No gráfico da esquerda, na Figura~\ref{fig:epsilon-derivada-2d}, está a representação de uma vista em duas dimensões da perda $\epsilon$-insensível. Já no gráfico da direta, na Figura~\ref{fig:epsilon-derivada-3d}, está a superfície da derivada parcial da perda em Relação às predições $\hat{y}$ do modelo.

\begin{figure}[h!]
    \centering % Centraliza a figura

    % --- Define o valor de epsilon e o ponto de corte (y real) ---
    \def\epsilon{1.0}
    \def\yreal{5.0} % Valor de y (Real) para o corte 2D

    % --- SUBFIGURA (a): Gráfico 2D da Derivada (Corte em y=5) ---
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \begin{tikzpicture}
            \begin{axis}[
                width=\linewidth,  
                height=7cm,
                title={Visão 2D (corte em $y=\yreal$)},
                xlabel={Valor Previsto ($\hat{y}$)},
                ylabel={Gradiente ($\frac{\partial L}{\partial \hat{y}}$)},
                axis lines=middle,
                grid=major,
                grid style={dashed, gray!40},
                xmin=0, xmax=10,
                ymin=-1.5, ymax=1.5,
                ytick={-1, 0, 1},
                legend pos=north west,
                title style={font=\bfseries\small},
                label style={font=\small},
                tick label style={font=\scriptsize}
            ]
                
                % --- Gráfico da Derivada por partes (const plot) ---
                % Baseado na Equação 10.23: L' = 1 se y_hat < y - eps
                \addplot[const plot, color=blue, very thick, opacity=0.8] 
                    coordinates { (0, 1) (\yreal - \epsilon, 1) };
                \addlegendentry{$\hat{y} - y < -\epsilon \implies +1$}
                
                % L' = 0 se |y_hat - y| <= eps
                \addplot[const plot, color=gray, very thick, opacity=0.8] 
                    coordinates { (\yreal - \epsilon, 0) (\yreal + \epsilon, 0) };
                \addlegendentry{$|\hat{y} - y| \le \epsilon \implies 0$}
                
                % L' = -1 se y_hat - y > eps
                \addplot[const plot, color=red, very thick, opacity=0.8] 
                    coordinates { (\yreal + \epsilon, -1) (10, -1) };
                \addlegendentry{$\hat{y} - y > \epsilon \implies -1$}

                % --- Linhas de marcação ---
                % Linha vertical para o valor real
                \draw[dashed, gray] (axis cs:\yreal, -1.5) -- (axis cs:\yreal, 1.5);
                \node[above, gray!80, font=\tiny] at (axis cs:\yreal, 1.5) {Valor Real};
                
                % Linhas verticais para a margem epsilon
                \draw[dashed, black!60] (axis cs:\yreal-\epsilon, -1.5) -- (axis cs:\yreal-\epsilon, 1.5);
                \draw[dashed, black!60] (axis cs:\yreal+\epsilon, -1.5) -- (axis cs:\yreal+\epsilon, 1.5);

            \end{axis}
        \end{tikzpicture}
        \caption{Visão 2D (corte em $y=\yreal$).}%
        \label{fig:epsilon-derivada-2d}
    \end{subfigure}
    \hfill 
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \begin{tikzpicture}
            \begin{axis}[
                width=\linewidth,
                height=7cm,
                title={Superfície 3D completa},
                xlabel={$y$ (Real)},
                ylabel={$\hat{y}$ (Previsto)},
                zlabel={Gradiente ($\frac{\partial L}{\partial \hat{y}}$)},
                grid=major,
                view={150}{45}, % Mesmo ângulo de visão do seu template
                zmin=-1.5, zmax=1.5,
                ztick={-1, 0, 1}, % Ticks exatos da derivada
                title style={font=\bfseries\small},
                label style={font=\small},
                tick label style={font=\scriptsize},
                colormap/viridis % Mapa de cores do seu template
            ]
                % Gráfico da superfície da Derivada da Epsilon-Insensitive Loss
                % x = y (Real), y = y_hat (Previsto)
                % Fórmula (Equação 10.23):
                % (y - x > epsilon) ? -1 : ( (y - x < -epsilon) ? 1 : 0 )
                \addplot3[
                    mesh,           
                    color=red,   % Cor consistente com o gráfico 2D
                    shader=interp,  
                    domain=-5:5,    % Mesmo domínio do seu template
                    domain y=-5:5,  % Mesmo domínio do seu template
                    samples=15      % Mesma resolução da malha
                ] { (y - x > \epsilon) ? -1 : ( (y - x < -\epsilon) ? 1 : 0 ) }; 
            \end{axis}
        \end{tikzpicture}
        \caption{Superfície 3D completa.}%
        \label{fig:epsilon-derivada-3d}
    \end{subfigure}

    \caption{Visualizações da derivada (gradiente) da função de perda epsilon-insensível (com $\epsilon=1$).}%
    \label{fig:epsilon-derivada-completa}
    \fonte{O autor (2025).}
\end{figure}

Perceba como as características da função refletem em sua derivada, nas retas definidas por $\epsilon$ em que a perda é zero, consequentemente a sua derivada também será zero, algo que pode ser visto no gráfico da Figura~\ref{fig:epsilon-derivada-2d}, existem uma lacuna onde a função também é zero. Além disso, note que nos pontos em que acontece o ``bico'' da função original, eles viram uma reta vertical na derivada, como a Figura~\ref{fig:epsilon-derivada-3d}. Isso é interessante pois nos mostra de forma mais direta, quais serão os pontos em que será difícil de calcular o gradiente e com isso ele irá nos causar problemas.

Essa análise de descontinuidade pode ser feita também com o próprio gráfico da função, mas como o cálculo da derivada depende que a função seja contínua no ponto desejado, ao plotar os gráficos de uma função de perda, é possível ter uma noção mais explícita dos pontos ``problema'' de uma função de perda.

\medskip
\begin{center}
 * * *
\end{center}
\medskip

\subsubsection*{Aplicações da perda epsilon-insensível em aprendizado de máquina}%
\index{Aplicações práticas! Perda epsilon-insensível}

\begin{itemize}
    \item \textbf{Aplicação 1 (Área):}
    \item \textbf{Aplicação 2 (Área):}
    \item \textbf{Aplicação 3 (Área):}
    \item \textbf{Aplicação 4 (Área):}
\end{itemize}

\section{Perdas baseadas em distribuições de dados}%
\label{sec:perdas-baseadas-em-distribuicoes-de-dados}

Exceto pela perda $\epsilon$-insensível que trabalhava com distribuições que não eram simétricas, a maioria das perdas trabalhadas até agora neste capítulo possuem uma característica em comum: são perdas utilizadas para dados que seguem a distribuição normal. Foi possível ver isso inclusive na explicação sobre o erro quadrático médio, e como Gauss prova que o método dos mínimos quadrados é uma solução ideal para ser utilizado em cenários em que os dados seguem a distribuição Gaussiana.

Contudo, mesmo a distribuição normal sendo a mais comum para modelar os diferentes problemas que envolvem regressão, existem também outras distribuições que são igualmente importante. Essa seção foca em explicar três funções de perda que são utilizadas para trabalhar com distribuições específicas, são elas: a perda de Poisson, a perda Gamma e a perda de Tweedie.

Para explicar essas funções, esta seção toma um rumo diferente das outras vistas até o momento, ao invés de explicar diretamente a função de perda, antes é explicado sobre a sua respectiva distribuição, apresentando gráficos e fórmulas, como a função densidade e probabilidade. Tendo conhecimento sobre as distribuições, e então introduzida a função de perda, junto com suas fórmulas, derivadas, representações gráficas e também algumas aplicações em algoritmos de aprendizado de máquina.

Dito isso, é possível começar então explicando sobre a distribuição de Poisson.

\subsection{Perda de Poisson (Poisson loss)}%
\index{Funções de Perda!Perda de Poisson (\textit{Poisson loss})}

\subsubsection*{A distribuição de Poisson}%
\index{Distribuição de Poisson}

Imagine que você está procurando uma forma de modelar a quantidade de ligações que você pode receber em seu telefone por dia. Primeiro, você pensou que essa quantidade seguiria uma distribuição normal, mas encontrou um problema, a distribuição normal é contínua, e não tem como você receber $2,5$ ligações em um período. Além disso, você decidiu que precisava dividir o tempo em um conjunto de intervalos discretos, assim, todas as ligações que você receber das 14:00 as 14:59 ficarão no mesmo intervalo. Esse problema possui ainda uma terceira particularidade, não existe um limite de ligações que você pode receber, pode ser que das 13:00 as 13:59 você receba duas ligações, mas também pode ser que você receba 314 ligações naquele horário. Foi então que procurando mais sobre o assunto, você descobriu sobre uma distribuição diferente, que parece cumprir todos os requisitos para modelar esse seu problema, essa é a distribuição de Poisson.

\medskip
\begin{center}
 * * *
\end{center}
\medskip

A distribuição de Poisson é dada pela Equação~\ref{eq:poisson-distribution-pmf}, em que o espaço amostral é conjunto de inteiros não-negativos. Outro detalhe dessa distribuição, como aponta xxx, é que ela não possui uma limite superior nos valores que podem ser observados. Isso é um ponto interessante, pois na distribuição normal isso não acontece, ela apresenta um limite superior para esses valores.

\begin{equacaodestaque}{Distribuição de Poisson (PMF)}
    P(y_j| \lambda) = \frac{\lambda^{y_j} e^{-\lambda}}{y_j!}, \quad y_j \in {0, 1, 2, \ldots}%
    \label{eq:poisson-distribution-pmf}
\end{equacaodestaque}

Em que:

\begin{itemize}
    \item $\lambda$ representa a taxa de Poisson, ou seja, a contagem esperada
\end{itemize}

Essa distribuição possui diversas propriedades interessantes. Como explicam \textcite{GeneralizedLinearModels}, a distribuição de Poisson segue a média, a variância e todos os cumulativos de Y que são igual a $\lambda$. Isso significa que a média e a variância dessa distribuição são iguais, e como consequência o parâmetro $\lambda$ é o único responsável por controlar como a distribuição se comporta. Considerando a distribuição normal, que precisa de dois parâmetros ($\mu$ para a média e $\sigma^2$ para a variância), na Poisson o parâmetro $\lambda$ é quem dita a tendência central, onde os dados se agrupam, e também a dispersão, indicando se eles estão muito ou pouco espalhados.

É possível ver isso melhor nas plotagens dessa distribuição. Na Figura~\ref{fig:poisson-low-lambda} o parâmetro $\lambda$ é igual a 1, ele é baixo, o que significa que a média da distribuição é baixa, por isso a maioria dos dados está bem próximo de zero, mas além disso, a variância também é baixa, indicando que os dados são representados no gráfico de forma assimétrica. Já na Figura~\ref{fig:poisson-high-lambda} o cenário é o inverso, o $\lambda$ é alto, o que reflete em dados melhores distribuídos no gráfico, além de uma média maior, perceba como o gráfico passa a lembrar o de uma distribuição Normal, mas ainda sim, permanece discreto.

\begin{figure}[h!]
    \centering 

    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \begin{tikzpicture}
            \begin{axis}[
              axis x line=center,
              axis y line=center,
              xtick={0,2,...,19},
              ytick={0.1,0.2,...,0.4},
                domain = 0:18,
                samples = 19,
                xlabel={$k$},
                ylabel={$P[k]$},
                xlabel style={right},
                ylabel style={above left},
                ymax=0.5,
                xmax=20,
                x post scale=1.4
                ]
                \addplot+[ycomb,blue,thick] {poisson(1)};
                \addlegendentry{$\lambda = 1$}
            \end{axis}
        \end{tikzpicture}
        \caption{Com $\lambda$ baixo, a distribuição é assimétrica e ``espremida'' perto do zero.}%
        \label{fig:poisson-low-lambda}
    \end{subfigure}
    \hfill
    
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \begin{tikzpicture}
            \begin{axis}[
              axis x line=center,
              axis y line=center,
              xtick={0,2,...,19},
              ytick={0.1,0.2,...,0.4},
                domain = 0:18,
                samples = 19,
                xlabel={$k$},
                ylabel={$P[k]$},
                xlabel style={right},
                ylabel style={above left},
                ymax=0.5,
                xmax=20,
                x post scale=1.4
                ]
                \addplot+[ycomb,brown,thick] {poisson(9)};
                 \addlegendentry{$\lambda = 9$};
            \end{axis}
        \end{tikzpicture}

        \caption{Com $\lambda$ alto, a distribuição se aproxima de uma curva Normal (Gaussiana), mas permanece discreta.}%
        \label{fig:poisson-high-lambda}
    \end{subfigure}
    
    \caption{Comparação visual da Distribuição de Poisson com $\lambda$ baixo e alto.}
    \label{fig:poisson-comparison}
    \fonte{O autor (2025).}
\end{figure}

Além disso, vale a pena discutir também a função log-verossimilhança dessa função, pois é a partir dela que é possível chegar no cálculo da perda de Poisson. Dito isso, a log-verossimilhança negativa dessa função, que omite os termos constantes que não dependem de $\lambda_j$ é dada pela Equação~\ref{eq:log-verossimilhanca-negativa-de-poisson}

\begin{equacaodestaque}{Log-verossimilhança negativa para a distribuição de Poisson}
    l_j (\lambda_j) = \lambda_j - y_j \log(\lambda_j)%
    \label{eq:log-verossimilhanca-negativa-de-poisson}
\end{equacaodestaque}

Ainda no livro \textit{Generalized Linear Models}, \textcite{GeneralizedLinearModels} explicam algum cenários em que essa distribuição pode ser utilizada para descrever um problema, entre algum dos explicados pelo autor, vale a pena citar:

\begin{itemize}
    \item \textbf{Um ensaio biológico sobre tuberculina:} A distribuição de poisson com a sua função de log-verossimilhança é utilizada para descrever os casos de tuberculina em população de bovinos;
    \item \textbf{Um estudo sobre o dano das ondas em navios cargueiros:} xxx utilizam a distribuição de poisson assim como o cálculo da log-verossimilhança para estimar o dano das ondas em navios cargueiros.
\end{itemize}

Na prática um bom indicativo para considerar utilizar a distribuição de poisson para modelar os eventos e consequentemente utilizar a perda de Poisson para criar um modelo de regressão é considerar as próprias características dessa distribuição. Assim, ao analisar um problema e perceber que os dados seguem uma distribuição discreta, são inteiros não negativos e que a média dos dados é igual a variância, talvez valha a pena dar uma analisa e ver se a distribuição de Poisson pode ser aplicada.

\medskip
\begin{center}
 * * *
\end{center}
\medskip

Considerando o conteúdo introdutório discutido até o momento para explicar a distribuição de Poisson, é possível finalmente entender essa função com mais detalhes. A perda de Poisson é dada pela Equação~\ref{eq:poisson-loss}, perceba que ela é basicamente o cálculo da log-verossimilhança negativa, em que o termo $\lambda_j$ neste caso é dado pelo valor predito $\hat{y}_j$ pelo modelo.

\begin{equacaodestaque}{Perda de Poisson}
    \Loss_{\text{Poisson}}(y_j, \hat{y}_j) = \sum_{j = 0}^N \hat{y}_j -y_j \log(\hat{y}_j)%
    \label{eq:poisson-loss}
\end{equacaodestaque}

Em que:

\begin{itemize}
    \item $y_j$ representa o valor real da j-ésima amostra;
    \item $\hat{y}_j$ representa o valor predito pelo modelo;
    \item $N$ representa o número de amostras.
\end{itemize}

Considerando a fórmula da perda de Poisson, é possível também discutir os seus gráficos, os quais estão presentes na Figura~\ref{fig:poisson-loss-grid}, é interessante notar como os seus gráficos variam conforme os valores reais $y$ mudam. Quando o valor real é um valor pequeno, como está retratado na Figura~\ref{fig:poisson-2d-low} (para a visão em duas dimensões da função) e na Figura~\ref{fig:poisson-3d-low} (para a vista da superfície da função no espaço), a perda de Poisson possui uma convexidade mais ``apertada''. Já nos cenários da Figura~\ref{fig:poisson-2d-high} (visão em duas dimensões) e na Figura~\ref{fig:poisson-3d-high} (visão em três dimensões), o valor de $y$ é grande, o que resulta em uma função que é mais ``larga''.

\begin{figure}[h!]
    \centering

    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \begin{tikzpicture}
            \begin{axis}[
                width=\linewidth,  
                height=7cm,
                xlabel={Valor Previsto ($\hat{y}$)},
                ylabel={Perda},
                axis lines=middle,
                grid=major,
                grid style={dashed, gray!40},
                xmin=0, xmax=10,
                ymin=-1, ymax=5,
                legend pos=north west,
                title style={font=\bfseries\small},
                label style={font=\small},
                tick label style={font=\scriptsize},
                title={Visão 2D (Alvo $y=2$)}
            ]
                % Perda Poisson: x - y*ln(x) -> Aqui x é y_hat, y fixo em 2
                \addplot[domain=0.1:10, samples=100, color=blue, very thick] {x - 2*ln(x)};
                \addlegendentry{$y=2$}
                
                % Linha pontilhada no mínimo
                \draw[dashed, red!60] (axis cs:2, -2) -- (axis cs:2, {2-2*ln(2)});
                \node[anchor=north, font=\tiny, text=red] at (axis cs:2, -0.8) {Mínimo em $\hat{y}=2$};
            \end{axis}
        \end{tikzpicture}
        \caption{Comportamento para valor real baixo.}%
        \label{fig:poisson-2d-low}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \begin{tikzpicture}
            \begin{axis}[
                width=\linewidth,
                height=7cm,
                xlabel={$y$ (Real)},
                ylabel={$\hat{y}$ (Previsto)},
                zlabel={Loss},
                grid=major,
                view={135}{35}, % Angulo de visão
                legend pos=north east,
                title style={font=\bfseries\small},
                label style={font=\small},
                tick label style={font=\scriptsize},
                title={Superfície 3D (Valores Baixos)},
                zmin=-2, zmax=5
            ]
                \addplot3[
                    mesh,           % Tipo de gráfico: superfície
                    color=blue,
                    shader=interp,  % Suaviza as cores
                    domain=-5:5,    % Domínio de x (y_real)
                    domain y=-5:5,  % Domínio de y (y_previsto)
                    samples=15      % Resolução da malha
                ] { y - x*ln(y) };
            \end{axis}
        \end{tikzpicture}
        \caption{Superfície local (Região $1 \le y \le 5$).}%
        \label{fig:poisson-3d-low}
    \end{subfigure}

    \vspace{0.5cm} 

    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \begin{tikzpicture}
            \begin{axis}[
                width=\linewidth,  
                height=7cm,
                xlabel={Valor Previsto ($\hat{y}$)},
                ylabel={Perda},
                axis lines=middle,
                grid=major,
                grid style={dashed, gray!40},
                xmin=0, xmax=25,
                % Ajustando ymin/ymax para focar na curvatura
                ymin=-14, ymax=-10, 
                legend pos=north west,
                title style={font=\bfseries\small},
                label style={font=\small},
                tick label style={font=\scriptsize},
                title={Visão 2D (Alvo $y=10$)}
            ]
                % Perda Poisson para y=10
                \addplot[domain=1:25, samples=100, color=blue, very thick] {x - 10*ln(x)};
                \addlegendentry{$y=10$}
                
                % Linha pontilhada no mínimo
                \draw[dashed, blue!60] (axis cs:10, -20) -- (axis cs:10, {10-10*ln(10)});
                 \node[anchor=north, font=\tiny, text=blue] at (axis cs:10, -13.2) {Mínimo em $\hat{y}=10$};
            \end{axis}
        \end{tikzpicture}
        \caption{Comportamento para valor real alto.}%
        \label{fig:poisson-2d-high}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \begin{tikzpicture}
            \begin{axis}[
                width=\linewidth,
                height=7cm,
                xlabel={$y$ (Real)},
                ylabel={$\hat{y}$ (Previsto)},
                zlabel={Loss},
                grid=major,
                view={135}{35},
                title style={font=\bfseries\small},
                label style={font=\small},
                tick label style={font=\scriptsize},
                title={Superfície 3D (Valores Altos)}
            ]
                % Formula: y - x * ln(y) (x=Real, y=Previsto)
                \addplot3[
                    mesh,           % Tipo de gráfico: superfície
                    color=blue,
                    shader=interp,  % Suaviza as cores
                    domain=-5:5,    % Domínio de x (y_real)
                    domain y=-5:5,  % Domínio de y (y_previsto)
                    samples=15      % Resolução da malha
                ] { y - x*ln(y) };
            \end{axis}
        \end{tikzpicture}
        \caption{Superfície local (Região $8 \le y \le 12$).}%
        \label{fig:poisson-3d-high}
    \end{subfigure}

    \caption{Comparação da perda de Poisson ($L = \hat{y} - y \ln \hat{y}$) para diferentes magnitudes de $y$. Observe como a convexidade é mais ``apertada'' para valores pequenos e se alarga para valores grandes.}%
    \label{fig:poisson-loss-grid}
    \fonte{O autor (2025).}
\end{figure}

\medskip
\begin{center}
 * * *
\end{center}
\medskip

\subsubsection*{Características da perda de Poisson}

\begin{itemize}
    \item \textbf{Convexa:} Umas das vantagens de se utilizar a perda de Poisson, está no fato dela ser uma função convexa, o que facilita para que os otimizadores baseados em gradiente consigam mais facilmente encontrar um ponto de mínimo global, ou chegar muito próximo dele, casos estejam configurados com os parâmetros ideais.
    \item \textbf{Continuidade e suavidade:} Além de ser uma função convexa, a perda de Poisson também é uma função contínua e suave, isso se dá devido ao fato de não fazer o uso de cálculos que possam causar ``bicos'' nos seus gráficos, como o uso de módulos para calcular o valor absoluto. Se a função é suave e contínua, ela também se torna chamativa para ser utilizada em conjunto com otimizadores baseados em gradiente, pois estes não encontrarão dificuldades para percorrer a superfície da função.
    \item \textbf{Não-negatividade e link functions:} \textcite{LossesArticle} explicam que como dados de contagem não podem ser negativos, deve-se atentar com os valores preditos pelo modelo $\hat{y}_j$, pois eles deverão estar restritos em $\hat{y}_j > 0$. Os autores explicam que em modelos lineares generalizados a função exponencial é comumente utilizada como uma link function para garantir que esses valores se encaixem nesse intervalo \parencite{LossesArticle}. Dito isso, é possível discutir uma versão alternativa para descrever a perda de Poisson considerando que $\text{exp}$ está sendo usada como uma link function.
    
    Sabemos que um modelo de aprendizado de máquina, geralmente retorna algo que segue o padrão mostrado na Equação xxx
 
    \begin{equation}
        \hat{y}_j = \textbf{W}^T x_j + b
    \end{equation}

    Esse valor pode ser negativo e queremos que ele fique em um intervalo $\hat{y} > 0$. Então como \textcite{LossesArticle} sugerem, uma das soluções é utilizar a função exponencial como uma link function, então escrevemos

    \[
        \hat{y}_j = \text{exp}(\textbf{W}^T x_j + b)
    \]

    Ao substituir essa expressão na Equação~\ref{eq:poisson-loss}, que mostra a função de perda de Poisson, é possível chegar Em

    \[
        \Loss_{\text{Poisson}}(y_j, \hat{y}_j) = \sum_{j = 0}^N \text{exp}(\textbf{W}^T x_j + b) - y_j \log(\text{exp}(\textbf{W}^T x_j + b))
    \]

    É possível simplificar os cálculos do logaritmo, resultando na expressão

    \[
        \Loss_{\text{Poisson}}(y_j, \hat{y}_j) = \sum_{j = 0}^N \text{exp}(\textbf{W}^T x_j + b) - y_j  (\text{exp}(\textbf{W}^T x_j + b))
    \]

    A qual pode ser reescrita uma última vez como 

    \[
        \Loss_{\text{Poisson}}(y_j, \hat{y}_j) = \sum_{j = 0}^N \text{exp}(\hat{y}_j) - y_j  (\text{exp}(\hat{y}_j))
    \]

\end{itemize}

\medskip
\begin{center}
 * * *
\end{center}
\medskip

\subsubsection*{Gradiente da perda de Poisson}

Vistas algumas propriedades dessa função, cabe também discutir a sua derivada, que será útil para a retropropagação do gradiente e o aprendizado do modelo. Calculando a derivada da perda de Poisson em Relação àos valores preditos pelo modelo, é possível chegar na Equação~\ref{eq:poisson-loss-derivada}.

\begin{equacaodestaque}{Derivada da perda de poisson}
    \frac{\partial\Loss_{\text{Poisson}}}{\partial\hat{y}_j} = 1 -\frac{y_j}{\hat{y}_j} = \frac{\hat{y}_j -y_j}{\hat{y}_j}%
    \label{eq:poisson-loss-derivada}
\end{equacaodestaque}

Com base nessa equação, o próximo passo é plotar o gráfico dessa derivada, o qual pode ser visto na Figura~\ref{fig:poisson-loss-derivada}. Perceba que o gráfico foge do padrão visto até agora, é apenas a vista em duas dimensões da derivada, além disso, ele também apresenta curvas diferentes, para demonstrar como a função varia conforme os valores reais $y_j$ variam. Note que com a variação dos valores reais a curva da derivada começa a deslocar pelo plano do gráfico, para valores menores, como $y = 10$, são mais presentes no quarto quadrante, já em valores mais altos, $y = 2$ por exemplo, vão se aproximando mais do eixo das ordenadas.

\begin{figure}[h!]
    \centering
    \begin{tikzpicture}
        \begin{axis}[
            xlabel={Valor Previsto ($\hat{y}$)},
            ylabel={Gradiente da Perda ($\frac{\partial L}{\partial \hat{y}}$)},
            axis lines=middle,
            grid=major,
            grid style={dashed, gray!40},
            xmin=-1, xmax=20,
            ymin=-4, ymax=1.5,
            legend pos=south east,
            width=12cm,
            height=9cm,
            title style={font=\bfseries},
            label style={font=\small},
            tick label style={font=\scriptsize}
        ]
            % Curva para y=2
            \addplot[domain=0.5:20, samples=101, color=red, thick] {1 - 2/x};
            \addlegendentry{$y=2$}
            
            % Curva para y=5
            \addplot[domain=0.5:20, samples=101, color=blue, thick] {1 - 5/x};
            \addlegendentry{$y=5$}

            % Curva para y=10
            \addplot[domain=0.5:20, samples=101, color=green, thick] {1 - 10/x};
            \addlegendentry{$y=10$}
            
        \end{axis}
    \end{tikzpicture}
    \caption{Gráfico da derivada da perda de Poisson. O gradiente é zero quando $\hat{y}=y$ e assintótico a 1 para $\hat{y} \to \infty$.}%
    \label{fig:poisson-loss-derivada}
    \fonte{O autor (2025).}
\end{figure}

Além disso, como foi visto anteriormente, a função exponencial é utilizada como uma link function para resolver o problema dos dados da saída do modelo $\hat{y}$ serem sempre positivos, de forma que é possível escrever uma nova expressão, e até mais simplificada para o cenário em que $\text{exp}$ é combinada com a perda de Poisson para servir como uma link function. Dessa forma, essa expressão pode ser vista na Equação xx.

\begin{equacaodestaque}{Derivada da perda de Poisson com exp como link function}
    \frac{\partial\Loss_{\text{Poisson}}}{\partial\hat{y}_j} = \hat{y}_j - y_j%
    \label{eq:poisson-loss-com-link-function-derivada}
\end{equacaodestaque}

Dessa forma, a derivada passa ser a diferença entre o valor predito pelo modelo $\hat{y}_j$ com o valor real $y$.

\medskip
\begin{center}
 * * *
\end{center}
\medskip

\subsubsection*{Aplicações da perda de Poisson em aprendizado de máquina}%
\index{Aplicações práticas! Perda de Poisson}

Por último, é importante discutir algumas das aplicações em que a perda de Poisson é utilizada. É nítido ressaltar que ela estará relacionada com a distribuição de Poisson e com dados que segue essa distribuição, ou seja, que possuem a média igual a mediana. Usar a perda de Poisson para um cenário em que os dados seguem uma distribuição normal ou outro tipo de distribuição, não é algo ideal, uma vez que já existem funções que trabalham com essas situações. Um exemplo disso é o erro quadrático absoluto e sua relação com a distribuição Gaussiana.

Dito isso, vale citar os trabalhos:

\begin{itemize}
    \item \textbf{Aplicação 1 (Área):} Em \textit{Poisson regression in epidemiology}, \textcite{poisson-regression-in-epidemiology} faz uso da regressão de poisson para estimar os efeitos dos fatores de risco em incidência ou em taxas de mortalidade. Além disso, a regressão de Poisson também é utilizada no artigo para avaliar a relação dose-resposta para variáveis que representam níveis quantitativos de exposição \parencite{poisson-regression-in-epidemiology};
    \item \textbf{Predição de taxas de crime (Área):} Já em \textit{Poisson-Based Regression Analysis of Aggregate Crime Rates} \textcite{poisson-regression-for-crime-rates} utiliza modelos baseados na regressão de Poisson para analisar taxas de crime agregadas. O autor explica que no cenário em que estava trabalhando, a técnica de regressão por mínimos quadrados (a qual faz uso do erro quadrático médio, visto no início do capítulo), não é ideal, uma vez o tamanho da população de uma unidade agregada é pequeno quando comparado relativamente com a taxa de ofensas; de forma que as taxas de crime devem ser computadas a partir de um pequeno número de ofensas \parencite{poisson-regression-for-crime-rates};
    \item \textbf{Predição de sinistros em planos de saúde privados (Área):} Por fim, no trabalho \textit{Poisson regression and Zero-inflated Poisson regression: application to private health insurance data} \textcite{poisson-regression-and-zero-inflated-poisson-regression} utilizam a regressão de Poisson e uma técnica de regressão conhecida como regressão de Poisson zero-inflada, que é útil para dados que possuem um excesso de zeros. Para comparar e entender melhor qual técnica de regressão é mais interessante, os autores trabalham com o número de sinistros em um plano de saúde privado, no caso dos pesquisadores, o número de sinistros apresenta sobredispersão devido a predominância de zeros no conjunto de dados, de forma que a regressão de Poisson zero-inflada provou ser mais eficiente para lidar com esse tipo de problema \parencite{poisson-regression-and-zero-inflated-poisson-regression}.
\end{itemize}

\subsection{Deviância Gamma (Gamma Devience)}%
\index{Funções de Perda!Deviância Gamma (\textit{Gamma Devience})}

Para falar da função de perda deviância gamma, antes é necessário compreender dois conceitos que se relacionam: a função gamma e a distribuição gamma. Para isso, eles serão explicados primeiro nesta seção.

\subsubsection*{A Função Gamma}%
\index{Função Gamma}

As origens da função gamma se remontam ao século XVIII, com a busca de vários matemáticos importantes daquele período em encontrar uma forma de extender a notação da função fatorial para além do conjunto dos números naturais. Nesse sentido, vale citar o trabalho \textit{De progressionibus transcendentibus seu quarum termini generales algebraice dari nequeunt} (Sobre progressões transcendentais, ou seja, aquelas cujos termos gerais não podem ser dados algebricamente em português) de Leonhard Euler, o qual hoje é reconhecido por introduzir o conceito de função gamma que existe atualmente. 

No texto, Euler define esta função como sendo dada por

\[
\int_0^{\infty} \left(\ln \left(\frac{1}{t} \right) \right)^x
\]

contudo, esta notação sofreu várias mudanças até ser chegado em um consenso entre os matemáticos. A notação que prevaleu até os dias atuais foi a de Legendre, que em seu trabalho xxx, definiu que a função gamma era denotada pela Equação~\ref{eq:gamma-function}, foi ele inclusive que utilizou a letra grega $\Gamma$ para se referir a essa função.

\begin{equacaodestaque}{Função gamma}
    \Gamma(z) = \int_{0}^{\infty} t^{z-1} e^{-t} \, dt
    \label{eq:gamma-function}
\end{equacaodestaque}

Uma das maiores vantagens de se utilizar a função gamma é poder calcular a função fatorial para termos além do conjunto dos naturais, como está demonstrado na Equação~\ref{eq:factorial-with-gamma-function}. Dessa forma, a função gamma serviu como uma espécie de ``ponte'', unindo o a teoria dos números, que trabalha com conjuntos discretos, com o cálculo, que trabalha com conjuntos contínuos. 

\begin{equacaodestaque}{Fatorial com a função gamma}
    n! = \Gamma(n+1)
    \label{eq:factorial-with-gamma-function}
\end{equacaodestaque}

Entretanto, a função gamma não serve só de utilidade para extender o fatorial para outros conjuntos numéricos. Com ela, também é possível descrever uma nova distribuição de valores: a distribuição gamma. Ela será tratada em seguida.

\medskip
\begin{center}
 * * *
\end{center}
\medskip

\subsubsection*{A distribuição gamma}%
\index{Distribuição Gamma}

\begin{equacaodestaque}{Distribuição gamma (PDF)}
    f(y_j; k, \theta) = \frac{y_j^{k-1} e^{-y_j/\theta}}{\theta^k\Gamma(k)}%
    \label{eq:gamma-distribution-pdf}
\end{equacaodestaque}

\begin{figure}[h!]
    \centering % Centraliza a figura inteira
    
    % --- GRÁFICO 1: Forma Baixa (k=1) ---
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \begin{tikzpicture}
            \begin{axis}[
                title={Distribuição Gamma ($k=1, \theta=2$)},
                xlabel={$x$ (valor contínuo)},
                ylabel={$f(x; k, \theta)$},
                smooth, % Linha contínua suave
                ymin=0,
                xmin=0, xmax=15,
                grid=major,
                grid style={dashed, gray!40},
                label style={font=\small},
                tick label style={font=\scriptsize}
            ]
            % Com k=1, theta=2: (x^0 * exp(-x/2)) / (2^1 * gamma(1)) = 0.5 * exp(-x/2)
            
            % <--- MUDANÇA AQUI ---
            % Substituindo a fórmula complexa pela sua função definida:
            \addplot[domain=0.01:15, samples=101, color=blue, thick] 
                { gamma_pdf(x, 1, 2) };
            \end{axis}
        \end{tikzpicture}
        \caption{Com forma $k=1$, a distribuição é uma exponencial decrescente.}%
        \label{fig:gamma-low-k}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \begin{tikzpicture}
            \begin{axis}[
                title={Distribuição Gamma ($k=3, \theta=2$)},
                xlabel={$x$ (valor contínuo)},
                ylabel={$f(x; k, \theta)$},
                smooth,
                ymin=0,
                xmin=0, xmax=20, % Range maior para ver a cauda
                grid=major,
                grid style={dashed, gray!40},
                label style={font=\small},
                tick label style={font=\scriptsize}
            ]
            % Com k=3, theta=2: (x^2 * exp(-x/2)) / (2^3 * gamma(3)) = (x^2 * exp(-x/2)) / 16
            
            % <--- MUDANÇA AQUI ---
            % Substituindo a fórmula complexa pela sua função definida:
            \addplot[domain=0.01:20, samples=101, color=red, thick] 
                { gamma_pdf(x, 3, 2) };
        
            % Média da Gamma = k * theta = 3 * 2 = 6
            % A altura do pico (0.166) foi calculada
            \draw[dashed, red!70] (axis cs:6, 0) -- (axis cs:6, 0.166); 
            \node[above, font=\tiny] at (axis cs:6, 0.166) {Média ($\mu=6$)};

            \end{axis}
        \end{tikzpicture}
        \caption{Com forma $k=3$, a distribuição tem um pico claro e uma cauda longa à direita (assimétrica).}%
        \label{fig:gamma-high-k}
    \end{subfigure}
    
    \caption{Comparação visual da Distribuição Gamma variando o parâmetro de forma $k$.}%
    \label{fig:gamma-comparison}
    \fonte{O autor (2025).}
\end{figure}

\medskip
\begin{center}
 * * *
\end{center}
\medskip

\begin{equacaodestaque}{Perda gamma (\textit{gamma loss})}
    \Loss_{\text{Gamma}}(y_j, \hat{y}_j) = \log\left(\frac{\hat{y}_j}{y_j}\right) + \frac{y_j}{\hat{y}_j} -1%
    \label{eq:gamma-loss}
\end{equacaodestaque}

Em que:

\begin{itemize}
    \item $y_j$ representa o valor real da j-ésima amostra;
    \item $\hat{y}_j$ representa o valor predito pelo modelo;
    \item $N$ representa o número de amostras.
\end{itemize}

\begin{figure}[h!]
    \centering
    \begin{tikzpicture}
        \begin{axis}[
            xlabel={Valor Previsto ($\hat{y}$)},
            ylabel={Perda Calculada},
            axis lines=middle,
            grid=major,
            grid style={dashed, gray!40},
            xmin=-1, xmax=20,
            ymin=-1, ymax=6,  % Ajustado para melhor visualização da Gamma
            legend pos=north east, % Movido para um local melhor
            width=12cm,
            height=9cm,
            title style={font=\bfseries},
            label style={font=\small},
            tick label style={font=\scriptsize}
        ]
            % Fórmula: ln(x/y) + (y/x) - 1
            
            % Curva para y=2
            \addplot[domain=0.2:20, samples=101, color=red, thick] {ln(x/2) + (2/x) - 1};
            \addlegendentry{$y=2$}
            % Mínimo em y=2, perda=0
            \draw[dashed, red!50] (axis cs:2, -1) -- (axis cs:2, 0);

            % Curva para y=5
            \addplot[domain=0.5:20, samples=101, color=blue, thick] {ln(x/5) + (5/x) - 1};
            \addlegendentry{$y=5$}
            % Mínimo em y=5, perda=0
            \draw[dashed, blue!50] (axis cs:5, -1) -- (axis cs:5, 0);

            % Curva para y=10
            \addplot[domain=1:20, samples=101, color=green, thick] {ln(x/10) + (10/x) - 1};
            \addlegendentry{$y=10$}
            % Mínimo em y=10, perda=0
            \draw[dashed, green!50] (axis cs:10, -1) -- (axis cs:10, 0);
            
        \end{axis}
    \end{tikzpicture}
    \caption{Gráfico da Perda Gamma para diferentes valores reais de $y$. O mínimo de cada curva (Perda = 0) ocorre em $\hat{y}=y$.}%
    \label{fig:gamma-loss}
    \fonte{O autor (2025).}
\end{figure}

\medskip
\begin{center}
 * * *
\end{center}
\medskip

\subsubsection*{Características da perda gamma}

\begin{itemize}
    \item \textbf{Característica 1:}
    \item \textbf{Característica 2:}
    \item \textbf{Característica 3:}
\end{itemize}

\medskip
\begin{center}
 * * *
\end{center}
\medskip

\subsubsection*{Gradiente da perda Gamma}

\begin{equacaodestaque}{Derivada da perda gamma}
    \frac{\partial\Loss_{\text{Gamma}}}{\partial\hat{y}_j} = \frac{1}{\hat{y}_j} -\frac{y_j}{\hat{y}_j^2} = \frac{\hat{y}_j -y_j}{\hat{y}_j^2}%
    \label{eq:gamma-loss-derivada}
\end{equacaodestaque}

\begin{figure}[h!]
    \centering
    \begin{tikzpicture}
        \begin{axis}[
            xlabel={Valor Previsto ($\hat{y}$)},
            ylabel={Gradiente da Perda ($\frac{\partial L}{\partial \hat{y}}$)},
            axis lines=middle,
            grid=major,
            grid style={dashed, gray!40},
            xmin=0, xmax=20,  % Ajustado para a derivada da Gamma
            ymin=-4, ymax=0.5,   % Ajustado para a derivada da Gamma
            legend pos=north east, % Movido para um local melhor
            width=12cm,
            height=9cm,
            title style={font=\bfseries},
            label style={font=\small},
            tick label style={font=\scriptsize}
        ]
            % Fórmula: (x - y) / (x^2)
            
            % Curva para y=2
            \addplot[domain=1.2:20, samples=101, color=red, thick] {(x - 2) / (x^2)};
            \addlegendentry{$y=2$}
            
            % Curva para y=5
            \addplot[domain=1.2:20, samples=101, color=blue, thick] {(x - 5) / (x^2)};
            \addlegendentry{$y=5$}

            % Curva para y=10
            \addplot[domain=1.2:20, samples=101, color=green, thick] {(x - 10) / (x^2)};
            \addlegendentry{$y=10$}
            
        \end{axis}
    \end{tikzpicture}
    \caption{Gráfico da derivada da Perda Gamma. O gradiente é zero quando $\hat{y}=y$ e assintótico a 0 para $\hat{y} \to \infty$.}%
    \label{fig:gamma-loss-derivada}
    \fonte{O autor (2025).}
\end{figure}

\medskip
\begin{center}
 * * *
\end{center}
\medskip

\subsubsection*{Aplicações da perda gamma em aprendizado de máquina}%
\index{Aplicações práticas! Deviância Gamma}

\begin{itemize}
    \item \textbf{Aplicação 1 (Área):}
    \item \textbf{Aplicação 2 (Área):}
    \item \textbf{Aplicação 3 (Área):}
    \item \textbf{Aplicação 4 (Área):}
\end{itemize}

\subsection{Perda de Tweedie (Tweedie Loss)}%
\index{Funções de Perda!Perda de Tweedie (\textit{Tweedie Loss})}

\begin{equacaodestaque}{Perda de Tweedie (\textit{Tweedie Loss})}
    \Loss_{\text{Tweedie}}(y_j, \hat{y}_j; p) = -\frac{y_j \cdot \hat{y}_j^{1-p}}{1-p} + \frac{\hat{y}_j^{2-p}}{2-p}%
    \label{eq:tweedie-loss}
\end{equacaodestaque}

Em que:

\begin{itemize}
    \item $y_j$ representa o valor real da j-ésima amostra;
    \item $\hat{y}_j$ representa o valor predito pelo modelo;
    \item $N$ representa o número de amostras.
\end{itemize}

\begin{figure}[h!]
    \centering
    \begin{tikzpicture}
        \begin{axis}[
            xlabel={Valor Previsto ($\hat{y}$)},
            ylabel={Perda Calculada},
            axis lines=middle,
            grid=major,
            grid style={dashed, gray!40},
            xmin=-1, xmax=15,
            ymin=0, ymax=15,
            legend pos=north west,
            width=12cm,
            height=9cm,
            title style={font=\bfseries},
            label style={font=\small},
            tick label style={font=\scriptsize}
        ]
            % Definição do parâmetro p
            \def\p{1.5}

            % Curva para y=1
            \addplot[domain=0.1:15, samples=101, color=red, thick] 
                { -1*x^(1-\p)/(1-\p) + x^(2-\p)/(2-\p) };
            \addlegendentry{$y=1$}
            \draw[dashed, red!50] (axis cs:1, 0) -- (axis cs:1, {-1*1^(1-\p)/(1-\p) + 1^(2-\p)/(2-\p)});

            % Curva para y=4
            \addplot[domain=0.1:15, samples=101, color=blue, thick] 
                { -4*x^(1-\p)/(1-\p) + x^(2-\p)/(2-\p) };
            \addlegendentry{$y=4$}
            \draw[dashed, blue!50] (axis cs:4, 0) -- (axis cs:4, {-4*4^(1-\p)/(1-\p) + 4^(2-\p)/(2-\p)});
            
        \end{axis}
    \end{tikzpicture}
    \caption{Gráfico da Perda de Tweedie para $p=1.5$. O mínimo de cada curva ocorre em $\hat{y}=y$.}%
    \label{fig:tweedie-loss}
    \fonte{O autor (2025).}
\end{figure}

\medskip
\begin{center}
 * * *
\end{center}
\medskip

\subsubsection*{Características da Perda de Tweedie}

\begin{itemize}
    \item \textbf{Característica 1:}
    \item \textbf{Característica 2:}
    \item \textbf{Característica 3:}
\end{itemize}

\medskip
\begin{center}
 * * *
\end{center}
\medskip

\subsubsection*{Gradiente da perda de Tweedie}

\begin{equacaodestaque}{Derivada da Perda de Tweedie}
    \frac{\partial\Loss_{\text{Tweedie}}}{\partial\hat{y}_j} = \hat{y}_j^{-p}(\hat{y}_j -y_j)%
    \label{eq:tweedie-loss-derivada}
\end{equacaodestaque}

\begin{figure}[h!]
    \centering
    \begin{tikzpicture}
        \begin{axis}[
            xlabel={Valor Previsto ($\hat{y}$)},
            ylabel={Gradiente da Perda ($\frac{\partial L}{\partial \hat{y}}$)},
            axis lines=middle,
            grid=major,
            grid style={dashed, gray!40},
            xmin=-1, xmax=15,
            ymin=-2, ymax=1.5,
            legend pos=south east,
            width=12cm,
            height=9cm,
            title style={font=\bfseries},
            label style={font=\small},
            tick label style={font=\scriptsize}
        ]
            % Definição do parâmetro p
            \def\p{1.5}
            
            % Curva para y=1
            \addplot[domain=0.2:15, samples=101, color=red, thick] 
                {x^(-\p)*(x-1)};
            \addlegendentry{$y=1$}
            
            % Curva para y=4
            \addplot[domain=0.2:15, samples=101, color=blue, thick] 
                {x^(-\p)*(x-4)};
            \addlegendentry{$y=4$}

            % Linhas verticais onde o gradiente é zero
            \draw[dashed, red!50] (axis cs:1, -2) -- (axis cs:1, 1.5);
            \draw[dashed, blue!50] (axis cs:4, -2) -- (axis cs:4, 1.5);
            
        \end{axis}
    \end{tikzpicture}
    \caption{Gráfico da derivada da Perda de Tweedie para $p=1.5$. O gradiente é zero quando a previsão é igual ao valor real.}%
    \label{fig:tweedie-loss-derivada}
    \fonte{O autor (2025).}
\end{figure}

\medskip
\begin{center}
 * * *
\end{center}
\medskip

\subsubsection*{Algumas Aplicações da Perda de Tweedie}%
\index{Aplicações práticas! Perda de Tweedie}

\begin{itemize}
    \item \textbf{Aplicação 1 (Área):}
    \item \textbf{Aplicação 2 (Área):}
    \item \textbf{Aplicação 3 (Área):}
    \item \textbf{Aplicação 4 (Área):}
\end{itemize}

\subsection{Divergência Kullback-Leibler}%
\index{Funções de Perda!Divergência de Kullback-Leibler}

\begin{equacaodestaque}{Divergência KL entre duas Gaussianas}
    D_{KL}(P || Q) = \log\frac{\sigma_2}{\sigma_1} + \frac{\sigma_1^2 + (\mu_1 -\mu_2)^2}{2\sigma_2^2} -\frac{1}{2}%
    \label{eq:kl-divergence-gaussiana}
\end{equacaodestaque}

\begin{figure}[h!]
    \centering
    \begin{tikzpicture}
        \begin{axis}[
            xlabel={Valor da Variável Contínua (y)},
            ylabel={Densidade de Probabilidade},
            axis lines=left,
            grid=major,
            grid style={dashed, gray!40},
            xmin=-4, xmax=6,
            ymin=0, ymax=0.5,
            legend pos=north east,
            width=12cm,
            height=9cm,
            title style={font=\bfseries},
            label style={font=\small},
            tick label style={font=\scriptsize}
        ]
            % Distribuição Real P
            \addplot[
                domain=-4:6, samples=101, color=blue, very thick,
                ] {exp(-(x-0)^2 / (2*1^2)) / (1 * sqrt(2*pi))};
            \addlegendentry{Distribuição Real $P \sim \mathcal{N}(\mu_1, \sigma_1^2)$}

            % Distribuição Prevista Q
            \addplot[
                domain=-4:6, samples=101, color=red, thick,
            ] {exp(-(x-1.5)^2 / (2*1.5^2)) / (1.5 * sqrt(2*pi))};
            \addlegendentry{Distribuição Prevista $Q \sim \mathcal{N}(\mu_2, \sigma_2^2)$}
            
        \end{axis}
    \end{tikzpicture}
    \caption{A Divergência KL mede a diferença entre a distribuição prevista pelo modelo ($Q$) e a distribuição real dos dados ($P$).}%
    \label{fig:kl-divergence-concept-regressao}
    \fonte{O autor (2025).}
\end{figure}

\medskip
\begin{center}
 * * *
\end{center}
\medskip

\subsubsection*{Características da Divergência de Kullback-Leibler} 

\begin{itemize}
    \item \textbf{Característica 1:}
    \item \textbf{Característica 2:}
    \item \textbf{Característica 3:}
\end{itemize}

\medskip
\begin{center}
 * * *
\end{center}
\medskip

\begin{equacaodestaque}{Derivadas da Divergência KL (Gaussiana)}
    \frac{\partial D_{KL}}{\partial \mu_2} = \frac{\mu_2 -\mu_1}{\sigma_2^2}
    \\[10pt] % Espaçamento vertical
    \frac{\partial D_{KL}}{\partial \sigma_2} = \frac{1}{\sigma_2} -\frac{\sigma_1^2 + (\mu_1 -\mu_2)^2}{\sigma_2^3}%
    \label{eq:kl-divergence-derivada-gaussiana}
\end{equacaodestaque}

\medskip
\begin{center}
 * * *
\end{center}
\medskip

\subsubsection*{Algumas Aplicações da Divergência de Kullback-Leibler em Problemas de Regressão}%
\index{Aplicações práticas! Divergência de Kullback-Leibler}

\begin{itemize}
    \item \textbf{Aplicação 1 (Área):}
    \item \textbf{Aplicação 2 (Área):}
    \item \textbf{Aplicação 3 (Área):}
    \item \textbf{Aplicação 4 (Área):}
\end{itemize}

\section{Comparativo: Funções de Perda para Regressão}

\begin{table}[htbp]
    \centering
    \begin{threeparttable}
        \caption{Comparativo das funções das funções de perda para problemas de regressão}%
        \label{tab:comparativo-funcoes-de-perda-para-regressao}

        \begin{tabularx}{\textwidth}{p{3.2cm} *{1}{>{\raggedright\arraybackslash}X}}
            \toprule
            \textbf{Função} & \textbf{Principais características} \\
            \midrule
            Erro quadrático médio (\textit{MSE}) & -  \\
            \addlinespace
            Erro absoluto médio (\textit{MAE}) & - \\
            \addlinespace
            Perda de Huber (\textit{Huber loss}) & - \\
            \addlinespace
            Perda log-cosh (\textit{Log-cosh loss}) & - \\
            \addlinespace
            Erro quadrático logarítmico médio (\textit{MSLE}) & - \\
            \addlinespace
            Perda quantílica (\textit{Quantile loss}) & - \\
            \addlinespace
            Perda epsilon-insensível (\textit{$\epsilon$-insentive}) & - \\
            \addlinespace
            Perda de Poisson (\textit{Poisson loss}) & - \\
            \addlinespace
            Deviância Gamma (\textit{Poisson loss}) & - \\
            \addlinespace
            Perda de Tweedie (\textit{Tweedie loss}) & - \\
            \addlinespace
            Divergência de Kullback-Leibler (\textit{KL-divergence}) & - \\
            \addlinespace
        \end{tabularx}
        
        \begin{tablenotes}[para]
            \small
            \item[] Fonte: O autor (2025).
        \end{tablenotes}

    \end{threeparttable}
\end{table}

\section{Fluxograma: Escolhendo a Função de Perda Ideal}

\medskip
\begin{center}
 * * *
\end{center}
\medskip

\subsubsection*{Indo Além das Funções de Perda Para Regressão}

Nesse capítulo foi visto uma série de funções de perdas que tinham uma característica em comum: são utilizadas para lidar com problemas de regressão. Contudo, esse não é o único conjunto de funções de perda que existe. Existem funções de perda que são ideais para problemas de classificação, seja ele com duas classes (binário), ou multi-classe, essas funções são o tópico central do Capítulo~\ref{cap:perda-classificacao}. Além disso, existem vários outros tipos de problemas, alguns bem mais específicos, como reconhecer objetos em uma foto. Dessa forma, o Capítulo~\ref{cap:perdas-especificas} busca fazer uma coletânea de outras funções de perda para situações mais específicas ao se construir um modelo de aprendizado de máquina.

Contudo, foram vistas apenas as funções de perda, e foi visto também que algumas podem servir como métricas, indicando de forma mais simples e direta se o modelo está performando bem ou não. Assim, um complemento para esse capítulo agora que você leitor conhece as funções de perda para regressão, é ler também o capítulo seguinte a este, o Capítulo~\ref{cap:metricas-de-avaliacao-para-regressao}. Ele é dedicado a explicar diferentes métricas que podem ser utilizadas para avaliar um modelo de regressão. De forma que será possível conhecer não só a função de perda ideal, mas também o conjunto de métricas ideais para avaliar o modelo desenvolvido.

\medskip
\begin{center}
 * * *
\end{center}
\medskip
