% ===================================================================
% Arquivo: capitulos/parte-III-pilares/cap-10-perda-binaria.tex
% ===================================================================

\chapter{Funções de perda para classificação}%
\label{cap:perda-classificacao}

\section{Exemplo ilustrativo:}

\section{Funções de perda para classificação binária}

\subsection{Entropia-cruzada binária (BCE)}%
\index{Funções de Perda!Entropia-cruzada binária (BCE)}%
\label{sec:binary-cross-entropy}

Para entender a função de perda \textbf{entropia-cruzada binária}, também chamada de \textbf{\textit{binary cross-entropy}} (\textbf{BCE}), é importante conhecer o conceito de Entropia, fundamental para o cálculo da BCE. Em \textit{A Matematical Theory of Comunication}, \textcite{EntropyShannon} estudava sobre formas eficientes de comunicação, em um dos momentos do artigo ele define o conceito de Entropia, como uma medida de incerteza, ou da ``escolha'', associada a um conjunto de eventos com determinada probabilidades. Essa fórmula está na Equação~\ref{eq:entropia-de-shannon}.

\begin{equacaodestaque}{Entropia de Shannon}
    H(p) = - k \sum_{i = 1}^{n} p_i \log pi%
    \label{eq:entropia-de-shannon}
\end{equacaodestaque}

Passado alguns anos, outros autores já estavam trabalhando com o conceito de entropia de Shannon. Um desses casos é o de \textcite{KullbackLeiblerDivergence}, que em \textit{On Information and Sufficiency} expandem o conceito de entropia para lidar com casos contínuos, além disso, introduzem uma medida para comparar duas distribuições de probabilidades $p$ e $q$, chamando-a de informação para discriminação. Essa medida futuramente, representada na Equação~\ref{eq:kl-divergence},  passou a ser conhecida com divergência de Kullback-Leibler (\textit{KL Divergence}).

\begin{equacaodestaque}{Divergência de Kullback-Leibler}
    I(1:2) = I_{1:2}(X) = \int f_1 (x) \log \frac{f_1(x)}{f_2 (x)} d \lambda (x)%
    \label{eq:kl-divergence}
\end{equacaodestaque}

A divergência KL mede a ``perda'' ou o ``excesso'' de informação quando é utilizada uma distribuição $q$ para aproximar a distribuição real $q$. A partir dessa fórmula chegaremos na definição de entropia-cruzada. Primeiro, utilizamos as propriedades do logaritmo para reescrever da divergência KL em

\[
    I(1:2) = I_{1:2}(X) = \int f_1 (x) (\log f_1 (x) - \log f_2 (x)) d\lambda (x)
\]

Em seguida, utilizando a propriedade distributiva, nós expandimos a equação, encontrando

\[
    I(1:2) = I_{1:2}(X) = \int f_1 (x) \log f_1 (x) - f_1 (x) \log f_2 (x) d \lambda (x)
\]

A partir dessa nova equação, separamos a integral em duas diferentes, chegando na expressão

\[
    I(1:2) = \int f_1 (x) \log f_1 (x) d\lambda (x) - \int f_1 (x) \log f_2 (x) d\lambda(x)
\]

O primeiro termo é quase igual a fórmula proposta por Shannon para a entropia, exceto por um sinal de menos, assim, o primeiro termo pode ser reescrito como $-H(f_1)$. Já o segundo termo é a própria definição de entropia-cruzada entre $f_1$ e $f_2$, portanto $H(f_1, f_2)$. A definição de entropia-cruzada está descrita na Equação~\ref{eq:cross-entropy}.

\begin{equacaodestaque}{Entropia-cruzada}
    H(f_1, f_2) = \int f_1 (x) \log f_2 (x) d\lambda(x)%
    \label{eq:cross-entropy}
\end{equacaodestaque}

Com base nesses dois termos, o da entropia-cruzada, e o da entropia de Shannon, reescrevemos mais uma vez a definição da Equação~\ref{eq:kl-divergence}, agora utilizando os termos resumidos. Temos então

\[
    I(1:2) = H(f_1, f_2) - H(f_2)
\]

que também pode ser escrita como

\[
    D_{KL} (f_1 || f_2) = H(f_1, f_2) - H(f_1)
\]

Dessa forma, dizemos que a divergência de Kullback-Leibler é a diferença entre a entropia-cruzada e a entropia de Shannon. Reescrevendo a equação mais uma vez é encontramos

\[
    H(f_1, f_2) = H(f_1) + I(1:2)
\]

Essa equação nos diz que o custo real de codificar os dados usando um modelo imperfeito $H(f_1, f_2)$ é igual ao custo de codificar usando um modelo perfeito $H(f_1)$ mais uma penalidade extra pela diferença entre o modelo perfeito e imperfeito. Concluímos que minimizar a divergência KL, é o mesmo que minimizar a entropia-cruzada em cenários de aprendizado de máquina. Isso se dá pois como a Entropia dos dados reais é uma constante, ao reduzir a entropia-cruzada, é ao mesmo tempo forçar a redução da divergência KL, aproximando mais o modelo dos dados da realidade.

A partir desses dois conceitos chegamos finalmente na entropia-cruzada binária, o tópico principal desta seção. A BCE é dada pela Equação \ref{eq:binary-cross-entropy}. A sua fórmula calcula a perda de forma elegante, aplicando o cálculo de duas entropias cruzadas. A primeira, $y \log(\hat{y})$, calcula a distância de uma classe 0 com o retorno do modelo $\hat{y}$. A segunda, $(1 - y) \log(1 - \hat{y})$, calcula a distância para os casos em que está sendo analisada a classe 1. Dessa forma, em um cenário em que o resultado é a classe 0, a primeira entropia-cruzada é toda multiplicado por zero, sendo eliminada da fórmula. O mesmo vale para quando o resultado é a classe 1, em que a segunda entropia-cruzada passa a ser zero.

\begin{equacaodestaque}{Entropia-cruzada binária}
    \mathcal{L}_{\text{BCE}} (y, \hat{y}) = -[y \log(\hat{y}) + (1 - y) \log(1 - \hat{y})]%
    \label{eq:binary-cross-entropy}
\end{equacaodestaque}

Em que

\begin{itemize}
    \item $y$ representa os rótulos dos dados, segue o formato $y \in {-1, +1}$;
    \item $\hat{y}$ representa a probabilidade prevista para a amostra.
\end{itemize}

Assim como visto no Capítulo~\ref{cap:perda-regressao}, neste capítulo representaremos as perdas para uma única predição. Contudo, no cenário de aprendizado de máquina, também podemos representar as perdas como a média do cálculo individual das perdas, como em

\[
    \mathcal{L}_{\text{BCE}} = \frac{1}{N} \sum_{j=1}^{N} \mathcal{L}_{\text{BCE}} (y_j, \hat{y}_j)
\]

A BCE é utilizada em de aprendizado de máquina há um bom tempo. Um dos trabalhos que cita o uso dessa função para ser utilizada classificação binária é o \textit{Connectionist Learning Procedures}, nele, \textcite{HintonConnectionist} explica que ao minimizar a entropia-cruzada binária para as distribuições do resultado desejado e o atual resultado era semelhante a maximizar a a verossimilhança do modelo gerar as saídas corretas.

O gráfico da entropia-cruzada binária apresenta duas curvas distintas, como evidenciado pela Figura~\ref{fig:binary-cross-entropy}. A primeira curva é responsável por indicar a probabilidade prevista pelo modelo para a classe zero. Enquanto a segunda, é referente à probabilidade prevista para a classe um. Elas são dependentes uma da outra, dado que a probabilidade para uma determinada classe será sempre 1 menos a probabilidade prevista para a outra classe.

\begin{figure}
    \begin{tikzpicture}
        \begin{axis}[
            xlabel={Probabilidade Prevista ($\hat{y}$)},
            ylabel={Perda ($\mathcal{L}$)},
            axis lines=left,            
            grid=major,                
            grid style={dashed, gray!40},  
            xmin=0, xmax=1,             
            ymin=0, ymax=5,             
            legend pos=north west,     
            width=\linewidth,
            height=7cm,
            title style={font=\bfseries},
            label style={font=\small},
            tick label style={font=\scriptsize}
        ]
            \addplot[
                domain=0.01:0.999,
                samples=100,
                color=black,
                very thick
            ] {-ln(x)};
            \addlegendentry{Classe 1 ($\mathcal{L} = -\log(\hat{y})$)}

            \addplot[
                domain=0.001:0.99,
                samples=100,
                color=black,
                dashed
            ] {-ln(1-x)};
            \addlegendentry{Classe 0 ($\mathcal{L} = -\log(1-\hat{y})$)}
            
        \end{axis}
    \end{tikzpicture}
    \caption{Visualizações da função de perda entropia-cruzada binária no plano.}
    \label{fig:binary-cross-entropy}
    \fonte{O autor (2025).}
\end{figure}

\subsubsection*{Características da entropia-cruzada binária}

\begin{description}[style=sameline, leftmargin=1.5em, font=\bfseries\color{black}] 
    \item[Continuidade, suavidade e diferenciabilidade:] A BCE é uma função de classe $C^{\infty}$, \textbf{infinitamente diferenciável} no domínio aberto $\hat{y} \in (0, 1)$ (ver Apêndice~\ref{ap:deducoes-perdas-derivadas-da-kl-divergence}). Podemos garantir que a saída do modelo esteja em um intervalo $(0,1)$ utilizando a função de ativação sigmoide logística na última camada. Fazendo isso, asseguramos a diferenciabilidade da perda, facilitando a otimização por gradiente.
    \item[Convexidade:] A BCE é \textbf{convexa} em relação predições $\hat{y}$ (ver Apêndice~\ref{ap:deducoes-perdas-derivadas-da-kl-divergence}). Sendo uma função convexa, a perda apresentará apenas um ponto de mínimo global, com formato favorável a utilização de otimizadores baseados em gradiente. Contudo, não é possível garantir que a BCE continuará sendo convexa em modelos de aprendizado profundo, devido às transformações não-lineares que ocorrem nas camadas.
    \item[Robustez:] A BCE não é Lipschitz-contínua, sendo \textbf{sensível a erros confiantes} (ver Apêndice~\ref{ap:deducoes-perdas-derivadas-da-kl-divergence}). Diferente das perdas para regressão, que podem ser sensível a \textit{outliers}, as perdas de classificação podem ser sensíveis a erros confiantes. Dado que a BCE possui a tendência de punir agressivamente erros confiantes, isso gera gradientes grandes em magnitude, podendo ocasionar explosões de gradientes.
    \item[Origem na teoria da informação:] Como foi visto no início da seção a BCE está estritamente ligada com o conceito de Entropia de Shannon. \textcite{LossesArticle} explicam que essa função mede a distância entre duas distribuições de Bernoulli, a distribuição real $P(y)$ e a distribuição de predições $Q(y)$.
    \item[Pune erros confiantes:] Quando $\hat{y}$ é perto de 1, mas o valor real é o oposto, neste caso, $y = 0$, o termo logarítmico $\log(\hat{y})$ ou $\log(1 - \hat{y})$ fica grande em magnitude, penalizando muito os erros confiantes. Essa relação também vale para o contrário, em que $\hat{y}$ é perto de 0, mas $y = 1$.
    \item[Desbalanceamento de classes:] Em cenários que uma classe é significativamente mais presente que outra, a BCE pode gerar modelos enviesados \parencite{LossesArticle}. Como solução, é possível utilizar a função de perda entropia-cruzada binária ponderada (ver Seção~\ref{sec:binary-weighted-cross-entropy}), essa função busca resolver o problema do desbalanceamento de classes aplicando pesos para as diferentes classes do problema estudado.
\end{description}

\subsubsection*{Gradiente da entropia-cruzada binária}

A derivada parcial da entropia-cruzada em relação à predição está na Equação~\ref{eq:binary-cross-entropy-derivada}. É a partir dela que é formado o vetor gradiente, que será retropropagado por toda a rede, da camada de saída até a camada de entrada, ajustando os parâmetros do modelo.

\begin{equacaodestaque}{Gradiente da entropia-cruzada binária em relação à predição}
    \nabla_{\hat{y}} \mathcal{L}_{\text{BCE}} (y, \hat{y}) = \frac{\hat{y} - y}{\hat{y}(1 - \hat{y})}
    \label{eq:binary-cross-entropy-derivada}
\end{equacaodestaque}

Essa equação explicita o motivo da BCE não ser Lipschitz-contínua, pois quando $\hat{y}$ for um valor muito pequeno e $y = 1$ temos

\[
    \lim_{\hat{y} \to 0} \frac{\hat{y} - 1}{\hat{y}(1 - \hat{y})} = - \infty
\]

Analogamente, quando $\hat{y}$ for um valor próximo de 1 e $y = 0$ temos

\[
    \lim_{\hat{y} \to 0} \frac{1}{(1 - \hat{y})} = + \infty
\]

Assim, uma maneira de tratar a explosão do gradiente ao calcular a derivada da perda é utilizar a sigmoide na camada de saída. Visto que a sigmoide irá garantir que as saídas do modelo estejam no intervalo $[0, 1]$, além disso, ao combinar essa função de ativação com a BCE é garantido um cálculo mais simples para o gradiente. Essa nova derivada está representada na Equação~\ref{eq:bce-derivada-com-sigmoide}, o gradiente torna-se a diferença da predição com o valor real.

\begin{equacaodestaque}{Gradiente da entropia-cruzada binária em relação à predição com sigmoide logística}
    \nabla_{\hat{y}} \mathcal{L}_{\text{BCE}} (y, \hat{y}) = \hat{y} - y
    \label{eq:bce-derivada-com-sigmoide}
\end{equacaodestaque}

Esse gradiente será sempre limitado, pois $\hat{y}$ está entre 0 e 1 e $y$ é 1 ou 0, de forma que o gradiente estará no intervalo $[-1, 1]$.

Essa situação na qual ocorre a explosão do gradiente está apresentada na Figura~\ref{fig:binary-cross-entropy}. Existem duas assíntotas verticais em 0 e 1 no gráfico, indicando que o gradiente cresce indefinidamente quando se aproxima desses pontos.

\begin{figure}[h!]
    \centering
    \begin{tikzpicture}
        \begin{axis}[
            xlabel={Probabilidade Prevista ($\hat{y}$)},
            ylabel={Gradiente ($\nabla_{\hat{y}} \mathcal{L}$)},
            axis lines=middle,
            grid=major,
            grid style={dashed, gray!40},
            xmin=-0.1, xmax=1.1,
            ymin=-30, ymax=30, % Aumentar o range do y para ver a assíntota
            legend pos=outer north east,
            width=\linewidth,
            height=7cm,
            title style={font=\bfseries},
            label style={font=\small},
            tick label style={font=\scriptsize}
        ]
            % Derivada para a classe real y=1
            % Fórmula: (y_hat - 1) / (y_hat * (1 - y_hat)) = -1 / y_hat
            \addplot[
                domain=0.005:1, % Domínio para evitar divisão por zero
                samples=100,
                color=black,
                very thick
            ] {-1/x};
            \addlegendentry{Classe 1}

            % Derivada para a classe real y=0
            % Fórmula: (y_hat - 0) / (y_hat * (1 - y_hat)) = 1 / (1 - y_hat)
            \addplot[
                domain=0:0.99, % Domínio para evitar divisão por zero
                samples=100,
                color=black,
                dashed
            ] {1/(1-x)};
            \addlegendentry{Classe 0}
            
        \end{axis}
    \end{tikzpicture}
    \caption{Visualização do gradiente de perda entropia-cruzada binária.}
    \label{fig:binary-cross-entropy-derivada}
    \fonte{O autor (2025).}
\end{figure}

\subsubsection*{Aplicações da entropia-cruzada binária}%
\index{Aplicações práticas! Entropia Cruzada Binária}

\begin{description}[style=sameline, leftmargin=1.5em, font=\bfseries\color{black}] 
    \item[Aplicação 1 (Área):]
    \item[Aplicação 2 (Área):]
    \item[Aplicação 3 (Área):]
\end{description}

\medskip

Como dito na seção de propriedades da perda BCE, ela apresenta a tendência de enviesar o aprendizado do modelo em cenários em que um classe é mais comum que a outra. Nestes casos, uma solução é utilizar a sua versão ponderada, a entropia-cruzada binária ponderada.

\subsection{Entropia-cruzada binária ponderada (WBCE)}%
\index{Funções de Perda!Entropia-cruzada binária ponderada (WBCE)}%
\label{sec:binary-weighted-cross-entropy}

A \textbf{entropia-cruzada binária ponderada}, também chamada de \textbf{\textit{weighted binary cross-entropy}} (\textbf{WBCE}), é uma função de perda para classificação binária que tem como objetivo ser utilizada em casos de desbalanceamento de classes nos dados. Para fazer isso, a BCE adiciona pesos a cada uma das classes, como na Equação~\ref{eq:binary-weighted-cross-entropy}. A classe menos presente recebe um peso maior, consequentemente seu gradiente também será maior em magnitude, forçando o modelo a aprender mais sobre os detalhes da classe em minoria. textcite{LossesArticle} explicam que a WBCE é utilizada em casos que os erros são caros e críticos.

\begin{equacaodestaque}{Entropia-cruzada binária ponderada}
    \mathcal{L}_{\text{WBCE}} (y, \hat{y}) = - [\alpha_1 y \log (\hat{y}) + \alpha_0 (1 - y) \log (1 - \hat{y})]
    \label{eq:binary-weighted-cross-entropy}
\end{equacaodestaque}

Em que

\begin{description}[style=sameline, leftmargin=1.5em, font=\bfseries\color{black}] 
    \item $y$ representa os rótulos dos dados, segue o formato $y \in {-1, +1}$;
    \item $\hat{y}$ representa a probabilidade prevista para a amostra.
    \item[$N$] representa o número de amostras;
    \item[$\alpha_0$] representa o peso para a classe 0;
    \item[$\alpha_1$] representa o peso para a classe 1.
\end{description}


Conforme são ajustados os hiperparâmetros $\alpha_0$ e $\alpha_1$, o gráfico que antes era simétrico, passa a apresentar uma curva maior que a outra. Como a Figura~\ref{fig:comparativo-entropia-cruzada-ponderada-binaria} mostra, a curva que atinge uma menor altura é a que apresenta um baixo valor para o peso $\alpha$, e vice-versa. Essa característica é essencial para permitir um gradiente em maior magnitude e com isso um melhor aprendizado da classe em desvatagem numérica.

\begin{figure}[h!]
    \centering
    % Figura da Esquerda (Peso maior para a Classe 0)
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \begin{tikzpicture}
            \def\alphaZero{5.0} % Peso alto para a classe 0
            \def\alphaUm{1.0}   % Peso normal para a classe 1
            \begin{axis}[
                xlabel={Probabilidade Prevista ($\hat{y}$)},
                ylabel={Perda $\mathcal{L}$},
                axis lines=left,
                grid=major,
                grid style={dashed, gray!40},
                xmin=0, xmax=1,
                ymin=0, ymax=25, % Ajustar ymax para a curva mais íngreme
                legend pos=north east,
                width=\textwidth,
                label style={font=\small},
                tick label style={font=\scriptsize},
                title style={font=\bfseries, yshift=-5pt},
            ]
                % Curva para y=1
                \addplot[
                    domain=0.01:0.999, samples=100, color=black, very thick
                ] {-\alphaUm*ln(x)};
                \addlegendentry{$y=1$}

                % Curva para y=0
                \addplot[
                    domain=0.001:0.99, samples=100, color=black, dashed
                ] {-\alphaZero*ln(1-x)};
                \addlegendentry{$y=0$}
            \end{axis}
        \end{tikzpicture}
        \caption{Alto peso para a classe 0 ($\alpha_0=5.0, \alpha_1=1.0$).}
        \label{fig:comparativo-entropia-cruzada-ponderada-binaria-com-alto-peso-para-classe-0}
    \end{subfigure}
    \hfill 
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \begin{tikzpicture}
            \def\alphaZero{1.0} % Peso normal para a classe 0
            \def\alphaUm{5.0}   % Peso alto para a classe 1
            \begin{axis}[
                xlabel={Probabilidade Prevista ($\hat{y}$)},
                ylabel={Perda $\mathcal{L}$},
                axis lines=left,
                grid=major,
                grid style={dashed, gray!40},
                xmin=0, xmax=1,
                ymin=0, ymax=25, % Manter a mesma escala de y
                legend pos=north west,
                width=\textwidth,
                label style={font=\small},
                tick label style={font=\scriptsize},
                title style={font=\bfseries, yshift=-5pt},
            ]
                % Curva para y=1
                \addplot[
                    domain=0.01:0.999, samples=100, color=black, very thick
                ] {-\alphaUm*ln(x)};
                \addlegendentry{$y=1$}

                % Curva para y=0
                \addplot[
                    domain=0.001:0.99, samples=100, color=black, dashed
                ] {-\alphaZero*ln(1-x)};
                \addlegendentry{$y=0$}
            \end{axis}
        \end{tikzpicture}
        \caption{Alto peso para a classe 1 ($\alpha_0=1.0, \alpha_1=5.0$).}
        \label{fig:comparativo-entropia-cruzada-ponderada-binaria-com-alto-peso-para-classe-1}
    \end{subfigure}
    
    \caption{Visualizações da função de perda entropia-cruda binária ponderada para diferentes valores de $\alpha_0$ e $\alpha_1$ }
    \label{fig:comparativo-entropia-cruzada-ponderada-binaria}
\end{figure}
\subsubsection*{Características da entropia-cruzada binária ponderada}

Por ser uma variante da BCE, a entropia-cruzada binária ponderada herda suas características da função primária. 

\begin{description}[style=sameline, leftmargin=1.5em, font=\bfseries\color{black}] 
    \item[Continuidade, suavidade e diferenciabilidade:] A WBCE é uma função de classe $C^{\infty}$, \textbf{infinitamente diferenciável} no domínio aberto $\hat{y}_i \in (0, 1)$ (ver Apêndice~\ref{ap:deducoes-perdas-derivadas-da-kl-divergence}). Dessa forma, ao utilizar a WBCE tanto com otimizadores baseados em gradiente, quanto em otimizadores de segunda ordem, não serão encontrados problemas de não-diferenciabilidade, facilitando a aplicação desses métodos.
    \item[Convexidade:] A WBCE é uma função \textbf{convexa} em relação predições $\hat{y}$ (ver Apêndice~\ref{ap:deducoes-perdas-derivadas-da-kl-divergence}). Esta é uma vantagem para a WBCE caso seja combinada com otimizadores baseados em gradiente. Contudo, devido às transformações não-lineares, sua convexidade pode ser afetada caso seja utilizada em redes neurais.
    \item[Robustez:] A WBCE não é Lipschitz-contínua, sendo \textbf{sensível a erros confiantes} (ver Apêndice~\ref{ap:deducoes-perdas-derivadas-da-kl-divergence}). Assim como BCE, erros confiantes do modelo podem enviesar o treinamento, gerando explosões de gradientes e o mal ajuste dos parâmetros da rede.
\end{description}

\subsubsection*{Gradiente da entropia-cruzada binária ponderada}

A derivada parcial da WBCE em relação à predição é calculada através da Equação~\ref{eq:binary-weighted-cross-entropy-derivada}. Sua fórmula é semelhante à da BCE com a adição dos hiperparâmetros $\alpha$ que são tratados como constantes na diferenciação da Equação~\ref{eq:binary-weighted-cross-entropy}.

\begin{equacaodestaque}{Gradiente da entropia-cruzada binária ponderada em relação à predição}
    \nabla_{\hat{y}} \mathcal{L}_{\text{WBCE}} = \frac{\alpha_0(1-y)\hat{y} - \alpha_1 y(1-\hat{y})}{\hat{y}(1-\hat{y})}
    \label{eq:binary-weighted-cross-entropy-derivada}
\end{equacaodestaque}

De forma semelhante à BCE, a WBCE pode ser aplicada em conjunto com a sigmoide logística garantir a saída do modelo no intervalo $[0,1]$ e com isso, evitar o problema dos gradientes explosivos. Essa variante da derivada está na Equação~\ref{eq:wbce-gradiente-com-simgoide}.

\begin{equacaodestaque}{Gradiente da entropia-cruzada binária ponderada em relação à predição}
    \frac{\nabla}{\nabla \hat{y}} \mathcal{L}_{\text{WBCE}} = 
        \begin{cases}
            \alpha_0 \cdot \hat{y} & \text{se} \quad y = 0 \\
            \alpha_1 \cdot (\hat{y} - 1) & \text{se} \quad y = 1 \\
        \end{cases}
    \label{eq:wbce-gradiente-com-simgoide}
\end{equacaodestaque}

Como a Figura~\ref{fig:binary-weighted-cross-entropy-derivada-comparacao} apresenta, as curvas do gradiente da WBCE também são assimétricas. A curva mais alta, é que possui o maior valor de $\alpha$. Ao gerar um gradiente em maior magnitude para a classe em minoria, a WBCE permite uma variação maior na atualização dos parâmetros para esse classe. A classe em maioria aparece mais, e consequentemente gera mais atualizações nos pesos e vieses. Ao adicionar pesos, a WBCE garante que as duas classes recebam o mesmo grau de importância. Essa ideia é parecida com a dos otimizadores baseados em gradientes adaptativos, como o AdaGrad, vistos no Capítulo~\ref{cap:retropropagacao-gradiente}.

\begin{figure}[h!]
    \centering
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \begin{tikzpicture}
            \def\alphaZero{5.0} % Peso alto para a classe 0
            \def\alphaUm{1.0}   % Peso normal para a classe 1
            \begin{axis}[
                title={Derivada da WCE com $\alpha_0=5.0, \alpha_1=1.0$},
                xlabel={Probabilidade Prevista ($\hat{y}$)},
                ylabel={Gradiente ($\nabla_{\hat{y}} \mathcal{L}$)},
                axis lines=middle,
                grid=major,
                grid style={dashed, gray!40},
                xmin=-0.1, xmax=1.1,
                ymin=-55, ymax=55, % Aumentar range para ver o efeito
                legend pos=outer north east,
                width=\textwidth,
                label style={font=\small},
                tick label style={font=\scriptsize},
                title style={font=\bfseries, yshift=-5pt},
            ]
                % Derivada para y=1
                \addplot[
                    domain=0.02:1, samples=100, color=black, very thick
                ] {-\alphaUm/x};
                \addlegendentry{$y=1$}

                % Derivada para y=0
                \addplot[
                    domain=0:0.98, samples=100, color=black, dashed
                ] {\alphaZero/(1-x)};
                \addlegendentry{$y=0$}
            \end{axis}
        \end{tikzpicture}
        \caption{Gradiente amplificado para erros na classe 0.}
        \label{fig:wce-derivada-alpha0}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \begin{tikzpicture}
            \def\alphaZero{1.0} % Peso normal para a classe 0
            \def\alphaUm{5.0}   % Peso alto para a classe 1
            \begin{axis}[
                title={Derivada da WCE com $\alpha_0=1.0, \alpha_1=5.0$},
                xlabel={Probabilidade Prevista ($\hat{y}$)},
                ylabel={Gradiente ($\nabla_{\hat{y}} \mathcal{L}$)},
                axis lines=middle,
                grid=major,
                grid style={dashed, gray!40},
                xmin=-0.1, xmax=1.1,
                ymin=-55, ymax=55, % Manter a mesma escala de y
                legend pos=outer north east,
                width=\textwidth,
                label style={font=\small},
                tick label style={font=\scriptsize},
                title style={font=\bfseries, yshift=-5pt},
            ]
                % Derivada para y=1
                \addplot[
                    domain=0.02:1, samples=100, color=black, very thick
                ] {-\alphaUm/x};
                \addlegendentry{$y=1$}

                % Derivada para y=0
                \addplot[
                    domain=0:0.98, samples=100, color=black, dashed
                ] {\alphaZero/(1-x)};
                \addlegendentry{$y=0$}
            \end{axis}
        \end{tikzpicture}
        \caption{Gradiente amplificado para erros na classe 1.}
        \label{fig:wce-derivada-alpha1}
    \end{subfigure}
    
    \caption{Comparação do gradiente da entropia-cruzada ponderada para diferentes valores de $\alpha_0$ e $\alpha_1$.}
    \label{fig:binary-weighted-cross-entropy-derivada-comparacao}
\end{figure}

\subsubsection*{Aplicações da entropia-cruzada binária ponderada}%
\index{Aplicações práticas! Entropia-cruzada binária ponderada}

\begin{description}[style=sameline, leftmargin=1.5em, font=\bfseries\color{black}] 
    \item[Aplicação 1 (Área):]
    \item[Aplicação 2 (Área):]
    \item[Aplicação 3 (Área):]
\end{description}

\subsection{Perda Hinge (Hinge loss)}%
\index{Funções de Perda!Perda hinge}

A \textbf{perda \textit{hinge}} é função de perda para de máquinas de vetores de suporte de classificação, sendo proposta no artigo \textit{Support-Vector Networks} dos autores \textcite{HingeLoss}. No trabalho, \textcite{HingeLoss} introduzem um problema os dados analisados podem ser separados sem erros, de forma que os autores querem então separar esses dados gerando a menor quantidade possível de erros. Para isso, \textcite{HingeLoss} introduzem um conjunto de variáveis não-negativas $\xi_i \ge 0, i = 1, 2, \cdots, l$, e também a função que querem minimizar, denotada por

\[
    `\Phi ' = \sum_{i = 1}^l \xi_i^{\sigma}
\]

Para $\sigma > 0$, são sujeitas as restrições:

\[
    y_i (\textbf{w} \cdot \textbf{x}_i + b) \ge 1 - \xi_i, i = 1, 2, \cdots, l
\]

\[
    \xi \ge 0, i = 1, 2, \cdots, l
\]

Para que $\xi$ siga as restrições propostas, ele deve ser maior ou igual às restrições. Como ele deve ser o menor possível para minimizar o erro, seu menor tamanho é o máximo entre 0 e $1 - y(\textbf{w} \cdot \textbf{x}_i + b)$. Escrevemos algo da forma

\[
    \xi = \max (0, 1 - y(\textbf{w} \cdot \textbf{x}_i + b))
\]

Essa função encontrada ao buscar o mínimo entre esses dois termos, ficou conhecida como \textbf{\textit{hinge loss}}. A Equação~\ref{eq:hinge-loss} descreve essa função com as notações desta obra.

\begin{equacaodestaque}{Perda hinge}
    \mathcal{L}_{\text{hinge}} (y, f(x)) = \max(0, 1 - y \cdot f(x))
    \label{eq:hinge-loss}
\end{equacaodestaque}

Em que

\begin{description}[style=sameline, leftmargin=1.5em, font=\bfseries\color{black}] 
    \item[$y$] representa os rótulos dos dados, segue o formato $y \in {-1, +1}$;
    \item[$f(x)$] representa a saída bruta do modelo ou a função de decisão (geralmente the signed distance da barreira de decisão)
\end{description}

\textcite{LossesArticle} explicam que para calcular a margem para uma amostra $x$, deve ser calculado o produto $yf(x)$, uma classificação correta com uma margem suficiente gera um resultado positivo para esse produto, reduzindo a perda para zero.

Diferente dos gráficos da BCE que apresentam curvas suaves devido ao uso dos logaritmos, a Figura~\ref{fig:hinge-loss} evidencia que a perda \textit{hinge} é composta por retas.

\begin{figure}
    \begin{tikzpicture}
        \begin{axis}[
            xlabel={Saída Bruta do Modelo ($f(x)$)},
            ylabel={Perda ($\mathcal{L}$)},
            axis lines=middle,    
            grid=major,             
            grid style={dashed, gray!40},
            xmin=-3.5, xmax=3.5,      
            ymin=-0.5, ymax=4.5,       
            legend pos=north west,    
            width=\linewidth,
            height=7cm,             
            title style={font=\bfseries},
            label style={font=\small},
            tick label style={font=\scriptsize}
        ]
            % Curva para a classe real y=+1
            \addplot[
                domain=-3:3, 
                samples=100, 
                color=black, 
                very thick
            ] {max(0, 1-x)};
            \addlegendentry{Classe Real = 1}

            % Curva para a classe real y=-1
            \addplot[
                domain=-3:3, 
                samples=100, 
                color=black, 
                dashed
            ] {max(0, 1+x)};
            \addlegendentry{Classe Real = -1}
            
            % Opcional: Linhas tracejadas para marcar as margens
            \draw[dashed, gray!70] (axis cs:1, 0) -- (axis cs:1, 4.5);
            \draw[dashed, gray!70] (axis cs:-1, 0) -- (axis cs:-1, 4.5);
            \node[above, gray!80, font=\tiny, rotate=90] at (axis cs:1.1, 2) {Margem};
            \node[above, gray!80, font=\tiny, rotate=90] at (axis cs:-0.9, 2) {Margem};
            
        \end{axis}
    \end{tikzpicture}
    \caption{Visualização da função de perda \textit{hinge} no plano.}
    \label{fig:hinge-loss}
    \fonte{O autor (2025).}
\end{figure}

\subsubsection*{Características da perda hinge}

\begin{description}[style=sameline, leftmargin=1.5em, font=\bfseries\color{black}] 
    \item[Continuidade, suavidade e diferenciabilidade:] A perda \textit{hinge} é uma função de classe $C^0$, \textbf{contínua} (ver Apêndice~\ref{ap:deducoes-perda-hinge}). Por utilizar a função $\max$ em sua fórmula, essa função de perda apresenta uma ``quinas'' em seu relevo, fazendo com que a perda \textit{hinge} não seja totalmente suave. Um fato que pode dificultar a otimização utilizando métodos de segunda ordem.
    \item[Convexidade:] A perda \textit{hinge} é uma função \textbf{convexa} (ver Apêndice~\ref{ap:deducoes-perda-hinge}). A função $\max$ une duas funções convexas, garantindo como resultado uma função também convexa. Ao utilizar a perda \textit{hinge} em máquinas de vetores de suporte, é possível garantir um único ponto de mínimo.
    \item[Robustez:] A perda Hinge é uma função 1-Lipschitz-contínua, portanto, é \textbf{robusta a erros confiantes} (ver Apêndice~\ref{ap:deducoes-perda-hinge}). Ao derivar a perda \textit{hinge} e calcular o seu gradiente no infinito, é visto que ele não cresce indefinidamente. Pelo contrário, ele fica constante em 1.
    \item[Esparsidade:] Semelhante a perda $\epsilon$-insensível (ver Seção-xx) a perda \textit{hinge} promove gradientes esparsos (ver Apêndice-xx). Isso se dá devido à uma zona de perda zero que ocorre com o objetivo  
\end{description}

\subsubsection*{Gradiente da perda Hinge}

Da perda \textit{hinge} calculamos o seu gradiente em relação à saída bruta do modelo $f(x)$, o qual é dado através de uma função por partes. Como essa função apresenta um ponto de não-diferenciabilidade no caso $y \cdot f(x) = 1$ utilizamos o subgradiente para definir o intervalo de resultados possíveis para a derivada neste ponto. Essa derivada está na Equação~\ref{eq:hinge-loss-derivada}

\begin{equacaodestaque}{Gradiente da perda Hinge em relação à saída bruta do modelo}
    \nabla_{f(x)} \mathcal{L} = 
    \begin{cases} 
      -y & \text{se } y \cdot f(x) < 1 \\
      0 & \text{se } y \cdot f(x) > 1 \\
      [-y, 0] & \text{se } y \cdot f(x) = 1
    \end{cases}
    \label{eq:hinge-loss-derivada}
\end{equacaodestaque}

Na prática, as bibliotecas tratam esses pontos de não-diferenciabilidade escolhendo um dos dois lados para garantir um valor definido. Isso pode ser visto em xx 

Além da Equação~\ref{eq:hinge-loss-derivada} que já evidencia a continuidade de Lipschitz, a Figura~\ref{fig:hinge-loss-derivada-grafico} trás essa propriedade de forma gráfica. Na derivada, a perda \textit{hinge} deixa de ser duas retas conectas e torna-se uma espécie de degrau unitário para o cálculo do gradiente.

\begin{figure}[h!]
    \centering
    \begin{tikzpicture}
        \begin{axis}[
            width=\linewidth,  
            height=7cm,
            xlabel={Saída Bruta do Modelo ($f(x)$)},
            ylabel={Gradiente ($\nabla_{f(x)} \mathcal{L}$)},
            axis lines=middle,
            grid=major,
            grid style={dashed, gray!40},
            xmin=-3.5, xmax=3.5,
            ymin=-1.5, ymax=1.5, % Ajustado para mostrar valores negativos e positivos
            ytick={-1, 0, 1},    % Mostra apenas os valores relevantes no eixo Y
            legend pos=north west,
            title style={font=\bfseries},
            label style={font=\small},
            tick label style={font=\scriptsize}
        ]
        
            % --- CASO 1: Classe Real y = +1 ---
            % Se f(x) < 1, gradiente é -1. Se f(x) > 1, gradiente é 0.
            \addplot[
                domain=-3.5:3.5, 
                samples=200, 
                color=black, 
                very thick
            ] {
                (x < 1) * -1 + (x >= 1) * 0
            };
            \addlegendentry{Para $y = 1$}

            % --- CASO 2: Classe Real y = -1 ---
            % Se f(x) > -1, o erro cresce com x, logo gradiente é +1.
            % Se f(x) < -1, o erro é 0, logo gradiente é 0.
            \addplot[
                domain=-3.5:3.5, 
                samples=200, 
                color=black, 
                dashed,
                thick
            ] {
                (x > -1) * 1 + (x <= -1) * 0
            };
            \addlegendentry{Para $y = -1$}

            % Linhas verticais de guia para as margens (opcional, ajuda a leitura)
            \draw[dotted, gray] (axis cs:1, -1.5) -- (axis cs:1, 1.5);
            \draw[dotted, gray] (axis cs:-1, -1.5) -- (axis cs:-1, 1.5);
            
            % Labels pequenos para indicar onde a margem "quebra"
            \node[below, gray, font=\tiny] at (axis cs:1, 0) {Margem (+1)};
            \node[below, gray, font=\tiny] at (axis cs:-1, 0) {Margem (-1)};

        \end{axis}
    \end{tikzpicture}
    \caption{Relação linear por partes do gradiente da função de perda \textit{hinge}.}
    \label{fig:hinge-loss-derivada-grafico}
    \fonte{O autor (2025).}
\end{figure}

\subsubsection*{Aplicações da perda hinge}%
\index{Aplicações práticas! Perda hinge}

\begin{description}[style=sameline, leftmargin=1.5em, font=\bfseries\color{black}] 
    \item[Aplicação 1 (Área):]
    \item[Aplicação 2 (Área):]
    \item[Aplicação 3 (Área):]
\end{description}

\subsection{Perda hinge quadrática}

A fórmula da perda Hinge quadrática não difere muito da perda Hinge tradicional, neste caso, essa variante eleva ao quadrado todo o resultado da perda calculada pela perda Hinge. Ela pode ser vista na Equação \ref{eq:squared-hinge-loss}.

\begin{equacaodestaque}{Perda hinge quadrática}
    \mathcal{L}_{\text{squared hinge}}(y, f(x)) = (\max(0, 1 - y \cdot f(x)))^2
    \label{eq:squared-hinge-loss}
\end{equacaodestaque}

Por utilizar um termo ao quadrado para calcular a perda, o seu gráfico também muda. Como pode ser visto na Figura \ref{fig:squared-hinge-loss}, ele agora apresenta um comportamento mais suave, além disso, os seus valores agora crescem de forma mais rápida que a perda Hinge tradicional, fazendo com que ela lide com os erros de forma mais severa que sua versão tradicional.

\begin{figure}[h!]
    \centering
    \begin{tikzpicture}
        \begin{axis}[
            xlabel={Saída Bruta do Modelo ($f(x)$)},
            ylabel={Perda ($\mathcal{L}$)},
            axis lines=middle,
            grid=major,
            grid style={dashed, gray!40},
            xmin=-3.5, xmax=3.5,
            ymin=-0.5, ymax=9.5,       
            legend pos=north west,
            width=\linewidth,
            height=7cm,
            title style={font=\bfseries},
            label style={font=\small},
            tick label style={font=\scriptsize}
        ]
            % Curva para a classe real y=+1
            \addplot[
                domain=-3:3, 
                samples=201, % Aumentei os samples para uma curva mais suave
                color=black, 
                very thick
            ] {(max(0, 1-x))^2}; % Adicionado o ^2
            \addlegendentry{Classe Real = 1 ($L=(\max(0, 1-f(x)))^2$)}

            % Curva para a classe real y=-1
            \addplot[
                domain=-3:3, 
                samples=201, 
                color=black, 
                dashed
            ] {(max(0, 1+x))^2}; % Adicionado o ^2
            \addlegendentry{Classe Real = -1 ($L=(\max(0, 1+f(x)))^2$)}
            
            % Linhas tracejadas para marcar as margens
            \draw[dashed, gray!70] (axis cs:1, 0) -- (axis cs:1, 9.5);
            \draw[dashed, gray!70] (axis cs:-1, 0) -- (axis cs:-1, 9.5);
            \node[above, gray!80, font=\tiny, rotate=90] at (axis cs:1.1, 4) {Margem};
            \node[above, gray!80, font=\tiny, rotate=90] at (axis cs:-0.9, 4) {Margem};
            
        \end{axis}
    \end{tikzpicture}
    \caption{Visualização da função perda hinge quadrática no plano.}
    \label{fig:squared-hinge-loss}
    \fonte{O autor (2025).}
\end{figure}

\subsubsection*{Características da perda hinge quadrática}

\begin{description}[style=sameline, leftmargin=1.5em, font=\bfseries\color{black}] 
    \item[Continuidade, suavidade e diferenciabilidade:] A perda Hinge quadrática é uma função de classe $C^1$, \textbf{suave} (ver Apêndice~\ref{ap:deducoes-perda-hinge-quadratica}).
    \item[Convexidade:] A perda Hinge quadrática é uma função \textbf{convexa} (ver Apêndice~\ref{ap:deducoes-perda-hinge-quadratica}).
    \item[Robustez:] A perda Hinge quadrática não é Lipschitz-contínua, sendo \textbf{sensível a erros confiantes} (ver Apêndice~\ref{ap:deducoes-perda-hinge-quadratica})..
\end{description}

\subsubsection*{Gradiente da perda hinge quadrática}

Para calcular a sua derivada deve-se aplicar a regra da cadeia, separando então termo ao quadrado da função perda hinge tradicional. De forma que ao fazer isso é possível encontrar a Equação \ref{eq:squared-hinge-loss-derivada}.

\begin{equacaodestaque}{Gradiente da perda hinge quadrática}
    \nabla_{f(x)} \mathcal{L}_{\text{squared hinge}} = 
    \begin{cases} 
        -2y(1 - y \cdot f(x)) & \text{se } y \cdot f(x) < 1 \\
        0 & \text{se } y \cdot f(x) \ge 1
    \end{cases}
    \label{eq:squared-hinge-loss-derivada}
\end{equacaodestaque}

De forma semelhante ao que foi feito com as outras funções até agora, é possível plotar o gráfico da derivada da perda Hinge. Ele está representado na Figura \ref{fig:squared-hinge-loss-derivada}. Note, que diferente do gráfico da perda Hinge em que a correção a ser feita é dada de forma constante, a derivada da perda Hinge quadrática é calculada de forma linear.

\begin{figure}[h!]
    \centering
    \begin{tikzpicture}
        \begin{axis}[
            xlabel={Saída Bruta do Modelo ($f(x)$)},
            ylabel={Gradiente da Perda ($\nabla_{f(x)} \mathcal{L}$)},
            axis lines=middle,
            grid=major,
            grid style={dashed, gray!40},
            xmin=-3.5, xmax=3.5,
            ymin=-8.5, ymax=8.5,         % Ajustar o y para a escala linear da derivada
            legend pos=north west,
            width=\linewidth,
            height=7cm,
            title style={font=\bfseries},
            label style={font=\small},
            tick label style={font=\scriptsize}
        ]
            % Derivada para a classe real y=+1
            % Fórmula: (x < 1) ? (2*(x-1)) : 0
            \addplot[
                domain=-3:3, 
                samples=10, 
                color=black, 
                very thick
            ] {(x < 1) ? (2*(x-1)) : 0};
            \addlegendentry{Classe Real = 1}

            % Derivada para a classe real y=-1
            % Fórmula: (x > -1) ? (2*(x+1)) : 0
            \addplot[
                domain=-3:3, 
                samples=10, 
                color=black, 
                dashed
            ] {(x > -1) ? (2*(x+1)) : 0};
            \addlegendentry{Classe Real = -1}
            
            % Linhas tracejadas para marcar as margens
            \draw[dashed, gray!70] (axis cs:1, -8.5) -- (axis cs:1, 8.5);
            \draw[dashed, gray!70] (axis cs:-1, -8.5) -- (axis cs:-1, 8.5);
            \node[above, gray!80, font=\tiny, rotate=90] at (axis cs:1.1, 2) {Margem};
            \node[above, gray!80, font=\tiny, rotate=90] at (axis cs:-0.9, 2) {Margem};
            
        \end{axis}
    \end{tikzpicture}
    \caption{Representação gráfica da derivada da função de perda Squared Hinge Loss. O gradiente é proporcional à distância da margem.}
    \label{fig:squared-hinge-loss-derivada}
    \fonte{O autor (2025).}
\end{figure}

\subsubsection*{Aplicações da perda hinge quadrática}%
\index{Aplicações práticas! Perda hinge quadrática}

\begin{description}[style=sameline, leftmargin=1.5em, font=\bfseries\color{black}] 
    \item[Aplicação 1 (Área):]
    \item[Aplicação 2 (Área):]
    \item[Aplicação 3 (Área):]
\end{description}

\section{Funções de perda para classificação multi-classe}

\subsection{Exemplo ilustrativo:}

\subsection{Entropia-cruzada categórica (CCE)}%
\index{Funções de Perda!Entropia-cruzada categórica (CCE)}

Entendido os conceitos de entropia cruzada binária, o próximo passo é conhecer a entropia cruzada categórica, uma das funções de perda que se é utilizada para resolver problemas de classificação multi-classe. Para isso, a CCE estende o conceito da BCE para lidar com $y_i \in {0,1}^C$ em uma representação \textit{one-hot} entre $C$ diferentes classes \parencite{LossesArticle}. Dessa forma, diferente da entropia cruzada binária em que haviam dois cálculos de entropia-cruzada que eram então subtraídos para chegar na perda para um conjunto $(y_j, \hat{y}_j)$, a CCE faz uso de múltiplas entropias-cruzadas, uma para cada classe, as quais são somadas chegando então na perda.

Considerando um conjunto $\hat{y}_j = [\hat{y}_{j,1}, \dots, \hat{y}_{j,C}]$ que se refere a distribuição de probabilidade para uma amostra $j$, a entropia cruzada categórica por amostra é dada então pela Equação \ref{eq:categorical-cross-entropy-per-sample}.

\begin{equacaodestaque}{Entropia-cruzada categórica}
    \mathcal{L}_{CCE} (y, \hat{y}) = - \sum_{c=1}^{C} y_c \log(\hat{y}_c)
    \label{eq:categorical-cross-entropy-per-sample}
\end{equacaodestaque}

Em que

\begin{description}[style=sameline, leftmargin=1.5em, font=\bfseries\color{black}] 
    \item[$y_j$] representa o valor real da j-ésima amostra;
    \item[$\hat{y}_j$] representa o valor predito pelo modelo;
    \item[$N$] representa o número de amostras.
\end{description}

Considerando essas duas fórmulas, é possível também analisar o gráfico da entropia cruzada categórica, o qual está representado na Figura \ref{fig:categorical-cross-entropy}. Neste caso, está sendo representado o gráfico de apenas uma das funções $-\log(\hat{y}_k)$, mas o gráfico real da CCE é mais complexo, pois seria o resultado da soma de um conjunto de funções $-\log(\hat{y}_k)$, impendido a plotagem do gráfico real.

\begin{figure}

    \begin{tikzpicture}
        \begin{axis}[
            xlabel={Probabilidade Prevista para a Classe Correta ($\hat{y}_k$)},
            ylabel={Perda Calculada},
            axis lines=left,              % Eixos no canto inferior esquerdo
            grid=major,                   % Adiciona uma grade principal
            grid style={dashed, gray!40},   % Estilo da grade
            xmin=0, xmax=1.05,            % Limites do eixo x
            ymin=0, ymax=5,               % Limites do eixo y
            legend pos=north east,        % Posição da legenda
            width=\linewidth,
            height=7cm,           
            title style={font=\bfseries},
            label style={font=\small},
            tick label style={font=\scriptsize}
        ]
            % Plota a função -log(y_k_hat)
            \addplot[
                domain=0.01:1, % Domínio para evitar log(0)
                samples=100,
                color=black,
                very thick
            ] {-ln(x)};
            
            \addlegendentry{$L = -\log(\hat{y}_k)$}
            
        \end{axis}
    \end{tikzpicture}

    \caption{Visualização da função de perda entropia-cruzada categórica no plano.}
    \label{fig:categorical-cross-entropy}
    \fonte{O autor (2025).}

\end{figure}

\subsubsection*{Características da entropia-cruzada categórica}

\begin{description}[style=sameline, leftmargin=1.5em, font=\bfseries\color{black}] 
    \item[Continuidade, suavidade e diferenciabilidade:] A CCE é de classe $C^{\infty}$, \textbf{infinitamente diferenciável} no domínio aberto $\hat{y}_i \in (0, 1)$ (ver Apêndice~\ref{ap:deducoes-perdas-derivadas-da-kl-divergence}).
    \item[Convexidade:] A CCE é \textbf{convexa} em relação predições $\hat{y}$ (ver Apêndice~\ref{ap:deducoes-perdas-derivadas-da-kl-divergence}).
    \item[Robustez:] A CCE não é Lipschitz-contínua, sendo \textbf{sensível a erros confiantes} (ver Apêndice~\ref{ap:deducoes-perdas-derivadas-da-kl-divergence}).
    \item[Saída softmax:] Semelhante a função de entropia cruzada binária, a qual aceitava valores entre 0 e 1 e por isso precisava de ser utilizada junto com funções como a sigmoide logística, a entropia cruzada categórica também tem seus requisitos. Como um modelo de rede neural geralmente retorna como saída \textit{logits} $z_i \in \mathbb{R}^C$, ao utilizar uma função \textit{softmax} é possível garantir que a $\sum_{j} \hat{p}_{i,j} = 1$, ou seja, que xxxx \parencite{LossesArticle}.
    \item[Penalização alta para erros confiantes:] Como \textcite{LossesArticle} explicam, a entropia cruzada categórica segue a mesma ideia da entropia cruzada binária, de forma que essa função também tem como propriedade punir os erros confiantes de forma mais elevada.
    \item[Requisitos \textit{one-hot}] Como dito anteriormente a CCE aceita um vetor em formato \textit{one-hot}, isso significa que os valores tanto para $y_j$ quanto para $\hat{y}_j$ devem estar em formato de probabilidade, estando em valores entre zero e um \footnote{Para trabalhar com valores em que os rótulos estão organizados com valores inteiros é possível utilizar uma variante da \textit{categorical cross-entropy}: a \textit{sparse cross-entropy}. Essa função está melhor explicada na Seção \ref{sec:sparse-cross-entropy}}.
\end{description}

\subsubsection*{Gradiente da entropia-cruzada categórica}

É possível também discutir a derivada da entropia cruzada categórica, pois ela trás uma discussão interessante. Como dito anteriormente, na maioria das vezes a saída da última camada de um modelo de classificação multi-classe é adicionada a função de ativação \textit{softmax}, permitindo que os valores da saída se enquadrem no intervalo ${0, 1}$, como consequência, é possível simplificar o cálculo do gradiente para essa função. 

Antes é lembrar da fórmula da \textit{softmax}, a qual é dada pela Equação \ref{eq:perdas-softmax}.

\begin{equation}
    \mathcal{A}_{\text{softmax}}(z_i) = \frac{e^{z_i}}{\sum_{j=1}^{K} e^{z_j}}
    \label{eq:perdas-softmax}
\end{equation}

\textbf{Passo 1: Aplicar a regra da cadeia}

Considerando a equação da entropia cruzada categórica e a da função de ativação \textit{softmax}, o primeiro passo é calcular a regra da cadeia, pois a perda $\Loss_{CCE}$ depende das previsões $\hat{y}_j$ e as previsões por sua vez dependem dos \textit{logits}. Além disso, vale notar que um único \textit{logit} $z_i$ pode afetar todas as saídas da \textit{softmax}, por conta do somatório no denominador. 

Assim, é possível escrever a regra da cadeia como sendo:

\[
    \frac{\partial \Loss}{\partial z_i} = \sum_{j=1}^{C} \frac{\partial \Loss}{\partial \hat{y}_j} \cdot \frac{\partial \hat{y}_j}{\partial z_i}
\]

\textbf{Passo 2: Calcular as derivadas parciais}

\textbf{Passo 2A: Derivada da Perda em Relação à Previsão ($\frac{\partial \Loss}{\partial \hat{y}_j}$) }

Para fazer esse cálculo é possível derivar a perda em relação a uma única saída $\hat{y}_j$ encontrando então:

\[
    \frac{\partial \Loss}{\partial \hat{y}_j} = \frac{\partial}{\partial \hat{y}_j} \left( - \sum_{k=1}^{C} y_k \log(\hat{y}_k) \right) = -y_j \cdot \frac{1}{\hat{y}_j} = -\frac{y_j}{\hat{y}_j}
\]

\textbf{Passo 2B: Derivada da Softmax}

Para fazer esse cálculo é preciso considerar dois diferentes casos, o primeiro é quando $i = j$, ou seja derivada da saída de uma classe em relação à sua própria entrada de \textit{logit}, assim, utilizando a regra do quociente é possível chegar na expressão:

\[
    \frac{\partial \hat{y}_i}{\partial z_i} = \frac{e^{z_i}(\sum_k e^{z_k}) - e^{z_i} \cdot e^{z_i}}{(\sum_k e^{z_k})^2}
\]

Simplificando a expressão uma primeira vez:

\[
    \frac{e^{z_i}}{\sum_k e^{z_k}} - \left( \frac{e^{z_i}}{\sum_k e^{z_k}} \right)^2
\]

É possível chegar então nos termos:

\[
    \hat{y}_i - \hat{y}_i^2 = \hat{y}_i (1 - \hat{y}_i)
\]

Já para o caso em que que é calculada a derivada da saída de uma classe em relação à entrada de \textit{logit} de outra classe, ou seja $i \neq j$ a equação é dada por:

\[
    \frac{\partial \hat{y}_j}{\partial z_i} = \frac{0 \cdot (\sum_k e^{z_k}) - e^{z_j} \cdot e^{z_i}}{(\sum_k e^{z_k})}
\]

Simplificando ela é possível chegar em:

\[
    - \left( \frac{e^{z_j}}{\sum_k e^{z_k}} \right) \left( \frac{e^{z_i}}{\sum_k e^{e_z}} \right) = -\hat{y}_j \hat{y}_i
\].

\textbf{Passo 3: Juntando os termos e simplificando as expressões}

Agora, o próximo passo é substituir as derivadas parciais calculadas na regra da cadeia, para isso será separado o somatório nos casos em que $i = j$ e nos casos que $i \neq = j$.

\[
    \frac{\partial \Loss}{\partial z_i} = \left( \frac{\partial \Loss}{\partial \hat{y}_i} \cdot \frac{\partial \hat{y}_i}{\partial z_i} \right) + \sum_{j \neq i} \left( \frac{\partial \Loss}{\partial \hat{y}_j} \cdot \frac{\partial \hat{y}_j}{\partial z_i} \right)
\]

Substituindo os resultados encontrados:

\[
    \left( - \frac{y_i}{\hat{y}_i} \cdot \hat{y}_i (1 - \hat{y}_i) \right) + \sum_{j \neq i} \left( - \frac{y_j}{\hat{y}_j} \cdot (-\hat{y}_j\hat{y}_i) \right)
\]

Simplificando os termos:

\[
    - y_i (1 - \hat{y}_i) + \sum_{j \neq i} (y_j \hat{y}_i)
\]

Aplicando a distributiva:

\[
    - y_i + y_i \hat{y}_i + \hat{y}_i \sum_{j \neq i} y_j
 \]

 Perceba um detalhe interessante, $y$ representa um vetor \textit{one-hot encoded}, o qual contém todas as probabilidades para cada uma das classes que o modelo está analisando, isso significa que a soma de todos esses elementos vai ser 1. Com isso é possível escrever que $\sum_{j = 1}^C y_j = 1$. Contudo, na expressão que está sendo desenvolvida nos temos todos os elementos exceto pelo o i-ésimo, portanto: $\sum_{j\neq i} y_j = 1 - y_i$.

 Substituindo essa informação na equação é possível chegar em:

 \[
    y_i + y_i \hat{y}_i + \hat{y}_i (1 - y_i)
 \]

 Aplicando mais uma vez a distributiva para expandir o termo $\hat{y}_i (1 - y_i)$:

 \[
    -y_i + y_i \hat{y}_i + \hat{y}_i - y_i \hat{y}_i
 \]

 Perceba que os termos $y_i \hat{y}_i $ e $y_i \hat{y}_i$ se cancelam, então é possível chegar na expressão \ref{eq:category-cross-entropy-derivada}. A qual representa a derivada da \textit{categorical cross-entropy} para um cenário em que o último componente do modelo é a função de ativação softmax. Note que ao invés de ser uma derivada complexa como nos outros casos vistos até agora, a derivada da CCE passa a ser apenas o cálculo da diferença entre os valores preditos e os valores reais.

\begin{equacaodestaque}{Gradiente entropia-cruzada categórica para a Softmax}
    \nabla_{z_i} \mathcal{L} (y, \hat{y}) = \hat{y}_i - y_i
    \label{eq:category-cross-entropy-derivada}
\end{equacaodestaque}

Note também que a que essa simplificação dos cálculos para a derivada na entropia cruzada utilizando a \textit{softmax} também reflete no gráfico, que é composto apenas de , como é possível ver na Figura \ref{fig:categorical-cross-entropy-derivada-com-softmax}.

\begin{figure}[h!]
    \centering
    \begin{tikzpicture}
        \begin{axis}[
            xlabel={Probabilidade Prevista para a Classe $i$ ($\hat{y}_i$)},
            ylabel={Gradiente da Perda ($\frac{\partial L}{\partial z_i}$)},
            axis lines=middle,             % Eixos centrados em (0,0)
            grid=major,                  % Adiciona uma grade principal
            grid style={dashed, gray!40},  % Estilo da grade
            xmin=-0.1, xmax=1.1,           % Limites do eixo x
            ymin=-1.1, ymax=1.1,           % Limites do eixo y
            legend pos=north west,         % Posição da legenda
            width=\linewidth,
            height=7cm,                   % Altura do gráfico
            title style={font=\bfseries},
            label style={font=\small},
            tick label style={font=\scriptsize}
        ]
            % Curva para a classe correta (y_i = 1)
            % A derivada é y_hat - 1
            \addplot[
                domain=0:1,
                samples=10,
                color=black,
                very thick
            ] {x - 1};
            \addlegendentry{Classe Correta ($y_i=1 \implies \hat{y}_i - 1$)}

            % Curva para uma classe incorreta (y_i = 0)
            % A derivada é y_hat - 0
            \addplot[
                domain=0:1,
                samples=10,
                color=black,
                dashed
            ] {x};
            \addlegendentry{Classe Incorreta ($y_i=0 \implies \hat{y}_i - 0$)}
            
        \end{axis}
    \end{tikzpicture}
    \caption{Representação gráfica da derivada da \textit{categorical cross entropy} com Softmax.}
    \label{fig:categorical-cross-entropy-derivada-com-softmax}
    \fonte{O autor (2025).}
\end{figure}

Caso a \textit{categorical cross entropy} não seja utilizada em conjunto com a função de ativação \textit{softmax} na saída do modelo, a sua derivada passa a ser um pouco mais complexa, sendo representada pela Equação \ref{eq:categorical-cross-entropy-derivada}.

\begin{equacaodestaque}{Gradiente da entropia-cruzada categórica em relação à predição}
    \nabla_{\hat{y}_i} \mathcal{L}_{\text{CCE}} = -\frac{y_i}{\hat{y}_i}
    \label{eq:categorical-cross-entropy-derivada}
\end{equacaodestaque}

\subsubsection*{Aplicações da entropia-cruzada categórica}%
\index{Aplicações práticas! Entropia Cruzada Categórica}

\begin{description}[style=sameline, leftmargin=1.5em, font=\bfseries\color{black}] 
    \item[Aplicação 1 (Área):]
    \item[Aplicação 2 (Área):]
    \item[Aplicação 3 (Área):]
\end{description}

Conhecida a CCE, uma das principais funções de perda para ser utilizada para problemas de classificação multi-classe, é possível se perguntar: O que acontece se os dados não estiverem codificados em formato \textit{one-hot}? Para contornar esse problema, uma solução é utilizar a \textit{sparse categorical cross-entropy}, a qual será explicada em sequência.

\subsection{Entropia-cruzada categórica esparsa (Sparse CCE)}%
\index{Funções de Perda!Entropia-cruzada categórica esparsa (Sparse CCE)}%
\label{sec:sparse-cross-entropy}

A entropia cruzada categórica esparsa é utilizada para os cenários em que os rótulos das classes são dados em inteiros $y_j \in {1, 2, \cdots C}$ \parencite{LossesArticle}. A \textit{sparse CCE} para um conjunto de individual de predição é dada pela Equação \ref{eq:sparse-categorical-cross-entropy-per-sample}

\begin{equacaodestaque}{Entropia-cruzada categórica esparsa para uma amostra $j$}
    \mathcal{L}_{\text{sparse CCE}}(y, \hat{y}) = - \sum_{c=1}^{C} y_c \log(\hat{y}_c)
    \label{eq:sparse-categorical-cross-entropy-per-sample}
\end{equacaodestaque}

Perceba que as fórmulas da \textit{sparse categorical cross entropy} são iguais as da sua versão para rótulos codificados para formato \textit{one-hot}, por isso os seus gráficos também serão iguais, assim, o próximo passo é discutir algumas das características dessa função de perda.

\subsubsection*{Características da entropia-cruzada categórica esparsa}

\begin{description}[style=sameline, leftmargin=1.5em, font=\bfseries\color{black}] 
    \item[Continuidade, suavidade e diferenciabilidade:] A CCE esparsa é de classe $C^{\infty}$, \textbf{infinitamente diferenciável} no domínio aberto $\hat{y}_i \in (0, 1)$ (ver Apêndice~\ref{ap:deducoes-perdas-derivadas-da-kl-divergence}).
    \item[Convexidade:] A CCE esparsa é \textbf{convexa} em relação predições $\hat{y}$ (ver Apêndice~\ref{ap:deducoes-perdas-derivadas-da-kl-divergence}).
    \item[Robustez:] A CCE esparsa não é Lipschitz-contínua, sendo \textbf{sensível a erros confiantes} (ver Apêndice~\ref{ap:deducoes-perdas-derivadas-da-kl-divergence}).
    \item[Eficiência:] \textcite{LossesArticle} explicam que em problemas de classificação com um grande número de classes, a codificação \textit{one-hot} pode ser memória-intensiva, ao utilizar a \textit{sparse CCE}, os seus indexes que já apontam diretamente para a probabilidade conseguem escapar de trabalhar com dados em formato \textit{one-hot}.
    \item[Similaridades com a CCE:]  A \textit{sparse CCE} possui grandes similaridades com a CCE original, isso significa que vantagens como a diferenciabilidade e a penalização de erros muito confiantes que são características da \textit{categorical cross entropy}, também estão presentes na sua versão esparsa.
\end{description}

Além dessa variante, a \textit{categorical cross-entropy} possui uma versão que faz uso de pesos para as classes, buscando trabalhar com casos em que existe uma presença maior de algumas classes do que de outras. Essa é a \textit{weighted categorical cross-entropy}, que será vista em seguida.

\subsubsection*{Aplicações da Entropia-Cruzada Categórica Esparsa}%
\index{Aplicações práticas! Entropia Cruzada Categórica Esparsa}

\begin{description}[style=sameline, leftmargin=1.5em, font=\bfseries\color{black}] 
    \item[Aplicação 1 (Área):]
    \item[Aplicação 2 (Área):]
    \item[Aplicação 3 (Área):]
\end{description}

\subsection{Entropia-cruzada categórica ponderada (WCCE)}%
\index{Funções de Perda!Entropia-cruzada categórica ponderada (WCCE)}

A fórmula da \textit{weighted categorical cross-entropy} lembra bastante a fórmula da \textit{weighted binary cross-entropy}, pois sua única diferença com a variante original é a adição de um peso multiplicando o erro para aquela classe. Essa função de perda para uma amostra $j$ é dada pela Equação \ref{eq:weighted-categorical-cross-entropy-per-sample}.

\begin{equacaodestaque}{Entropia-cruzada categórica Ponderada}
    \mathcal{L}_{\text{WCCE}}(y, \hat{y}) = - \sum_{c=1}^{C} \alpha y_c \log(\hat{y}_c)
    \label{eq:weighted-categorical-cross-entropy-per-sample}
\end{equacaodestaque}

Em que

\begin{description}[style=sameline, leftmargin=1.5em, font=\bfseries\color{black}] 
    \item[$y_j$] representa o valor real da j-ésima amostra;
    \item[$\hat{y}_j$] representa o valor predito pelo modelo;
    \item[$N$] representa o número de amostras.
\end{description}

\subsubsection*{Características da entropia-cruzada categórica ponderada}

\begin{description}[style=sameline, leftmargin=1.5em, font=\bfseries\color{black}] 
    \item[Continuidade, suavidade e diferenciabilidade:] A WCCE é de classe $C^{\infty}$, \textbf{infinitamente diferenciável} no domínio aberto $\hat{y}_i \in (0, 1)$ (ver Apêndice~\ref{ap:deducoes-perdas-derivadas-da-kl-divergence}).
    \item[Convexidade:] A WCCE é \textbf{convexa} em relação predições $\hat{y}$ (ver Apêndice~\ref{ap:deducoes-perdas-derivadas-da-kl-divergence}).
    \item[Robustez:] A WCCE não é Lipschitz-contínua, sendo \textbf{sensível a erros confiantes} (ver Apêndice~\ref{ap:deducoes-perdas-derivadas-da-kl-divergence}).
\end{description}

Assim, de forma semelhante à \textit{weighted binary cross-entropy} é possível utilizar a sua variante multi-classe para ser trabalhada em cenários em que uma classe, ou um grupo de classes aparece de forma mais frequente que outro.

\subsubsection*{Aplicações da Entropia-Cruzada Categórica Ponderada}%
\index{Aplicações práticas! Entropia Cruzada Categórica Ponderada}

\begin{description}[style=sameline, leftmargin=1.5em, font=\bfseries\color{black}] 
    \item[Aplicação 1 (Área):]
    \item[Aplicação 2 (Área):]
    \item[Aplicação 3 (Área):]
\end{description}

\section{Perda multi-rótulo (multilabel loss)}%
\index{Funções de Perda!Multilabel Loss}

\begin{equacaodestaque}{Perda multi-rótulo}
    \mathcal{L}_{\text{multilabel}} = - \sum_{j=1}^{q} [y_j \log(\hat{y}_j) + (1 - y_j) \log(1 - \hat{y}_j)]
    \label{eq:multilabel-loss}
\end{equacaodestaque}

\begin{figure}[h!]
    \centering
    \begin{tikzpicture}
        \begin{axis}[
            title={Contribuição de um Rótulo para a Perda Multirrótulo},
            xlabel={Probabilidade Prevista para o Rótulo $j$ ($\hat{y}_j$)},
            ylabel={Contribuição Individual para a Perda},
            axis lines=left,
            grid=major,
            grid style={dashed, gray!40},
            xmin=0, xmax=1,
            ymin=0, ymax=5,
            legend pos=north west,
            width=\linewidth,
            height=7cm,
            title style={font=\bfseries},
            label style={font=\small},
            tick label style={font=\scriptsize}
        ]
            % Curva para quando o rótulo está presente (y_j = 1)
            \addplot[
                domain=0.01:0.999, samples=100, color=black, very thick
            ] {-ln(x)};
            \addlegendentry{Rótulo Presente ($y_j=1$)}

            % Curva para quando o rótulo está ausente (y_j = 0)
            \addplot[
                domain=0.001:0.99, samples=100, color=black, dashed
            ] {-ln(1-x)};
            \addlegendentry{Rótulo Ausente ($y_j=0$)}
            
        \end{axis}
    \end{tikzpicture}
    \caption{A perda multi-rótulo é a soma das contribuições de cada rótulo individual, que se comportam como a BCE.}
    \label{fig:multilabel-loss}
    \fonte{O autor (2025).}
\end{figure}

\subsubsection*{Características da perda multi-rótulo}

\begin{description}[style=sameline, leftmargin=1.5em, font=\bfseries\color{black}] 
    \item[Continuidade, suavidade e diferenciabilidade:] A perda multi-rótulo uma função de classe $C^\infty$, \textbf{infinitamente diferenciável}, no hipercubo aberto $(0, 1)^q$ (ver Apêndice~\ref{ap:deducoes-multilabel-loss}).
    \item[Convexidade:] A perda multi-rótulo é uma função \textbf{convexa} em relação ao vetor de predições $\mathbf{\hat{y}}$ (ver Apêndice~\ref{ap:deducoes-multilabel-loss}).
    \item[Robustez:] A perda multi-rótulo não é Lipschitz-contínua (ver Apêndice~\ref{ap:deducoes-multilabel-loss}).
\end{description}

\subsubsection*{Gradiente da perda multi-rótulo}

\begin{equacaodestaque}{Derivada da Perda Multirrótulo}
    \nabla_{z_i} \mathcal{L}_{\text{Multilabel}}= \hat{y}_i - y_i
    \label{eq:multilabel-loss-derivada}
\end{equacaodestaque}

\begin{figure}[h!]
    \centering
    \begin{tikzpicture}
        \begin{axis}[
            title={Derivada da Perda Multirrótulo (para um Rótulo)},
            xlabel={Probabilidade Prevista para o Rótulo $i$ ($\hat{y}_i$)},
            ylabel={Gradiente da Perda ($\frac{\partial L}{\partial z_i}$)},
            axis lines=middle,
            grid=major,
            grid style={dashed, gray!40},
            xmin=-0.1, xmax=1.1,
            ymin=-1.1, ymax=1.1,
            legend pos=north west,
            width=\linewidth,
            height=7cm,
            title style={font=\bfseries},
            label style={font=\small},
            tick label style={font=\scriptsize}
        ]
            % Curva para rótulo presente (y_i = 1)
            \addplot[domain=0:1, samples=10, color=black, very thick] {x - 1};
            \addlegendentry{Rótulo Presente ($y_i=1$)}

            % Curva para rótulo ausente (y_i = 0)
            \addplot[domain=0:1, samples=10, color=black, dashed] {x};
            \addlegendentry{Rótulo Ausente ($y_i=0$)}
            
        \end{axis}
    \end{tikzpicture}
    \caption{O gradiente para cada rótulo é simplesmente a diferença entre a probabilidade prevista e o valor real (0 ou 1).}
    \label{fig:multilabel-loss-derivada}
    \fonte{O autor (2025).}
\end{figure}

\subsubsection*{Aplicações da perda multi-rótulo}%
\index{Aplicações práticas! Multi-label}

\begin{description}[style=sameline, leftmargin=1.5em, font=\bfseries\color{black}] 
    \item[Aplicação 1 (Área):]
    \item[Aplicação 2 (Área):]
    \item[Aplicação 3 (Área):]
\end{description}

\section{Comparativo: funções de perda para classificação}

\begin{table}[htbp]
    \centering
    \begin{threeparttable}
        \caption{Comparativo das funções de perda para problemas de classificação}
        \label{tab:comparativo-funcoes-de-perda-para-classificacao}
        % p{3.2cm} define uma largura fixa para a primeira coluna.
        % As 3 colunas 'X' restantes dividem o espaço que sobra de forma flexível.
        % >{\raggedright\arraybackslash} alinha o texto à esquerda para melhor leitura.
        \begin{tabularx}{\textwidth}{p{3.2cm} *{1}{>{\raggedright\arraybackslash}X}}
            \toprule
            \textbf{Função} & \textbf{Principais Características} \\
            \midrule
            Entropia Cruzada Binária (BCE) & - \\
            \addlinespace
            entropia cruzada binária ponderada (\textit{WBCE}) & - \\
            \addlinespace
            Perda Hinge (\textit{Hinge Loss}) & - \\
            \addlinespace
            Entropia Cruzada Categórica (CCE) & -  \\
            \addlinespace
            Entropia Cruzada categórica Esparsa (\textit{Sparse CCE}) & - \\
            \addlinespace
            Entropia Cruzada Categórica Ponderada (\textit{WCCE}) & - \\
            \addlinespace
            Perda Multirrótulo (\textit{Multilabel Loss}) & - \\
            \addlinespace
        \end{tabularx}
        
        \begin{tablenotes}[para]
            \small
            \item[] Fonte: O autor (2025).
        \end{tablenotes}

    \end{threeparttable}
\end{table}

\section{Fluxograma: Escolhendo a Função de Perda Ideal}
