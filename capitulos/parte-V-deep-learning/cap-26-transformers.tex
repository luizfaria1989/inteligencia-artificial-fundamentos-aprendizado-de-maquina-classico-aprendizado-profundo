% ===================================================================
% Arquivo: capitulos/parte_V_deep_learning/cap_26_transformers.tex
% ===================================================================

\chapter{Transformers}
\label{cap:transformers}

\section{As Limitações das RNNs: O Gargalo Sequencial}

\section{A Ideia Central: Self-Attention (Query, Key, Value)}

\section{Escalando a Atenção: Multi-Head Attention}

\section{A Arquitetura Completa: O Bloco Transformer}

\section{Entendendo a Posição: Codificação Posicional}

\section{As Três Grandes Arquiteturas}

\subsection{Encoder-Only (Ex: BERT): Para tarefas de entendimento}

\subsection{Decoder-Only (Ex: GPT): Para tarefas de geração}

\subsection{Encoder-Decoder (Ex: T5): Para tarefas de tradução/sumarização}

\section{Além do Texto: Vision Transformers (ViT)}