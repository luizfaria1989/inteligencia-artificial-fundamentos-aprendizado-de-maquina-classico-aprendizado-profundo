@book{DeepLearningBook,
  author    = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  title     = {Deep Learning},
  publisher = {MIT Press},
  year      = {2016},
}

@book{MaosAObra,
  author = {Aurélien Géron},
  title = {Mãos à Obra: Aprendizado de Máquina com Scikit-Learn e TensorFlow},
  publisher = {Alta Books},
  year = {2019},
}

-- Método do gradiente original por Cauchy
@article{CauchyMetodoDoGradiente,
  title   = {M{\'e}thode g{\'e}n{\'e}rale pour la r{\'e}solution des syst{\`e}mes d'{\'e}quations simultan{\'e}es},
  author  = {Cauchy, Augustin-Louis},
  journal = {Comptes Rendus Hebdomadaires des S{\'e}ances de l'Acad{\'e}mie des Sciences},
  volume  = {25},
  pages   = {536--538},
  year    = {1847}
}

-- Artigo que introduz a retropropagação
@article{BackpropagationArticle,
  author  = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
  title   = {Learning Representations by Back-Propagating Errors},
  journal = {Nature},
  year    = {1986},
  volume  = {323},
  pages   = {533--536},
}

-- Artigo que introduz os primeiros teoremas da aproximação universal
@article{Cybenko1989,
  author  = {Cybenko, George},
  title   = {Approximation by Superpositions of a Sigmoidal Function},
  journal = {Mathematics of Control, Signals, and Systems},
  year    = {1989},
  volume  = {2},
  number  = {4},
  pages   = {303--314},
}

-- Artigo que introduz os teoremas da aproximação universal para funções como a relu
@article{Leshno1993,
      title = {Multilayer feedforward networks with a nonpolynomial activation function can approximate any function},
      journal = {Neural Networks},
      volume = {6},
      number = {6},
      pages = {861-867},
      year = {1993},
      issn = {0893-6080},
      doi = {https://doi.org/10.1016/S0893-6080(05)80131-5},
      url = {https://www.sciencedirect.com/science/article/pii/S0893608005801315},
      author = {Moshe Leshno and Vladimir Ya. Lin and Allan Pinkus and Shimon Schocken},
      keywords = {Multilayer feedforward networks, Activation functions, Role of threshold, Universal approximation capabilities, (μ) approximation},
      abstract = {Several researchers characterized the activation function under which multilayer feedforward networks can act as universal approximators. We show that most of all the characterizations that were reported thus far in the literature are special cases of the following general result: A standard multilayer feedforward network with a locally bounded piecewise continuous activation function can approximate any continuous function to any degree of accuracy if and only if the network's activation function is not a polynomial. We also emphasize the important role of the threshold, asserting that without it the last theorem does not hold.}
}
-- Artigo que introduz o método do gradiente com momentum
@article{polyak1964,
  title={Some methods of speeding up the convergence of iteration methods},
  author={Polyak, Boris T.},
  journal={USSR Computational Mathematics and Mathematical Physics},
  volume={4},
  number={5},
  pages={1--17},
  year={1964},
  publisher={Elsevier}
}

-- Artigo que introduz o método de otimização Adam
@misc{AdamMethod,
      title={Adam: A Method for Stochastic Optimization}, 
      author={Diederik P. Kingma and Jimmy Ba},
      year={2017},
      eprint={1412.6980},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1412.6980}, 
}

-- Artigo que introduz o método de otimização AdaGrad
@article{AdaGradMethod,
  author  = {John Duchi and Elad Hazan and Yoram Singer},
  title   = {Adaptive Subgradient Methods for Online Learning and Stochastic Optimization},
  journal = {Journal of Machine Learning Research},
  year    = {2011},
  volume  = {12},
  number  = {61},
  pages   = {2121--2159},
  url     = {http://jmlr.org/papers/v12/duchi11a.html}
}

-- Artigo que introduz o método de otimização Nadam
@inproceedings{NadamMethod,
  title={Incorporating Nesterov Momentum into Adam},
  author={Dozat, Timothy},
  booktitle={ICLR 2016 Workshop Track},
  year={2016},
  url={https://openreview.net/forum?id=OM0DEvNMIxAaI-fG_O1-I}
}

-- Artigo que introduz o método de otimização AdamW
@misc{AdamWMethod,
      title={Decoupled Weight Decay Regularization}, 
      author={Ilya Loshchilov and Frank Hutter},
      year={2019},
      eprint={1711.05101},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1711.05101}, 
}

-- Artigo que introduz o método de otimização RAdam
@misc{RAdamMethod,
      title={On the Variance of the Adaptive Learning Rate and Beyond}, 
      author={Liyuan Liu and Haoming Jiang and Pengcheng He and Weizhu Chen and Xiaodong Liu and Jianfeng Gao and Jiawei Han},
      year={2021},
      eprint={1908.03265},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1908.03265}, 
}

-- Artigo que introduz o método do gradiente estocástico
@article{StochasticGradientDescentMethod,
  author    = {Herbert Robbins and Sutton Monro},
  title     = {A Stochastic Approximation Method},
  journal   = {The Annals of Mathematical Statistics},
  year      = {1951},
  volume    = {22},
  number    = {3},
  pages     = {400--407},
  doi       = {10.1214/aoms/1177729586}
}

-- Adeline 
-- SGD aparece pela primeira vez em aprendizado de máquina
@inproceedings{Adeline,
  author    = {Bernard Widrow and Marcian E. Hoff},
  title     = {Adaptive Switching Circuits},
  booktitle = {1960 IRE WESCON Convention Record},
  year      = {1960},
  part      = {4},
  pages     = {96--104},
  publisher = {IRE}
}

-- Artigo que introduz o método de otimização do gradiente acelerado de nesterov (NAG)
@article{NAGMethod,
  title={A method for solving the convex programming problem with convergence rate $O(1/k^2)$},
  author={Yurii Nesterov},
  journal={Proceedings of the USSR Academy of Sciences},
  year={1983},
  volume={269},
  pages={543-547},
  url={https://api.semanticscholar.org/CorpusID:145918791}
}

-- Slides que introduziram o RMSProp
@misc{RMSPropMethod,
  author = {Tieleman, Tijmen and Hinton, Geoffrey},
  title  = {Lecture 6.5---RMSProp: Divide the gradient by a running average of its recent magnitude},
  year   = {2012},
  howpublished = {COURSERA: Neural Networks for Machine Learning},
  url    = {https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf}
}

-- Um dos artigos que introduziu a sigmoide logística
@article{SigmoidVerhulst1845,
  author = {Verhulst, P. F.},
  title = {Recherches math{\'e}matiques sur la loi d'accroissement de la population},
  journal = {Nouveaux M{\'e}moires de l'Acad{\'e}mie Royale des Sciences et Belles-Lettres de Bruxelles},
  volume = {18},
  pages = {1--42},
  year = {1845},
  publisher = {L'Acad{\'e}mie Royale}
}

-- Um dos artigos que estuda o comportamento de neurônios e os compara com a sigmoide
@article{SigmoidWilsonCowan,
  author = {Wilson, Hugh R. and Cowan, Jack D.},
  title = {Excitatory and inhibitory interactions in localized populations of model neurons},
  journal = {Biophysical Journal},
  volume = {12},
  number = {1},
  pages = {1--24},
  year = {1972},
  publisher = {Biophysical Society},
  doi = {10.1016/S0006-3495(72)86068-5}
}

-- Artigo que introduz as redes de elman, utiliza a sigmoide como funcao de ativacao
@article{ElmanNetwork,
  author = {Elman, Jeffrey L.},
  title = {Finding structure in time},
  journal = {Cognitive Science},
  volume = {14},
  number = {2},
  pages = {179--211},
  year = {1990},
  doi = {10.1207/s15516709cog1402_1},
  publisher = {Wiley Online Library}
}

-- Artigo que introduz o perceptron
@article{PerceptronRosenblatt,
  author = {Rosenblatt, F.},
  title = {The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain},
  journal = {Psychological Review},
  volume = {65},
  number = {6},
  pages = {386--408},
  year = {1958},
  doi = {10.1037/h0042519}
}

-- Artigo que explica algumas funcoes de ativacao e suas propriedades
@misc{ActivationFunctionsLederer,
      title={Activation Functions in Artificial Neural Networks: A Systematic Overview}, 
      author={Johannes Lederer},
      year={2021},
      eprint={2101.09957},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2101.09957}, 
}

-- Artigo que introduziu a tangente hiperbólica
@incollection{TanhLambert,
      author    = {Lambert, J. H.},
      title     = {M{\'e}moire sur quelques propri{\'e}t{\'e}s remarquables des quantit{\'e}s transcendantes circulaires et logarithmiques},
      booktitle = {Pi: A Source Book},
      editor    = {Berggren, Lennart and Borwein, Jonathan M. and Borwein, Peter B.},
      publisher = {Springer-Verlag},
      address   = {New York},
      year      = {2004},
      pages     = {129--140},
      doi       = {10.1007/978-1-4757-4217-6_18},
      note      = {Original work published in 1768}
}

-- Artigo que introduziu a Le-Net-5
@article{LecunLeNet1998,
    author    = {LeCun, Yann and Bottou, L{\'e}on and Bengio, Yoshua and Haffner, Patrick},
    title     = {Gradient-based learning applied to document recognition},
    journal   = {Proceedings of the IEEE},
    volume    = {86},
    number    = {11},
    pages     = {2278--2324},
    year      = {1998},
    doi       = {10.1109/5.726791}
}

-- Artigo que introduziu a função de ativação sigmoidal softsign
@article{Softsign1998,
    author = {Foundation, The and Elliott, David},
    year = {1998},
    month = {12},
    pages = {},
    title = {A better Activation Function for Artificial Neural Networks}
}

-- Artigo que introduziu a função de ativação Leaky ReLU (LReLU)
@article{LeakyReLUArticle,
  author = {Andrew L. Mass, Awni Y. Hannum and Andrew Y. Ng},
  journal = {},
  number = {},
  title = {Rectifier Nonlinearities Improve Neural Networks Acustic Models},
  volume = {},
  year = {2013}
}

-- Artigo que introduziu a função de ativação Parametric ReLU (PReLU)
@misc{PReLUArticle,
      title={Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification}, 
      author={Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
      year={2015},
      eprint={1502.01852},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1502.01852}, 
}

-- Artigo que introduziu a função de ativação Exponential Linear Unit (ELU)
@misc{ELUArticle,
      title={Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)}, 
      author={Djork-Arné Clevert and Thomas Unterthiner and Sepp Hochreiter},
      year={2016},
      eprint={1511.07289},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1511.07289}, 
}

-- Artigo que introduziu a função de ativação SELU
@misc{SELUArticle,
      title={Self-Normalizing Neural Networks}, 
      author={Günter Klambauer and Thomas Unterthiner and Andreas Mayr and Sepp Hochreiter},
      year={2017},
      eprint={1706.02515},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1706.02515}, 
}

-- Artigo que introduziu a função de ativação GELU
@misc{GELUArticle,
      title={Gaussian Error Linear Units (GELUs)}, 
      author={Dan Hendrycks and Kevin Gimpel},
      year={2023},
      eprint={1606.08415},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1606.08415}, 
}

-- Artigo que introduziu o dataset CIFAR
@inproceedings{CIFAR10,
  title={Learning Multiple Layers of Features from Tiny Images},
  author={Alex Krizhevsky},
  year={2009},
  url={https://api.semanticscholar.org/CorpusID:18268744}
}

-- Um dos artigos que citam a função de ativação Noisy ReLU (NReLU)
@inproceedings{Nair2010,
  author    = {Nair, Vinod and Hinton, Geoffrey E.},
  title     = {Rectified Linear Units Improve Restricted Boltzmann Machines},
  booktitle = {Proceedings of the 27th International Conference on Machine Learning (ICML-10)},
  year      = {2010},
  pages     = {807--814},
}

-- Um dos artigos que estuda o comportamento das células nervosas
-- Utiliza uma versão da relu para explicar o comportamento
@article{Householder1941,
  author  = {Householder, Alston S.},
  title   = {A Theory of Steady-State Activity in Nerve-Fiber Networks: I. Definitions and Preliminary Theorems},
  journal = {The Bulletin of Mathematical Biophysics},
  year    = {1941},
  volume  = {3},
  number  = {2},
  pages   = {63--69},
}

-- Artigo que introduz a rede neural convolucional AlexNet
@inproceedings{AlexNet,
      author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
      booktitle = {Advances in Neural Information Processing Systems},
      editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},
      pages = {},
      publisher = {Curran Associates, Inc.},
      title = {ImageNet Classification with Deep Convolutional Neural Networks},
      url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf},
      volume = {25},
      year = {2012}
}

-- Um dos artigos que cita a RReLU
@article{XuRReLU,
  author  = {Xu, Bing and Wang, Naiyan and Chen, Tianqi and Li, Mu},
  title   = {Empirical Evaluation of Rectified Activations in Convolutional Network},
  journal = {arXiv preprint arXiv:1505.00853},
  year    = {2015},
}

-- Um dos artigos que cita propriedades da ReLU
@article{Glorot,
  author = {Xavier Glorot, Antaine Border and Yoshua Bengio},
  journal = {},
  number = {},
  title = {Deep Sparse Rectifier Neural Networks},
  volume = {},
  year = {2011}
}

-- Um dos artigos que cita o problema dos neurônios agonizantes
@misc{DyingReluDouglas,
      title={Why ReLU Units Sometimes Die: Analysis of Single-Unit Error Backpropagation in Neural Networks}, 
      author={Scott C. Douglas and Jiutian Yu},
      year={2018},
      eprint={1812.05981},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1812.05981}, 
}