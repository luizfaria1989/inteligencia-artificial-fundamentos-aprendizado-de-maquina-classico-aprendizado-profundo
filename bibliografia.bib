@book{DeepLearningBook,
  author    = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  title     = {Deep Learning},
  publisher = {MIT Press},
  year      = {2016},
}

@book{MaosAObra,
  author = {Aurélien Géron},
  title = {Mãos à Obra: Aprendizado de Máquina com Scikit-Learn e TensorFlow},
  publisher = {Alta Books},
  year = {2019},
}

-- Método do gradiente original por Cauchy
@article{CauchyMetodoDoGradiente,
  title   = {M{\'e}thode g{\'e}n{\'e}rale pour la r{\'e}solution des syst{\`e}mes d'{\'e}quations simultan{\'e}es},
  author  = {Cauchy, Augustin-Louis},
  journal = {Comptes Rendus Hebdomadaires des S{\'e}ances de l'Acad{\'e}mie des Sciences},
  volume  = {25},
  pages   = {536--538},
  year    = {1847}
}

-- Artigo que introduz a retropropagação
@article{BackpropagationArticle,
  author  = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
  title   = {Learning Representations by Back-Propagating Errors},
  journal = {Nature},
  year    = {1986},
  volume  = {323},
  pages   = {533--536},
}

-- Artigo que introduz os primeiros teoremas da aproximação universal
@article{Cybenko1989,
  author  = {Cybenko, George},
  title   = {Approximation by Superpositions of a Sigmoidal Function},
  journal = {Mathematics of Control, Signals, and Systems},
  year    = {1989},
  volume  = {2},
  number  = {4},
  pages   = {303--314},
}

-- Artigo que introduz os teoremas da aproximação universal para funções como a relu
@article{Leshno1993,
      title = {Multilayer feedforward networks with a nonpolynomial activation function can approximate any function},
      journal = {Neural Networks},
      volume = {6},
      number = {6},
      pages = {861-867},
      year = {1993},
      issn = {0893-6080},
      doi = {https://doi.org/10.1016/S0893-6080(05)80131-5},
      url = {https://www.sciencedirect.com/science/article/pii/S0893608005801315},
      author = {Moshe Leshno and Vladimir Ya. Lin and Allan Pinkus and Shimon Schocken},
      keywords = {Multilayer feedforward networks, Activation functions, Role of threshold, Universal approximation capabilities, (μ) approximation},
      abstract = {Several researchers characterized the activation function under which multilayer feedforward networks can act as universal approximators. We show that most of all the characterizations that were reported thus far in the literature are special cases of the following general result: A standard multilayer feedforward network with a locally bounded piecewise continuous activation function can approximate any continuous function to any degree of accuracy if and only if the network's activation function is not a polynomial. We also emphasize the important role of the threshold, asserting that without it the last theorem does not hold.}
}
-- Artigo que introduz o método do gradiente com momentum
@article{polyak1964,
  title={Some methods of speeding up the convergence of iteration methods},
  author={Polyak, Boris T.},
  journal={USSR Computational Mathematics and Mathematical Physics},
  volume={4},
  number={5},
  pages={1--17},
  year={1964},
  publisher={Elsevier}
}

-- Artigo que introduz o método de otimização Adam
@misc{AdamMethod,
      title={Adam: A Method for Stochastic Optimization}, 
      author={Diederik P. Kingma and Jimmy Ba},
      year={2017},
      eprint={1412.6980},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1412.6980}, 
      urldate={2025-09-15},
}

-- Artigo que introduz o método de otimização AdaGrad
@article{AdaGradMethod,
  author  = {John Duchi and Elad Hazan and Yoram Singer},
  title   = {Adaptive Subgradient Methods for Online Learning and Stochastic Optimization},
  journal = {Journal of Machine Learning Research},
  year    = {2011},
  volume  = {12},
  number  = {61},
  pages   = {2121--2159},
  url     = {http://jmlr.org/papers/v12/duchi11a.html},
  urldate={2025-09-15},
}

-- Artigo que introduz o método de otimização Nadam
@inproceedings{NadamMethod,
  title={Incorporating Nesterov Momentum into Adam},
  author={Dozat, Timothy},
  booktitle={ICLR 2016 Workshop Track},
  year={2016},
  url={https://openreview.net/forum?id=OM0DEvNMIxAaI-fG_O1-I},
  urldate={2025-09-15},
}

-- Artigo que introduz o método de otimização AdamW
@misc{AdamWMethod,
      title={Decoupled Weight Decay Regularization}, 
      author={Ilya Loshchilov and Frank Hutter},
      year={2019},
      eprint={1711.05101},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1711.05101}, 
      urldate={2025-09-15},
}

-- Artigo que introduz o método de otimização RAdam
@misc{RAdamMethod,
      title={On the Variance of the Adaptive Learning Rate and Beyond}, 
      author={Liyuan Liu and Haoming Jiang and Pengcheng He and Weizhu Chen and Xiaodong Liu and Jianfeng Gao and Jiawei Han},
      year={2021},
      eprint={1908.03265},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1908.03265}, 
      urldate={2025-09-15},
}

-- Artigo que introduz o método do gradiente estocástico
@article{StochasticGradientDescentMethod,
  author    = {Herbert Robbins and Sutton Monro},
  title     = {A Stochastic Approximation Method},
  journal   = {The Annals of Mathematical Statistics},
  year      = {1951},
  volume    = {22},
  number    = {3},
  pages     = {400--407},
  doi       = {10.1214/aoms/1177729586}
}

-- Adeline 
-- SGD aparece pela primeira vez em aprendizado de máquina
@inproceedings{Adeline,
  author    = {Bernard Widrow and Marcian E. Hoff},
  title     = {Adaptive Switching Circuits},
  booktitle = {1960 IRE WESCON Convention Record},
  year      = {1960},
  part      = {4},
  pages     = {96--104},
  publisher = {IRE}
}

-- Artigo que introduz o método de otimização do gradiente acelerado de nesterov (NAG)
@article{NAGMethod,
  title={A method for solving the convex programming problem with convergence rate $O(1/k^2)$},
  author={Yurii Nesterov},
  journal={Proceedings of the USSR Academy of Sciences},
  year={1983},
  volume={269},
  pages={543-547},
  url={https://api.semanticscholar.org/CorpusID:145918791}
}

-- Slides que introduziram o RMSProp
@misc{RMSPropMethod,
  author = {Tieleman, Tijmen and Hinton, Geoffrey},
  title  = {Lecture 6.5---RMSProp: Divide the gradient by a running average of its recent magnitude},
  year   = {2012},
  howpublished = {COURSERA: Neural Networks for Machine Learning},
  url    = {https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf},
  urldate={2025-10-03},
}

-- Um dos artigos que introduziu a sigmoide logística
@article{SigmoidVerhulst1845,
  author = {Verhulst, P. F.},
  title = {Recherches math{\'e}matiques sur la loi d'accroissement de la population},
  journal = {Nouveaux M{\'e}moires de l'Acad{\'e}mie Royale des Sciences et Belles-Lettres de Bruxelles},
  volume = {18},
  pages = {1--42},
  year = {1845},
  publisher = {L'Acad{\'e}mie Royale}
}

-- Um dos artigos que estuda o comportamento de neurônios e os compara com a sigmoide
@article{SigmoidWilsonCowan,
  author = {Wilson, Hugh R. and Cowan, Jack D.},
  title = {Excitatory and inhibitory interactions in localized populations of model neurons},
  journal = {Biophysical Journal},
  volume = {12},
  number = {1},
  pages = {1--24},
  year = {1972},
  publisher = {Biophysical Society},
  doi = {10.1016/S0006-3495(72)86068-5}
}

-- Artigo que introduz as redes de elman, utiliza a sigmoide como funcao de ativacao
@article{ElmanNetwork,
  author = {Elman, Jeffrey L.},
  title = {Finding structure in time},
  journal = {Cognitive Science},
  volume = {14},
  number = {2},
  pages = {179--211},
  year = {1990},
  doi = {10.1207/s15516709cog1402_1},
  publisher = {Wiley Online Library}
}

-- Artigo que introduz o perceptron
@article{PerceptronRosenblatt,
  author = {Rosenblatt, F.},
  title = {The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain},
  journal = {Psychological Review},
  volume = {65},
  number = {6},
  pages = {386--408},
  year = {1958},
  doi = {10.1037/h0042519}
}

-- Artigo que explica algumas funcoes de ativacao e suas propriedades
@misc{ActivationFunctionsLederer,
      title={Activation Functions in Artificial Neural Networks: A Systematic Overview}, 
      author={Johannes Lederer},
      year={2021},
      eprint={2101.09957},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2101.09957}, 
}

-- Artigo que introduziu a tangente hiperbólica
@incollection{TanhLambert,
      author    = {Lambert, J. H.},
      title     = {M{\'e}moire sur quelques propri{\'e}t{\'e}s remarquables des quantit{\'e}s transcendantes circulaires et logarithmiques},
      booktitle = {Pi: A Source Book},
      editor    = {Berggren, Lennart and Borwein, Jonathan M. and Borwein, Peter B.},
      publisher = {Springer-Verlag},
      address   = {New York},
      year      = {2004},
      pages     = {129--140},
      doi       = {10.1007/978-1-4757-4217-6_18},
      note      = {Original work published in 1768}
}

-- Artigo que introduziu a Le-Net-5
@article{LecunLeNet1998,
    author    = {LeCun, Yann and Bottou, L{\'e}on and Bengio, Yoshua and Haffner, Patrick},
    title     = {Gradient-based learning applied to document recognition},
    journal   = {Proceedings of the IEEE},
    volume    = {86},
    number    = {11},
    pages     = {2278--2324},
    year      = {1998},
    doi       = {10.1109/5.726791}
}

-- Artigo que introduziu a função de ativação sigmoidal softsign
@article{Softsign1998,
    author = {Foundation, The and Elliott, David},
    year = {1998},
    month = {12},
    pages = {},
    title = {A better Activation Function for Artificial Neural Networks}
}

-- Artigo que introduziu a função de ativação Leaky ReLU (LReLU)
@article{LeakyReLUArticle,
  author = {Andrew L. Mass, Awni Y. Hannum and Andrew Y. Ng},
  journal = {},
  number = {},
  title = {Rectifier Nonlinearities Improve Neural Networks Acustic Models},
  volume = {},
  year = {2013}
}

-- Artigo que introduziu a função de ativação Parametric ReLU (PReLU)
@misc{PReLUArticle,
      title={Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification}, 
      author={Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
      year={2015},
      eprint={1502.01852},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1502.01852}, 
}

-- Artigo que introduziu a função de ativação Exponential Linear Unit (ELU)
@misc{ELUArticle,
      title={Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)}, 
      author={Djork-Arné Clevert and Thomas Unterthiner and Sepp Hochreiter},
      year={2016},
      eprint={1511.07289},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1511.07289}, 
}

-- Artigo que introduziu a função de ativação SELU
@misc{SELUArticle,
      title={Self-Normalizing Neural Networks}, 
      author={Günter Klambauer and Thomas Unterthiner and Andreas Mayr and Sepp Hochreiter},
      year={2017},
      eprint={1706.02515},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1706.02515}, 
}

-- Artigo que introduziu a função de ativação GELU
@misc{GELUArticle,
      title={Gaussian Error Linear Units (GELUs)}, 
      author={Dan Hendrycks and Kevin Gimpel},
      year={2023},
      eprint={1606.08415},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1606.08415}, 
}

-- Artigo que introduziu o dataset CIFAR
@inproceedings{CIFAR10,
  title={Learning Multiple Layers of Features from Tiny Images},
  author={Alex Krizhevsky},
  year={2009},
  url={https://api.semanticscholar.org/CorpusID:18268744}
}

-- Um dos artigos que citam a função de ativação Noisy ReLU (NReLU)
@inproceedings{Nair2010,
  author    = {Nair, Vinod and Hinton, Geoffrey E.},
  title     = {Rectified Linear Units Improve Restricted Boltzmann Machines},
  booktitle = {Proceedings of the 27th International Conference on Machine Learning (ICML-10)},
  year      = {2010},
  pages     = {807--814},
}

-- Um dos artigos que estuda o comportamento das células nervosas
-- Utiliza uma versão da relu para explicar o comportamento
@article{Householder1941,
  author  = {Householder, Alston S.},
  title   = {A Theory of Steady-State Activity in Nerve-Fiber Networks: I. Definitions and Preliminary Theorems},
  journal = {The Bulletin of Mathematical Biophysics},
  year    = {1941},
  volume  = {3},
  number  = {2},
  pages   = {63--69},
}

-- Artigo que introduz a rede neural convolucional AlexNet
@inproceedings{AlexNet,
      author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
      booktitle = {Advances in Neural Information Processing Systems},
      editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},
      pages = {},
      publisher = {Curran Associates, Inc.},
      title = {ImageNet Classification with Deep Convolutional Neural Networks},
      url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf},
      volume = {25},
      year = {2012}
}

-- Um dos artigos que cita a RReLU
@article{XuRReLU,
  author  = {Xu, Bing and Wang, Naiyan and Chen, Tianqi and Li, Mu},
  title   = {Empirical Evaluation of Rectified Activations in Convolutional Network},
  journal = {arXiv preprint arXiv:1505.00853},
  year    = {2015},
}

-- Um dos artigos que cita propriedades da ReLU
@article{Glorot,
  author = {Xavier Glorot, Antaine Border and Yoshua Bengio},
  journal = {},
  number = {},
  title = {Deep Sparse Rectifier Neural Networks},
  volume = {},
  year = {2011}
}

-- Um dos artigos que cita o problema dos neurônios agonizantes
@misc{DyingReluDouglas,
      title={Why ReLU Units Sometimes Die: Analysis of Single-Unit Error Backpropagation in Neural Networks}, 
      author={Scott C. Douglas and Jiutian Yu},
      year={2018},
      eprint={1812.05981},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1812.05981}, 
}

-- Um dos artigos que cita o problema do gradiente explosivo
@misc{ExplodingGradient,
      title={The exploding gradient problem demystified - definition, prevalence, impact, origin, tradeoffs, and solutions}, 
      author={George Philipp and Dawn Song and Jaime G. Carbonell},
      year={2018},
      eprint={1712.05577},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1712.05577}, 
}

-- Documentação da hard sigmoid do PyTorch
@online{PyTorchHardSigmoid,
  author = {{PyTorch}},
  title = {Hardsigmoid},
  url = {https://pytorch.org/docs/stable/generated/torch.nn.Hardsigmoid.html},
  year = {2025},
  urldate = {2025-10-10}
}

-- Documentação da hard tanh do PyTorch
@online{PyTorchHardTanh,
  author = {{PyTorch}},
  title = {Hardtanh},
  url = {https://pytorch.org/docs/stable/generated/torch.nn.Hardtanh.html},
  year = {2025},
  urldate = {2025-10-10}
}

-- Artigo que discute um pouco das propriedades das funções de ativação
@misc{PropriedadesFuncoesDeAtivacao,
      title={A Survey on Activation Functions and their relation with Xavier and He Normal Initialization}, 
      author={Leonid Datta},
      year={2020},
      eprint={2004.06632},
      archivePrefix={arXiv},
      primaryClass={cs.NE},
      url={https://arxiv.org/abs/2004.06632}, 
}

-- Documentação da leaky relu do PyTorch
@online{PyTorchLeakyReLU,
  author = {{PyTorch}},
  title = {LeakyReLU},
  url = {https://docs.pytorch.org/docs/stable/generated/torch.nn.LeakyReLU.html},
  year = {2025},
  urldate = {2025-10-11}
}

-- Livro que cria a regressão dos mínimos quadrados e a mse
@book{Legendre1805,
  author    = {Legendre, Adrien-Marie},
  title     = {Nouvelles m{\'e}thodes pour la d{\'e}termination des orbites des com{\`e}tes},
  publisher = {Firmin Didot},
  year      = {1805},
  address   = {Paris},
  note      = {Cont{\'e}m o ap{\^e}ndice "Sur la M{\'e}thode des Moindres Quarr{\'e}s", que apresenta a primeira publica{\c c}{\~a}o do m{\'e}todo dos m{\'i}nimos quadrados.}
}

-- Livro do Gauss que cita a regressao dos minimos quadrados e a mse
@book{Gauss1809,
  author    = {Gauss, Carl Friedrich},
  title     = {{Theoria motus corporum coelestium in sectionibus conicis solem ambientium}},
  publisher = {sumtibus F. Perthes et I. H. Besser},
  year      = {1809},
  address   = {Hamburgi}
}

-- Artigo das funções de perda
@article{LossesArticle,
   title={A comprehensive survey of loss functions and metrics in deep learning},
   volume={58},
   ISSN={1573-7462},
   url={http://dx.doi.org/10.1007/s10462-025-11198-7},
   DOI={10.1007/s10462-025-11198-7},
   number={7},
   journal={Artificial Intelligence Review},
   publisher={Springer Science and Business Media LLC},
   author={Terven, Juan and Cordova-Esparza, Diana-Margarita and Romero-González, Julio-Alejandro and Ramírez-Pedraza, Alfonso and Chávez-Urbiola, E. A.},
   year={2025},
   urldate = {2025-10-17},
   month=apr 
}

-- Um dos artigos que faz uso do erro absoluto médio
@article{GreedyFunctionApproximation,
    author = {Jerome H. Friedman},
    title = {{Greedy function approximation: A gradient boosting machine.}},
    volume = {29},
    journal = {The Annals of Statistics},
    number = {5},
    publisher = {Institute of Mathematical Statistics},
    pages = {1189 -- 1232},
    keywords = {boosting, decision trees, Function estimation, robust nonparametric regression},
    year = {2001},
    doi = {10.1214/aos/1013203451},
    URL = {https://doi.org/10.1214/aos/1013203451},
    urldate = {2025-10-17}
}

-- Um dos artigos que faz uso do erro absoluto médio
@misc{ImageToImage,
      title={Image-to-Image Translation with Conditional Adversarial Networks}, 
      author={Phillip Isola and Jun-Yan Zhu and Tinghui Zhou and Alexei A. Efros},
      year={2018},
      eprint={1611.07004},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1611.07004}, 
      urldate={2025-10-17}
}

-- Artigo que introduz a huber loss
@article{HuberLoss,
  author = {Peter J. Huber},
  title = {{Robust Estimation of a Location Parameter}},
  volume = {35},
  journal = {The Annals of Mathematical Statistics},
  number = {1},
  publisher = {Institute of Mathematical Statistics},
  pages = {73 -- 101},
  year = {1964},
  doi = {10.1214/aoms/1177703732},
  URL = {https://doi.org/10.1214/aoms/1177703732},
  urldate={2025-10-17}
}

-- Definição de Entropia por Shannon
@ARTICLE{EntropyShannon,
  author={Shannon, C. E.},
  journal={The Bell System Technical Journal}, 
  title={A mathematical theory of communication}, 
  year={1948},
  volume={27},
  number={3},
  pages={379-423},
  keywords={},
  doi={10.1002/j.1538-7305.1948.tb01338.x}
}

-- Criação da Divergência de Kullback-leibler
@article{KullbackLeiblerDivergence,
  ISSN = {00034851},
  URL = {http://www.jstor.org/stable/2236703},
  author = {S. Kullback and R. A. Leibler},
  journal = {The Annals of Mathematical Statistics},
  number = {1},
  pages = {79--86},
  publisher = {Institute of Mathematical Statistics},
  title = {On Information and Sufficiency},
  urldate = {2025-10-18},
  volume = {22},
  year = {1951},
}

@article{HintonConnectionist,
  title = {Connectionist learning procedures},
  journal = {Artificial Intelligence},
  volume = {40},
  number = {1},
  pages = {185-234},
  year = {1989},
  issn = {0004-3702},
  doi = {https://doi.org/10.1016/0004-3702(89)90049-0},
  url = {https://www.sciencedirect.com/science/article/pii/0004370289900490},
  author = {Geoffrey E. Hinton},
  abstract = {A major goal of research on networks of neuron-like processing units is to discover efficient learning procedures that allow these networks to construct complex internal representations of their environment. The learning procedures must be capable of modifying the connection strengths in such a way that internal units which are not part of the input or output come to represent important features of the task domain. Several interesting gradient-descent procedures have recently been discovered. Each connection computes the derivative, with respect to the connection strength, of a global measure of the error in the performance of the network. The strength is then adjusted in the direction that decreases the error. These relatively simple, gradient-descent learning procedures work well for small tasks and the new challenge is to find ways of improving their convergence rate and their generalization abilities so that they can be applied to larger, more realistic tasks.},
  urldate={2025-10-18}
}

-- Artigo que introduz a Hinge Loss
@article{HingeLoss,
	abstract = {Thesupport-vector network is a new learning machine for two-group classification problems. The machine conceptually implements the following idea: input vectors are non-linearly mapped to a very high-dimension feature space. In this feature space a linear decision surface is constructed. Special properties of the decision surface ensures high generalization ability of the learning machine. The idea behind the support-vector network was previously implemented for the restricted case where the training data can be separated without errors. We here extend this result to non-separable training data.},
	author = {Cortes, Corinna and Vapnik, Vladimir},
	date = {1995/09/01},
	date-added = {2025-10-19 14:29:02 -0300},
	date-modified = {2025-10-19 14:29:02 -0300},
	doi = {10.1007/BF00994018},
	id = {Cortes1995},
	isbn = {1573-0565},
	journal = {Machine Learning},
	number = {3},
	pages = {273--297},
	title = {Support-vector networks},
	url = {https://doi.org/10.1007/BF00994018},
	volume = {20},
	year = {1995},
	bdsk-url-1 = {https://doi.org/10.1007/BF00994018}
  
}

-- Artigo que explica as propriedades estatísticas da função de perda log-cosh
@misc{StatisticalPropetiesLogCosh,
      title={Statistical Properties of the log-cosh Loss Function Used in Machine Learning}, 
      author={Resve A. Saleh and A. K. Md. Ehsanes Saleh},
      year={2024},
      eprint={2208.04564},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/2208.04564}, 
      urldate = {2025-10-21},
}

-- Artigo que usa regressão linear para prever os custos médicos
-- Usam o MAE e o MSE e o Rˆ2 como métricas de avaliação
@article{MedicalCostsEstimationUsingLR,
  author  = {Dwikasaria, Ni Made Dita and Sutramiani, Ni Putu and Putri, Komang Sri Yanisa and Kusuma, Nyoman Tri Rahaditya and Pramana, Made Dimas Aldi Dwi and Darma, I Wayan Agus Surya},
  title   = {Medical Costs Estimation Using Linear Regression Method},
  journal = {JURNAL ILMIAH MERPATI},
  year    = {2023},
  volume  = {11},
  number  = {3},
  pages   = {171--179},
  month   = {December},
  issn    = {2252-3006},
  eissn   = {2685-2411},
  keywords= {Medical Costs, Data Mining, Estimation, Linear Regression},
  url={https://www.google.com/url?sa=t&source=web&rct=j&opi=89978449&url=https://scispace.com/pdf/medical-costs-estimation-using-linear-regression-method-38k6wo4bri.pdf&ved=2ahUKEwiT5fDb8r-QAxVhK7kGHQArCKkQFnoECBgQAQ&usg=AOvVaw22f-dAUr1phgkxjmJ7Nh7a},
  urldate={2025-10-25},
}

-- Artigo que usa técnicas de regressão para prever preços de casas
-- Usam o MAE e o MSE como métricas de avaliação
@article{OptimalHousePricePrediction,
   title={An Optimal House Price Prediction Algorithm: XGBoost},
   volume={3},
   ISSN={2813-2203},
   url={http://dx.doi.org/10.3390/analytics3010003},
   DOI={10.3390/analytics3010003},
   number={1},
   journal={Analytics},
   publisher={MDPI AG},
   author={Sharma, Hemlata and Harsora, Hitesh and Ogunleye, Bayode},
   year={2024},
   month=jan, 
   pages={30–45},
   urldate={2025-10-25}
}

-- Artigo que usam técnicas de regressão e combinam com para prever a produção de milho no cinturão do milho nos eua
-- Usam o RMSE como métrica de avaliacao
@article{CouplingMachineLearningAndCropModeling,
   title={Coupling machine learning and crop modeling improves crop yield prediction in the US Corn Belt},
   volume={11},
   ISSN={2045-2322},
   url={http://dx.doi.org/10.1038/s41598-020-80820-1},
   DOI={10.1038/s41598-020-80820-1},
   number={1},
   journal={Scientific Reports},
   publisher={Springer Science and Business Media LLC},
   author={Shahhosseini, Mohsen and Hu, Guiping and Huber, Isaiah and Archontoulis, Sotirios V.},
   year={2021},
   month=jan,
   urldate={2025-10-25}
}

-- Artigo que usa técnicas de regressão para prever demanda de energia em microgrids
-- Usa o MSE indiretamente, transformando-o em EW-MSE, uma funcao de perda especial para resolver o problema do artigo
@misc{OptimizingFL,
      title={Optimizing Federated Learning for Scalable Power-demand Forecasting in Microgrids}, 
      author={Roopkatha Banerjee and Sampath Koti and Gyanendra Singh and Anirban Chakraborty and Gurunath Gurrala and Bhushan Jagyasi and Yogesh Simmhan},
      year={2025},
      eprint={2508.08022},
      archivePrefix={arXiv},
      primaryClass={cs.DC},
      url={https://arxiv.org/abs/2508.08022}, 
      urldate={2025-10-25},
}

--
--
@misc{nedungadi2025aircastimprovingairpollution,
      title={AirCast: Improving Air Pollution Forecasting Through Multi-Variable Data Alignment}, 
      author={Vishal Nedungadi and Muhammad Akhtar Munir and Marc Rußwurm and Ron Sarafian and Ioannis N. Athanasiadis and Yinon Rudich and Fahad Shahbaz Khan and Salman Khan},
      year={2025},
      eprint={2502.17919},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2502.17919}, 
      urldate={2025-10-27},
}

--
--
@article{MinimumOpenDataSubsetForWindPowerPrediction,
  author = {Zuben, Elizabeth and Schell, Kristen},
  year = {2025},
  month = {03},
  pages = {},
  title = {Minimum Open Data Subset for Wind Power Prediction},
  doi = {10.5194/wes-2025-29},
}

--
--
@misc{Noise2Noise,
      title={Noise2Noise: Learning Image Restoration without Clean Data}, 
      author={Jaakko Lehtinen and Jacob Munkberg and Jon Hasselgren and Samuli Laine and Tero Karras and Miika Aittala and Timo Aila},
      year={2018},
      eprint={1803.04189},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1803.04189}, 
      urldate={2025-10-27},
}

--
--
@misc{chao2022regressionmetriclosslearning,
      title={Regression Metric Loss: Learning a Semantic Representation Space for Medical Images}, 
      author={Hanqing Chao and Jiajin Zhang and Pingkun Yan},
      year={2022},
      eprint={2207.05231},
      archivePrefix={arXiv},
      primaryClass={eess.IV},
      url={https://arxiv.org/abs/2207.05231}, 
      urldate={2025-10-27},
}

-- Utilizam a perda de Huber para criar um algoritmo de emsemble para prever dados médicos
@misc{HuberLossSuperLearner,
      title={A Huber loss-based super learner with applications to healthcare expenditures}, 
      author={Ziyue Wu and David Benkeser},
      year={2022},
      eprint={2205.06870},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/2205.06870}, 
      urldate={2025-10-30}
}

--
--
@misc{RobustTrendHuberLoss,
      title={RobustTrend: A Huber Loss with a Combined First and Second Order Difference Regularization for Time Series Trend Filtering}, 
      author={Qingsong Wen and Jingkun Gao and Xiaomin Song and Liang Sun and Jian Tan},
      year={2019},
      eprint={1906.03751},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1906.03751}, 
      urldate={2025-10-30}
}

--
--
@misc{AdaptiveHuberRegression,
      title={Adaptive Huber Regression}, 
      author={Qiang Sun and Wenxin Zhou and Jianqing Fan},
      year={2018},
      eprint={1706.06991},
      archivePrefix={arXiv},
      primaryClass={math.ST},
      url={https://arxiv.org/abs/1706.06991}, 
      urldate={2025-10-30}
}

@INPROCEEDINGS{RobustAleatoricModelingVehicleLocalization,
      author={Hudnell, Max and Price, True and Frahm, Jan-Michael},
      booktitle={2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)}, 
      title={Robust Aleatoric Modeling for Future Vehicle Localization}, 
      year={2019},
      volume={},
      number={},
      pages={2944-2951},
      keywords={Uncertainty;Predictive models;Two dimensional displays;Task analysis;Robustness;Trajectory;Recurrent neural networks},
      doi={10.1109/CVPRW.2019.00355}
}

--
--
@inproceedings{FinantialMarketForecastingUsingRNN,
author = {Kehinde, Temitope and Khan, Waqar Ahmed and Chung, Sai-Ho},
year = {2023},
month = {10},
pages = {},
title = {Financial Market Forecasting using RNN, LSTM, BiLSTM, GRU and Transformer-Based Deep Learning Algorithms},
doi = {10.46254/EV01.20230037}
}

--
--
@article{SiameseRecurrentNeuralNetwork,
  author = {Fernández-Llaneza, Daniel and Ulander, Silas and Gogishvili, Dea and Nittinger, Eva and Zhao, Hongtao and Tyrchan, Christian},
  title = {Siamese Recurrent Neural Network with a Self-Attention Mechanism for Bioactivity Prediction},
  journal = {ACS Omega},
  volume = {6},
  number = {16},
  pages = {11086-11094},
  year = {2021},
  doi = {10.1021/acsomega.1c01266},
  note ={PMID: 34056263},
  URL = {https://doi.org/10.1021/acsomega.1c01266},
  eprint = {https://doi.org/10.1021/acsomega.1c01266},
  urldate = {2025-10-31},
}

@Article{AnEffectiveMethodForDetectingUnknowTypes,
  AUTHOR = {Yu, Li and Xu, Liuquan and Jiang, Xuefeng},
  TITLE = {An Effective Method for Detecting Unknown Types of Attacks Based on Log-Cosh Variational Autoencoder},
  JOURNAL = {Applied Sciences},
  VOLUME = {13},
  YEAR = {2023},
  NUMBER = {22},
  ARTICLE-NUMBER = {12492},
  URL = {https://www.mdpi.com/2076-3417/13/22/12492},
  ISSN = {2076-3417},
  urldate={2025-10-31},
}

@misc{SunKim-NetOverYourHead,
  author = {Sun, Hongtao and Kim, Ji Hoon "Andy"},
  title  = {A Net Over Your Head: A Neural Network Approach to Home Price Predictions},
  url = {https://cs229.stanford.edu/proj2017/final-reports/5191322.pdf},
  urldate = {2025-11-04}
}

@inproceedings{Winter2023PredictingLOS,
  author    = {Winter, Alexander and Hartwig, Mattis and Kirsten, Toralf},
  title     = {Predicting Hospital Length of Stay of Patients Leaving the Emergency Department},
  booktitle = {Proceedings of the 16th International Joint Conference on Biomedical Engineering Systems and Technologies (BIOSTEC 2023) - Volume 5: HEALTHINF},
  year      = {2023},
  pages     = {124--131},
  isbn      = {978-989-758-631-6},
  issn      = {2184-4305},
  doi       = {10.5220/0011671700003414},
  url = {https://www.scitepress.org/Papers/2023/116717/116717.pdf},
  urldate = {2025-11-04},
}

@misc{Jadon2022ComprehensiveSurvey,
  author = {Jadon, Aryan and Patil, Avinash and Jadon, Shruti},
  title  = {A Comprehensive Survey of Regression Based Loss Functions for Time Series Forecasting},
  year   = {2022},
  eprint = {2211.02989},
  archivePrefix = {arXiv},
  primaryClass = {cs.LG},
  url = {https://arxiv.org/pdf/2211.02989},
  urldate = {2025-11-04}
}

-- Artigo que introduz a regressão quantílica e a perda quantílica
@article{regression-quantiles,
  ISSN = {00129682, 14680262},
  URL = {http://www.jstor.org/stable/1913643},
  abstract = {A simple minimization problem yielding the ordinary sample quantiles in the location model is shown to generalize naturally to the linear model generating a new class of statistics we term "regression quantiles." The estimator which minimizes the sum of absolute residuals is an important special case. Some equivariance properties and the joint asymptotic distribution of regression quantiles are established. These results permit a natural generalization of the linear model of certain well-known robust estimators of location. Estimators are suggested, which have comparable efficiency to least squares for Gaussian linear models while substantially out-performing the least-squares estimator over a wide class of non-Gaussian error distributions.},
  author = {Roger Koenker and Gilbert Bassett},
  journal = {Econometrica},
  number = {1},
  pages = {33--50},
  publisher = {[Wiley, Econometric Society]},
  title = {Regression Quantiles},
  urldate = {2025-11-08},
  volume = {46},
  year = {1978}
}

-- Um dos artigos que trabalha com a regressão quantílica e com a perda quantílica
@misc{candila2023mixedfrequencyquantileregressionsforecast,
      title={Mixed--frequency quantile regressions to forecast Value--at--Risk and Expected Shortfall}, 
      author={Vincenzo Candila and Giampiero M. Gallo and Lea Petrella},
      year={2023},
      eprint={2011.00552},
      archivePrefix={arXiv},
      primaryClass={q-fin.ST},
      url={https://arxiv.org/abs/2011.00552}, 
      urldate={2025-11-08},
}

-- Um dos artigos que trabalha com a regressão quantílica e com a perda quantílica
@article{Engle2004CAViaR,
  title   = {{CAViaR: Conditional Autoregressive Value at Risk by Regression Quantiles}},
  author  = {Engle, Robert F. and Manganelli, Simone},
  journal = {Journal of Business \& Economic Statistics},
  volume  = {22},
  number  = {4},
  pages   = {367--381},
  year    = {2004},
  doi     = {10.1198/073500104000000313}
}

-- Um dos artigos que usa a regressão quantílica e a perda quantílica
@article{WINKELMANN2006131,
  title = {Reforming health care: Evidence from quantile regressions for counts},
  journal = {Journal of Health Economics},
  volume = {25},
  number = {1},
  pages = {131-145},
  year = {2006},
  issn = {0167-6296},
  doi = {https://doi.org/10.1016/j.jhealeco.2005.03.005},
  url = {https://www.sciencedirect.com/science/article/pii/S0167629605000433},
  author = {Rainer Winkelmann},
  keywords = {Co-payments, Prescription drugs, Count data, Quantile regression, Poisson model},
  abstract = {I consider the problem of estimating the effect of a health care reform on the frequency of individual doctor visits when the reform effect is potentially different in different parts of the outcome distribution. Quantile regression is a powerful method for studying such heterogeneous treatment effects. Only recently has this method been extended to situations where the dependent variable is a (non-negative integer) count. An analysis of a 1997 health care reform in Germany shows that lower quantiles, such as the first quartile, fell by substantially larger amounts than what would have been predicted based on Poisson or negative binomial models.}
}

-- Artigo que introduz a perda epsilon-insensivel
@inproceedings{SupportVectorRegressionMachines,
  author    = {Drucker, Harris and Burges, Christopher J. C. and Kaufman, Linda and Smola, Alexander J. and Vapnik, Vladimir},
  title     = {Support Vector Regression Machines},
  booktitle = {Advances in Neural Information Processing Systems 9 (NIPS 1996)},
  editor    = {Mozer, Michael C. and Jordan, Michael I. and Petsche, Thomas},
  pages     = {155--161},
  year      = {1997},
  publisher = {MIT Press},
}

-- Artigo que usa a regressão de poisson para prever taxas de crime
@article{poisson-regression-for-crime-rates,
	abstract = {This article introduces the use of regression models based on the Poissondistribution as a tool for resolving common problems in analyzing aggregatecrime rates. When the population size of an aggregate unit is small relativeto the offense rate, crime rates must be computed from a small number ofoffenses. Such data are ill-suited to least-squares analysis. Poisson-basedregression models of counts of offenses are preferable because they arebuilt on assumptions about error distributions that are consistent withthe nature of event counts. A simple elaboration transforms the Poissonmodel of offense counts to a model of per capita offense rates. Todemonstrate the use and advantages of this method, this article presentsanalyses of juvenile arrest rates for robbery in 264 nonmetropolitancounties in four states. The negative binomial variant of Poisson regressioneffectively resolved difficulties that arise in ordinary least-squaresanalyses.},
	author = {Osgood, D.  Wayne},
	date = {2000/03/01},
	date-added = {2025-11-19 09:48:48 -0300},
	date-modified = {2025-11-19 09:48:48 -0300},
	doi = {10.1023/A:1007521427059},
	id = {Osgood2000},
	isbn = {1573-7799},
	journal = {Journal of Quantitative Criminology},
	number = {1},
	pages = {21--43},
	title = {Poisson-Based Regression Analysis of Aggregate Crime Rates},
	url = {https://doi.org/10.1023/A:1007521427059},
	volume = {16},
	year = {2000},
	bdsk-url-1 = {https://doi.org/10.1023/A:1007521427059}
}


@article{poisson-regression-and-zero-inflated-poisson-regression,
	abstract = {Modeling event counts is important in many fields. For this purpose, the Poisson regression model is often used. However, this model assumes the equidispersion of the data. Unfortunately, this assumption is often violated in the observed data. The source of overdispersion depends on many situations. When the source of overdispersion is the excess of zeroes, the Zero-inflated Poisson regression model fits better counts data. In this paper, we first review the theoretical framework of Poisson regression and Zero-inflated Poisson regression. The probability integral transform test and the Vuong's test are used to compare between the two models. Second, we fit these models to the number of claims in a private health insurance scheme. In our case, the number of claims is overdispersed because of the preponderance of zeroes in the data set. The results prove that Zero-inflated Poisson regression performs better the number of claims of the customers affiliated in the health insurance scheme in the Moroccan case.},
	author = {Mouatassim, Youn{\`e}s and Ezzahid, El Hadj},
	date = {2012/12/01},
	date-added = {2025-11-19 09:50:50 -0300},
	date-modified = {2025-11-19 09:50:50 -0300},
	doi = {10.1007/s13385-012-0056-2},
	id = {Mouatassim2012},
	isbn = {2190-9741},
	journal = {European Actuarial Journal},
	number = {2},
	pages = {187--204},
	title = {Poisson regression and Zero-inflated Poisson regression: application to private health insurance data},
	url = {https://doi.org/10.1007/s13385-012-0056-2},
	volume = {2},
	year = {2012},
	bdsk-url-1 = {https://doi.org/10.1007/s13385-012-0056-2}
}

@article{poisson-regression-in-epidemiology,
  author = {Viel, J. F.},
  title = {La r{\'e}gression de Poisson en {\'e}pid{\'e}miologie [Poisson regression in epidemiology]},
  journal = {Rev Epidemiol Sante Publique},
  year = {1994},
  volume = {42},
  number = {1},
  pages = {79--87},
  language = {French},
  pmid = {8134669}
}

@book{GeneralizedLinearModels,
  author    = {P. McCullagh, J.A. Nelder FRS},
  title     = {Generealized Linear Models},
  publisher = {Chapman and Hall},
  year      = {1983},
}

@book{rudin1976principles,
  author    = {Rudin, Walter},
  title     = {Principles of Mathematical Analysis},
  publisher = {McGraw-Hill},
  year      = {1976},
  edition   = {3rd},
  series    = {International Series in Pure and Applied Mathematics},
  address   = {United States},
  note      = {Includes Index}
}

@book{boyd2004convex,
  title={Convex optimization},
  author={Boyd, Stephen and Vandenberghe, Lieven},
  year={2004},
  publisher={Cambridge university press}
}

@article{yang2019predictive,
  title = {Predictive Statistical Representations of Observed and Simulated Rainfall Using Generalized Linear Models},
  author = {Yang, J. and Jun, M. and Schumacher, C. and Saravanan, R.},
  journal = {Journal of Climate},
  volume = {32},
  number = {11},
  pages = {3409--3427},
  year = {2019},
  month = {jun},
  publisher = {American Meteorological Society},
  doi = {10.1175/JCLI-D-18-0527.1},
  pmid = {32773963},
  pmcid = {PMC7409988},
  url={https://pubmed.ncbi.nlm.nih.gov/32773963/},
  urldate={12/21/2025}
}

@article{davoudikakhki2018analyzing,
  title = {Analyzing Large Workers' Compensation Claims Using Generalized Linear Models and Monte Carlo Simulation},
  author = {Davoudi Kakhki, Fatemeh and Freeman, Steven A. and Mosher, Gretchen A.},
  journal = {Safety},
  volume = {4},
  number = {4},
  pages = {57},
  year = {2018},
  month = {dec},
  publisher = {MDPI},
  doi = {10.3390/safety4040057}
}

@misc{herc_cost_data,
  title        = {Analyzing Cost Data},
  author       = {{Health Economics Resource Center}},
  howpublished = {U.S. Department of Veterans Affairs},
  year         = {04/24/2023},
  note         = {Disponível em: \url{https://herc.research.va.gov/include/page.asp?id=analytic-methods-analyzing-cost}. Acesso em: 21 dez. 2025}
}

@article{SHONO2008154,
title = {Application of the Tweedie distribution to zero-catch data in CPUE analysis},
journal = {Fisheries Research},
volume = {93},
number = {1},
pages = {154-162},
year = {2008},
issn = {0165-7836},
doi = {https://doi.org/10.1016/j.fishres.2008.03.006},
url = {https://www.sciencedirect.com/science/article/pii/S0165783608000945},
urldate = {12/21/2025},
author = {Hiroshi Shono},
keywords = {By-catch species, CPUE standardization, Generalized linear model, -fold cross-validation, Tweedie distribution model, Zero-catch problem},
}

@misc{gu2024dispersionmodelingzeroinflatedtweedie,
      title={Dispersion Modeling in Zero-inflated Tweedie Models with Applications to Insurance Claim Data Analysis}, 
      author={Yuwen Gu},
      year={2024},
      eprint={2405.14990},
      archivePrefix={arXiv},
      primaryClass={stat.ME},
      url={https://arxiv.org/abs/2405.14990}, 
      urldate={21/12/2025}
}

@article{kurz2017tweedie,
  title = {Tweedie distributions for fitting semicontinuous health care utilization cost data},
  author = {Kurz, Christoph F.},
  journal = {BMC Medical Research Methodology},
  volume = {17},
  number = {1},
  pages = {171},
  year = {2017},
  month = {dec},
  doi = {10.1186/s12874-017-0445-y},
  pmid = {29258428},
  pmcid = {PMC5735804},
  publisher = {BioMed Central},
  url={https://pmc.ncbi.nlm.nih.gov/articles/PMC5735804/?hl=pt-BR},
  urldate={12/21/2005}
}

@article{YU2006704,
title = {Support vector regression for real-time flood stage forecasting},
journal = {Journal of Hydrology},
volume = {328},
number = {3},
pages = {704-716},
year = {2006},
note = {The ICWRER - Symposium in Dresden, Germany},
issn = {0022-1694},
doi = {https://doi.org/10.1016/j.jhydrol.2006.01.021},
url = {https://www.sciencedirect.com/science/article/pii/S0022169406000473},
urldate = {12-21-2025},
author = {Pao-Shan Yu and Shien-Tsung Chen and I-Fan Chang},
keywords = {Flood forecasting, Water stage, Support vector regression, Parameter optimization},
}

@ARTICLE{ProteinQualityAssessment,
AUTHOR={Roy, Soumyadip  and Ben-Hur, Asa },    
TITLE={Protein quality assessment with a loss function designed for high-quality decoys}, 
JOURNAL={Frontiers in Bioinformatics},
VOLUME={Volume 3 - 2023},
YEAR={2023},
URL={https://www.frontiersin.org/journals/bioinformatics/articles/10.3389/fbinf.2023.1198218},
urldate={12-21-2025},
DOI={10.3389/fbinf.2023.1198218},
ISSN={2673-7647},
ABSTRACT={The prediction of protein 3D structure is essential for understanding protein function, drug discovery, and disease mechanisms; with the advent of methods like AlphaFold that are capable of producing very high quality decoys, ensuring the quality of those decoys can provide further confidence in the accuracy of their predictions.In this work we describe Q ϵ , a graph convolutional network that utilizes a minimal set of atom and residue features as input to predict the global distance test total score (GDTTS) and local distance difference test score (lDDT) of a decoy. To improve the model's performance, we introduce a novel loss function based on the ϵ-insensitive loss function used for SVM-regression. This loss function is specifically designed for the characteristics of the quality assessment problem, and provides predictions with improved accuracy over standard loss functions used for this task.Despite using only a minimal set of features, it matches the performance of recent state-of-the-art methods like DeepUMQA.}
}

@article{fu2011application,
  title = {Application of an integrated support vector regression method in prediction of financial returns},
  author = {Fu, Yuchen and Cheng, Yuanhu},
  journal = {International Journal of Information Engineering and Electronic Business},
  volume = {3},
  number = {3},
  pages = {37--43},
  year = {2011},
  month = {jun},
  publisher = {MECS Press},
  url = {http://www.mecs-press.org/},
  urldate = {12-21-2025},
}

-- Documentação da BCEWithLogits
@online{PyTorchBCEWithLogitsLoss,
  author = {{PyTorch}},
  title = {BCEWithLogitLoss},
  url = {https://docs.pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html},
  year = {2025},
  urldate = {2025-29-12}
}

@misc{cai2024sadaptergeneralizingvisiontransformer,
      title={S-Adapter: Generalizing Vision Transformer for Face Anti-Spoofing with Statistical Tokens}, 
      author={Rizhao Cai and Zitong Yu and Chenqi Kong and Haoliang Li and Changsheng Chen and Yongjian Hu and Alex Kot},
      year={2024},
      eprint={2309.04038},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2309.04038}, 
      urldate = {2025-31-12}
}

@misc{islam2024leveragingsentimentoffensivetext,
      title={Leveraging Sentiment for Offensive Text Classification}, 
      author={Khondoker Ittehadul Islam},
      year={2024},
      eprint={2412.17825},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2412.17825}, 
      urldate = {2025-31-12}
}

@misc{chervov2024protboostproteinfunctionprediction,
      title={ProtBoost: protein function prediction with Py-Boost and Graph Neural Networks -- CAFA5 top2 solution}, 
      author={Alexander Chervov and Anton Vakhrushev and Sergei Fironov and Loredana Martignetti},
      year={2024},
      eprint={2412.04529},
      archivePrefix={arXiv},
      primaryClass={q-bio.QM},
      url={https://arxiv.org/abs/2412.04529}, 
      urldate = {2025-31-12}
}

@misc{xie2015holisticallynestededgedetection,
      title={Holistically-Nested Edge Detection}, 
      author={Saining Xie and Zhuowen Tu},
      year={2015},
      eprint={1504.06375},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1504.06375}, 
      urldate = {2025-31-12},
}

@misc{li2025chex,
  title={CheX-DS: Improving Chest X-ray Image Classification with Ensemble Learning Based on DenseNet and Swin Transformer},
  author={Li, Xinran and Liu, Yu and Xu, Xiujuan and Zhao, Xiaowei},
  year={2025},
  eprint={2505.11168},
  archivePrefix={arXiv},
  primaryClass={cs.CV},
  url={https://arxiv.org/abs/2505.11168},
  urldate = {2025-31-12}
}

@misc{starodub2025surpassingstateartamd,
      title={Surpassing state of the art on AMD area estimation from RGB fundus images through careful selection of U-Net architectures and loss functions for class imbalance}, 
      author={Valentyna Starodub and Mantas Lukoševičius},
      year={2025},
      eprint={2510.26778},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2510.26778}, 
      urldate = {2025-31-12}
}

@misc{naveed2025adaptingsamcrossentropymasking,
      title={Adapting SAM via Cross-Entropy Masking for Class Imbalance in Remote Sensing Change Detection}, 
      author={Humza Naveed and Xina Zeng and Mitch Bryson and Nagita Mehrseresht},
      year={2025},
      eprint={2508.10568},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2508.10568}, 
      urldate = {2026-01-01}
}

@article{Xu_2015,
   title={Proximal gradient method for huberized support vector machine},
   volume={19},
   ISSN={1433-755X},
   url={http://dx.doi.org/10.1007/s10044-015-0485-z},
   DOI={10.1007/s10044-015-0485-z},
   number={4},
   journal={Pattern Analysis and Applications},
   publisher={Springer Science and Business Media LLC},
   author={Xu, Yangyang and Akrotirianakis, Ioannis and Chakraborty, Amit},
   year={2015},
   month=may, 
   pages={989–1005},
   urldate={2026-01-01}
}

@misc{luo2021learningsmoothhingelosses,
      title={Learning with Smooth Hinge Losses}, 
      author={JunRu Luo and Hong Qiao and Bo Zhang},
      year={2021},
      eprint={2103.00233},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2103.00233}, 
      urldate={2026-01-01}
}

@misc{masnadishirazi2015costsensitivesupportvectormachines,
      title={Cost-Sensitive Support Vector Machines}, 
      author={Hamed Masnadi-Shirazi and Nuno Vasconcelos and Arya Iranmehr},
      year={2015},
      eprint={1212.0975},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1212.0975}, 
      urldate={2026-01-01}
}

@misc{sun2024psvmsoftmarginsvmspnorm,
      title={$p$SVM: Soft-margin SVMs with $p$-norm Hinge Loss}, 
      author={Haoxiang Sun},
      year={2024},
      eprint={2408.09908},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2408.09908}, 
      urldate = {2026-01-02}
}

@misc{zeng2020fullycorrectivegradientboostingsquared,
      title={Fully-Corrective Gradient Boosting with Squared Hinge: Fast Learning Rates and Early Stopping}, 
      author={Jinshan Zeng and Min Zhang and Shao-Bo Lin},
      year={2020},
      eprint={2004.00179},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2004.00179}, 
      urldate={2026-01-02}
}

@misc{janocha2017lossfunctionsdeepneural,
      title={On Loss Functions for Deep Neural Networks in Classification}, 
      author={Katarzyna Janocha and Wojciech Marian Czarnecki},
      year={2017},
      eprint={1702.05659},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1702.05659}, 
      urldate={2026-01-02}
}

@article{mangasarian2001lagrangian,
  title={Lagrangian support vector machines},
  author={Mangasarian, Olvi L. and Musicant, David R.},
  journal={Journal of Machine Learning Research},
  volume={1},
  number={Sep},
  pages={161--177},
  year={2001},
  publisher={JMLR. org},
  doi={10.1162/15324430152748218},
  url={https://dl.acm.org/doi/10.1162/15324430152748218},
  urldate={2026-01-03}
}

@misc{heker2020jointliverlesionsegmentation,
      title={Joint Liver Lesion Segmentation and Classification via Transfer Learning}, 
      author={Michal Heker and Hayit Greenspan},
      year={2020},
      eprint={2004.12352},
      archivePrefix={arXiv},
      primaryClass={eess.IV},
      url={https://arxiv.org/abs/2004.12352}, 
      urldate={2026-01-04}
}

@misc{becirovic2025performancecomparisonmedicalimage,
      title={Performance comparison of medical image classification systems using TensorFlow Keras, PyTorch, and JAX}, 
      author={Merjem Bećirović and Amina Kurtović and Nordin Smajlović and Medina Kapo and Amila Akagić},
      year={2025},
      eprint={2507.14587},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2507.14587}, 
      urldate={2026-01-04}
}

@misc{oni2025optimizedcustomcnnrealtime,
      title={Optimized Custom CNN for Real-Time Tomato Leaf Disease Detection}, 
      author={Mangsura Kabir Oni and Tabia Tanzin Prama},
      year={2025},
      eprint={2502.18521},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2502.18521}, 
      urldate={2026-01-04}
}

@inproceedings{Camargo_2021, 
    series={BRESCI 2021},
   title={Text-to-hashtag Generation using Seq2Seq Learning},
   url={http://dx.doi.org/10.5753/bresci.2021.15797},
   DOI={10.5753/bresci.2021.15797},
   booktitle={Anais do XV Brazilian e-Science Workshop (BRESCI 2021)},
   publisher={Sociedade Brasileira de Computação},
   author={Camargo, Augusto and Carvalho, Wesley and Peressim, Felipe and Barzilay, Alan and Finger, Marcelo},
   year={2021},
   month=jul, pages={121–128},
   collection={BRESCI 2021},
   urldate={2026-01-04},
}

@misc{xia2025improvedpurefullyconnected,
      title={An Improved Pure Fully Connected Neural Network for Rice Grain Classification}, 
      author={Wanke Xia and Ruoxin Peng and Haoqi Chu and Xinlei Zhu and Zhiyu Yang and Lili Yang and Bo Lv and Xunwen Xiang},
      year={2025},
      eprint={2503.03111},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2503.03111}, 
      urldate={2026-01-04}
}

@misc{naeem2025efficientiotintrusiondetection,
      title={Efficient IoT Intrusion Detection with an Improved Attention-Based CNN-BiLSTM Architecture}, 
      author={Amna Naeem and Muazzam A. Khan and Nada Alasbali and Jawad Ahmad and Aizaz Ahmad Khattak and Muhammad Shahbaz Khan},
      year={2025},
      eprint={2503.19339},
      archivePrefix={arXiv},
      primaryClass={cs.CR},
      url={https://arxiv.org/abs/2503.19339}, 
      urldate={2026-01-04}
}

@misc{hosseini2024dilatedbalancedcrossentropy,
      title={Dilated Balanced Cross Entropy Loss for Medical Image Segmentation}, 
      author={Seyed Mohsen Hosseini and Mahdieh Soleymani Baghshah},
      year={2024},
      eprint={2412.06045},
      archivePrefix={arXiv},
      primaryClass={eess.IV},
      url={https://arxiv.org/abs/2412.06045}, 
      urldate={2026-01-04}
}

@misc{phan2020resolvingclassimbalanceobject,
      title={Resolving Class Imbalance in Object Detection with Weighted Cross Entropy Losses}, 
      author={Trong Huy Phan and Kazuma Yamamoto},
      year={2020},
      eprint={2006.01413},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2006.01413}, 
      urldate={2026-01-04}
}

@misc{farabi2025insideoutefficientnetv2sbaseddeep,
      title={InsideOut: An EfficientNetV2-S Based Deep Learning Framework for Robust Multi-Class Facial Emotion Recognition}, 
      author={Ahsan Farabi and Israt Khandaker and Ibrahim Khalil Shanto and Md Abdul Ahad Minhaz and Tanisha Zaman},
      year={2025},
      eprint={2510.03066},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2510.03066}, 
      urldate={2026-01-04}
}

@misc{zhu2023multilabelselfsupervisedlearningscene,
      title={Multi-Label Self-Supervised Learning with Scene Images}, 
      author={Ke Zhu and Minghao Fu and Jianxin Wu},
      year={2023},
      eprint={2308.03286},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2308.03286}, 
      urldate={2026-01-07}
}

@misc{asadi2025clinicallyinspiredhierarchicalmultilabelclassification,
      title={Clinically-Inspired Hierarchical Multi-Label Classification of Chest X-rays with a Penalty-Based Loss Function}, 
      author={Mehrdad Asadi and Komi Sodoké and Ian J. Gerard and Marta Kersten-Oertel},
      year={2025},
      eprint={2502.03591},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2502.03591}, 
      urldate={2026-01-07}
}

@misc{hou2018audiotagging,
      title={Audio Tagging With Connectionist Temporal Classification Model Using Sequential Labelled Data}, 
      author={Yuanbo Hou and Qiuqiang Kong and Shengchen Li},
      year={2018},
      eprint={1808.01935},
      archivePrefix={arXiv},
      primaryClass={cs.SD},
      url={https://arxiv.org/abs/1808.01935}, 
      urldate={2026-01-07}
}

