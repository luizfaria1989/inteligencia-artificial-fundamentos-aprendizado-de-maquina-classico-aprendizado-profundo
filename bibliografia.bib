@book{DeepLearningBook,
  author    = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  title     = {Deep Learning},
  publisher = {MIT Press},
  year      = {2016},
}

@book{MaosAObra,
  author = {Aurélien Géron},
  title = {Mãos à Obra: Aprendizado de Máquina com Scikit-Learn e TensorFlow},
  publisher = {Alta Books},
  year = {2019},
}

-- Método do gradiente original por Cauchy
@article{CauchyMetodoDoGradiente,
  title   = {M{\'e}thode g{\'e}n{\'e}rale pour la r{\'e}solution des syst{\`e}mes d'{\'e}quations simultan{\'e}es},
  author  = {Cauchy, Augustin-Louis},
  journal = {Comptes Rendus Hebdomadaires des S{\'e}ances de l'Acad{\'e}mie des Sciences},
  volume  = {25},
  pages   = {536--538},
  year    = {1847}
}

-- Artigo que introduz a retropropagação
@article{BackpropagationArticle,
  author  = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
  title   = {Learning Representations by Back-Propagating Errors},
  journal = {Nature},
  year    = {1986},
  volume  = {323},
  pages   = {533--536},
}

-- Artigo que introduz os primeiros teoremas da aproximação universal
@article{Cybenko1989,
  author  = {Cybenko, George},
  title   = {Approximation by Superpositions of a Sigmoidal Function},
  journal = {Mathematics of Control, Signals, and Systems},
  year    = {1989},
  volume  = {2},
  number  = {4},
  pages   = {303--314},
}

-- Artigo que introduz os teoremas da aproximação universal para funções como a relu
@article{Leshno1993,
      title = {Multilayer feedforward networks with a nonpolynomial activation function can approximate any function},
      journal = {Neural Networks},
      volume = {6},
      number = {6},
      pages = {861-867},
      year = {1993},
      issn = {0893-6080},
      doi = {https://doi.org/10.1016/S0893-6080(05)80131-5},
      url = {https://www.sciencedirect.com/science/article/pii/S0893608005801315},
      author = {Moshe Leshno and Vladimir Ya. Lin and Allan Pinkus and Shimon Schocken},
      keywords = {Multilayer feedforward networks, Activation functions, Role of threshold, Universal approximation capabilities, (μ) approximation},
      abstract = {Several researchers characterized the activation function under which multilayer feedforward networks can act as universal approximators. We show that most of all the characterizations that were reported thus far in the literature are special cases of the following general result: A standard multilayer feedforward network with a locally bounded piecewise continuous activation function can approximate any continuous function to any degree of accuracy if and only if the network's activation function is not a polynomial. We also emphasize the important role of the threshold, asserting that without it the last theorem does not hold.}
}
-- Artigo que introduz o método do gradiente com momentum
@article{polyak1964,
  title={Some methods of speeding up the convergence of iteration methods},
  author={Polyak, Boris T.},
  journal={USSR Computational Mathematics and Mathematical Physics},
  volume={4},
  number={5},
  pages={1--17},
  year={1964},
  publisher={Elsevier}
}

-- Artigo que introduz o método de otimização Adam
@misc{AdamMethod,
      title={Adam: A Method for Stochastic Optimization}, 
      author={Diederik P. Kingma and Jimmy Ba},
      year={2017},
      eprint={1412.6980},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1412.6980}, 
      urldate={2025-09-15},
}

-- Artigo que introduz o método de otimização AdaGrad
@article{AdaGradMethod,
  author  = {John Duchi and Elad Hazan and Yoram Singer},
  title   = {Adaptive Subgradient Methods for Online Learning and Stochastic Optimization},
  journal = {Journal of Machine Learning Research},
  year    = {2011},
  volume  = {12},
  number  = {61},
  pages   = {2121--2159},
  url     = {http://jmlr.org/papers/v12/duchi11a.html},
  urldate={2025-09-15},
}

-- Artigo que introduz o método de otimização Nadam
@inproceedings{NadamMethod,
  title={Incorporating Nesterov Momentum into Adam},
  author={Dozat, Timothy},
  booktitle={ICLR 2016 Workshop Track},
  year={2016},
  url={https://openreview.net/forum?id=OM0DEvNMIxAaI-fG_O1-I},
  urldate={2025-09-15},
}

-- Artigo que introduz o método de otimização AdamW
@misc{AdamWMethod,
      title={Decoupled Weight Decay Regularization}, 
      author={Ilya Loshchilov and Frank Hutter},
      year={2019},
      eprint={1711.05101},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1711.05101}, 
      urldate={2025-09-15},
}

-- Artigo que introduz o método de otimização RAdam
@misc{RAdamMethod,
      title={On the Variance of the Adaptive Learning Rate and Beyond}, 
      author={Liyuan Liu and Haoming Jiang and Pengcheng He and Weizhu Chen and Xiaodong Liu and Jianfeng Gao and Jiawei Han},
      year={2021},
      eprint={1908.03265},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1908.03265}, 
      urldate={2025-09-15},
}

-- Artigo que introduz o método do gradiente estocástico
@article{StochasticGradientDescentMethod,
  author    = {Herbert Robbins and Sutton Monro},
  title     = {A Stochastic Approximation Method},
  journal   = {The Annals of Mathematical Statistics},
  year      = {1951},
  volume    = {22},
  number    = {3},
  pages     = {400--407},
  doi       = {10.1214/aoms/1177729586}
}

-- Adeline 
-- SGD aparece pela primeira vez em aprendizado de máquina
@inproceedings{Adeline,
  author    = {Bernard Widrow and Marcian E. Hoff},
  title     = {Adaptive Switching Circuits},
  booktitle = {1960 IRE WESCON Convention Record},
  year      = {1960},
  part      = {4},
  pages     = {96--104},
  publisher = {IRE}
}

-- Artigo que introduz o método de otimização do gradiente acelerado de nesterov (NAG)
@article{NAGMethod,
  title={A method for solving the convex programming problem with convergence rate $O(1/k^2)$},
  author={Yurii Nesterov},
  journal={Proceedings of the USSR Academy of Sciences},
  year={1983},
  volume={269},
  pages={543-547},
  url={https://api.semanticscholar.org/CorpusID:145918791}
}

-- Slides que introduziram o RMSProp
@misc{RMSPropMethod,
  author = {Tieleman, Tijmen and Hinton, Geoffrey},
  title  = {Lecture 6.5---RMSProp: Divide the gradient by a running average of its recent magnitude},
  year   = {2012},
  howpublished = {COURSERA: Neural Networks for Machine Learning},
  url    = {https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf},
  urldate={2025-10-03},
}

-- Um dos artigos que introduziu a sigmoide logística
@article{SigmoidVerhulst1845,
  author = {Verhulst, P. F.},
  title = {Recherches math{\'e}matiques sur la loi d'accroissement de la population},
  journal = {Nouveaux M{\'e}moires de l'Acad{\'e}mie Royale des Sciences et Belles-Lettres de Bruxelles},
  volume = {18},
  pages = {1--42},
  year = {1845},
  publisher = {L'Acad{\'e}mie Royale}
}

-- Um dos artigos que estuda o comportamento de neurônios e os compara com a sigmoide
@article{SigmoidWilsonCowan,
  author = {Wilson, Hugh R. and Cowan, Jack D.},
  title = {Excitatory and inhibitory interactions in localized populations of model neurons},
  journal = {Biophysical Journal},
  volume = {12},
  number = {1},
  pages = {1--24},
  year = {1972},
  publisher = {Biophysical Society},
  doi = {10.1016/S0006-3495(72)86068-5}
}

-- Artigo que introduz as redes de elman, utiliza a sigmoide como funcao de ativacao
@article{ElmanNetwork,
  author = {Elman, Jeffrey L.},
  title = {Finding structure in time},
  journal = {Cognitive Science},
  volume = {14},
  number = {2},
  pages = {179--211},
  year = {1990},
  doi = {10.1207/s15516709cog1402_1},
  publisher = {Wiley Online Library}
}

-- Artigo que introduz o perceptron
@article{PerceptronRosenblatt,
  author = {Rosenblatt, F.},
  title = {The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain},
  journal = {Psychological Review},
  volume = {65},
  number = {6},
  pages = {386--408},
  year = {1958},
  doi = {10.1037/h0042519}
}

-- Artigo que explica algumas funcoes de ativacao e suas propriedades
@misc{ActivationFunctionsLederer,
      title={Activation Functions in Artificial Neural Networks: A Systematic Overview}, 
      author={Johannes Lederer},
      year={2021},
      eprint={2101.09957},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2101.09957}, 
}

-- Artigo que introduziu a tangente hiperbólica
@incollection{TanhLambert,
      author    = {Lambert, J. H.},
      title     = {M{\'e}moire sur quelques propri{\'e}t{\'e}s remarquables des quantit{\'e}s transcendantes circulaires et logarithmiques},
      booktitle = {Pi: A Source Book},
      editor    = {Berggren, Lennart and Borwein, Jonathan M. and Borwein, Peter B.},
      publisher = {Springer-Verlag},
      address   = {New York},
      year      = {2004},
      pages     = {129--140},
      doi       = {10.1007/978-1-4757-4217-6_18},
      note      = {Original work published in 1768}
}

-- Artigo que introduziu a Le-Net-5
@article{LecunLeNet1998,
    author    = {LeCun, Yann and Bottou, L{\'e}on and Bengio, Yoshua and Haffner, Patrick},
    title     = {Gradient-based learning applied to document recognition},
    journal   = {Proceedings of the IEEE},
    volume    = {86},
    number    = {11},
    pages     = {2278--2324},
    year      = {1998},
    doi       = {10.1109/5.726791}
}

-- Artigo que introduziu a função de ativação sigmoidal softsign
@article{Softsign1998,
    author = {Foundation, The and Elliott, David},
    year = {1998},
    month = {12},
    pages = {},
    title = {A better Activation Function for Artificial Neural Networks}
}

-- Artigo que introduziu a função de ativação Leaky ReLU (LReLU)
@article{LeakyReLUArticle,
  author = {Andrew L. Mass, Awni Y. Hannum and Andrew Y. Ng},
  journal = {},
  number = {},
  title = {Rectifier Nonlinearities Improve Neural Networks Acustic Models},
  volume = {},
  year = {2013}
}

-- Artigo que introduziu a função de ativação Parametric ReLU (PReLU)
@misc{PReLUArticle,
      title={Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification}, 
      author={Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
      year={2015},
      eprint={1502.01852},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1502.01852}, 
}

-- Artigo que introduziu a função de ativação Exponential Linear Unit (ELU)
@misc{ELUArticle,
      title={Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)}, 
      author={Djork-Arné Clevert and Thomas Unterthiner and Sepp Hochreiter},
      year={2016},
      eprint={1511.07289},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1511.07289}, 
}

-- Artigo que introduziu a função de ativação SELU
@misc{SELUArticle,
      title={Self-Normalizing Neural Networks}, 
      author={Günter Klambauer and Thomas Unterthiner and Andreas Mayr and Sepp Hochreiter},
      year={2017},
      eprint={1706.02515},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1706.02515}, 
}

-- Artigo que introduziu a função de ativação GELU
@misc{GELUArticle,
      title={Gaussian Error Linear Units (GELUs)}, 
      author={Dan Hendrycks and Kevin Gimpel},
      year={2023},
      eprint={1606.08415},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1606.08415}, 
}

-- Artigo que introduziu o dataset CIFAR
@inproceedings{CIFAR10,
  title={Learning Multiple Layers of Features from Tiny Images},
  author={Alex Krizhevsky},
  year={2009},
  url={https://api.semanticscholar.org/CorpusID:18268744}
}

-- Um dos artigos que citam a função de ativação Noisy ReLU (NReLU)
@inproceedings{Nair2010,
  author    = {Nair, Vinod and Hinton, Geoffrey E.},
  title     = {Rectified Linear Units Improve Restricted Boltzmann Machines},
  booktitle = {Proceedings of the 27th International Conference on Machine Learning (ICML-10)},
  year      = {2010},
  pages     = {807--814},
}

-- Um dos artigos que estuda o comportamento das células nervosas
-- Utiliza uma versão da relu para explicar o comportamento
@article{Householder1941,
  author  = {Householder, Alston S.},
  title   = {A Theory of Steady-State Activity in Nerve-Fiber Networks: I. Definitions and Preliminary Theorems},
  journal = {The Bulletin of Mathematical Biophysics},
  year    = {1941},
  volume  = {3},
  number  = {2},
  pages   = {63--69},
}

-- Artigo que introduz a rede neural convolucional AlexNet
@inproceedings{AlexNet,
      author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
      booktitle = {Advances in Neural Information Processing Systems},
      editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},
      pages = {},
      publisher = {Curran Associates, Inc.},
      title = {ImageNet Classification with Deep Convolutional Neural Networks},
      url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf},
      volume = {25},
      year = {2012}
}

-- Um dos artigos que cita a RReLU
@article{XuRReLU,
  author  = {Xu, Bing and Wang, Naiyan and Chen, Tianqi and Li, Mu},
  title   = {Empirical Evaluation of Rectified Activations in Convolutional Network},
  journal = {arXiv preprint arXiv:1505.00853},
  year    = {2015},
}

-- Um dos artigos que cita propriedades da ReLU
@article{Glorot,
  author = {Xavier Glorot, Antaine Border and Yoshua Bengio},
  journal = {},
  number = {},
  title = {Deep Sparse Rectifier Neural Networks},
  volume = {},
  year = {2011}
}

-- Um dos artigos que cita o problema dos neurônios agonizantes
@misc{DyingReluDouglas,
      title={Why ReLU Units Sometimes Die: Analysis of Single-Unit Error Backpropagation in Neural Networks}, 
      author={Scott C. Douglas and Jiutian Yu},
      year={2018},
      eprint={1812.05981},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1812.05981}, 
}

-- Um dos artigos que cita o problema do gradiente explosivo
@misc{ExplodingGradient,
      title={The exploding gradient problem demystified - definition, prevalence, impact, origin, tradeoffs, and solutions}, 
      author={George Philipp and Dawn Song and Jaime G. Carbonell},
      year={2018},
      eprint={1712.05577},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1712.05577}, 
}

-- Documentação da hard sigmoid do PyTorch
@online{PyTorchHardSigmoid,
  author = {{PyTorch}},
  title = {Hardsigmoid},
  url = {https://pytorch.org/docs/stable/generated/torch.nn.Hardsigmoid.html},
  year = {2025},
  urldate = {2025-10-10}
}

-- Documentação da hard tanh do PyTorch
@online{PyTorchHardTanh,
  author = {{PyTorch}},
  title = {Hardtanh},
  url = {https://pytorch.org/docs/stable/generated/torch.nn.Hardtanh.html},
  year = {2025},
  urldate = {2025-10-10}
}

-- Artigo que discute um pouco das propriedades das funções de ativação
@misc{PropriedadesFuncoesDeAtivacao,
      title={A Survey on Activation Functions and their relation with Xavier and He Normal Initialization}, 
      author={Leonid Datta},
      year={2020},
      eprint={2004.06632},
      archivePrefix={arXiv},
      primaryClass={cs.NE},
      url={https://arxiv.org/abs/2004.06632}, 
}

-- Documentação da leaky relu do PyTorch
@online{PyTorchLeakyReLU,
  author = {{PyTorch}},
  title = {LeakyReLU},
  url = {https://docs.pytorch.org/docs/stable/generated/torch.nn.LeakyReLU.html},
  year = {2025},
  urldate = {2025-10-11}
}

-- Livro que cria a regressão dos mínimos quadrados e a mse
@book{Legendre1805,
  author    = {Legendre, Adrien-Marie},
  title     = {Nouvelles m{\'e}thodes pour la d{\'e}termination des orbites des com{\`e}tes},
  publisher = {Firmin Didot},
  year      = {1805},
  address   = {Paris},
  note      = {Cont{\'e}m o ap{\^e}ndice "Sur la M{\'e}thode des Moindres Quarr{\'e}s", que apresenta a primeira publica{\c c}{\~a}o do m{\'e}todo dos m{\'i}nimos quadrados.}
}

-- Livro do Gauss que cita a regressao dos minimos quadrados e a mse
@book{Gauss1809,
  author    = {Gauss, Carl Friedrich},
  title     = {{Theoria motus corporum coelestium in sectionibus conicis solem ambientium}},
  publisher = {sumtibus F. Perthes et I. H. Besser},
  year      = {1809},
  address   = {Hamburgi}
}

-- Artigo das funções de perda
@article{LossesArticle,
   title={A comprehensive survey of loss functions and metrics in deep learning},
   volume={58},
   ISSN={1573-7462},
   url={http://dx.doi.org/10.1007/s10462-025-11198-7},
   DOI={10.1007/s10462-025-11198-7},
   number={7},
   journal={Artificial Intelligence Review},
   publisher={Springer Science and Business Media LLC},
   author={Terven, Juan and Cordova-Esparza, Diana-Margarita and Romero-González, Julio-Alejandro and Ramírez-Pedraza, Alfonso and Chávez-Urbiola, E. A.},
   year={2025},
   urldate = {2025-10-17},
   month=apr 
}

-- Um dos artigos que faz uso do erro absoluto médio
@article{GreedyFunctionApproximation,
    author = {Jerome H. Friedman},
    title = {{Greedy function approximation: A gradient boosting machine.}},
    volume = {29},
    journal = {The Annals of Statistics},
    number = {5},
    publisher = {Institute of Mathematical Statistics},
    pages = {1189 -- 1232},
    keywords = {boosting, decision trees, Function estimation, robust nonparametric regression},
    year = {2001},
    doi = {10.1214/aos/1013203451},
    URL = {https://doi.org/10.1214/aos/1013203451},
    urldate = {2025-10-17}
}

-- Um dos artigos que faz uso do erro absoluto médio
@misc{ImageToImage,
      title={Image-to-Image Translation with Conditional Adversarial Networks}, 
      author={Phillip Isola and Jun-Yan Zhu and Tinghui Zhou and Alexei A. Efros},
      year={2018},
      eprint={1611.07004},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1611.07004}, 
      urldate={2025-10-17}
}

-- Artigo que introduz a huber loss
@article{HuberLoss,
  author = {Peter J. Huber},
  title = {{Robust Estimation of a Location Parameter}},
  volume = {35},
  journal = {The Annals of Mathematical Statistics},
  number = {1},
  publisher = {Institute of Mathematical Statistics},
  pages = {73 -- 101},
  year = {1964},
  doi = {10.1214/aoms/1177703732},
  URL = {https://doi.org/10.1214/aoms/1177703732},
  urldate={2025-10-17}
}

-- Definição de Entropia por Shannon
@ARTICLE{EntropyShannon,
  author={Shannon, C. E.},
  journal={The Bell System Technical Journal}, 
  title={A mathematical theory of communication}, 
  year={1948},
  volume={27},
  number={3},
  pages={379-423},
  keywords={},
  doi={10.1002/j.1538-7305.1948.tb01338.x}
}

-- Criação da Divergência de Kullback-leibler
@article{KullbackLeiblerDivergence,
  ISSN = {00034851},
  URL = {http://www.jstor.org/stable/2236703},
  author = {S. Kullback and R. A. Leibler},
  journal = {The Annals of Mathematical Statistics},
  number = {1},
  pages = {79--86},
  publisher = {Institute of Mathematical Statistics},
  title = {On Information and Sufficiency},
  urldate = {2025-10-18},
  volume = {22},
  year = {1951},
}

@article{HintonConnectionist,
  title = {Connectionist learning procedures},
  journal = {Artificial Intelligence},
  volume = {40},
  number = {1},
  pages = {185-234},
  year = {1989},
  issn = {0004-3702},
  doi = {https://doi.org/10.1016/0004-3702(89)90049-0},
  url = {https://www.sciencedirect.com/science/article/pii/0004370289900490},
  author = {Geoffrey E. Hinton},
  abstract = {A major goal of research on networks of neuron-like processing units is to discover efficient learning procedures that allow these networks to construct complex internal representations of their environment. The learning procedures must be capable of modifying the connection strengths in such a way that internal units which are not part of the input or output come to represent important features of the task domain. Several interesting gradient-descent procedures have recently been discovered. Each connection computes the derivative, with respect to the connection strength, of a global measure of the error in the performance of the network. The strength is then adjusted in the direction that decreases the error. These relatively simple, gradient-descent learning procedures work well for small tasks and the new challenge is to find ways of improving their convergence rate and their generalization abilities so that they can be applied to larger, more realistic tasks.},
  urldate={2025-10-18}
}

-- Artigo que introduz a Hinge Loss
@article{HingeLoss,
	abstract = {Thesupport-vector network is a new learning machine for two-group classification problems. The machine conceptually implements the following idea: input vectors are non-linearly mapped to a very high-dimension feature space. In this feature space a linear decision surface is constructed. Special properties of the decision surface ensures high generalization ability of the learning machine. The idea behind the support-vector network was previously implemented for the restricted case where the training data can be separated without errors. We here extend this result to non-separable training data.},
	author = {Cortes, Corinna and Vapnik, Vladimir},
	date = {1995/09/01},
	date-added = {2025-10-19 14:29:02 -0300},
	date-modified = {2025-10-19 14:29:02 -0300},
	doi = {10.1007/BF00994018},
	id = {Cortes1995},
	isbn = {1573-0565},
	journal = {Machine Learning},
	number = {3},
	pages = {273--297},
	title = {Support-vector networks},
	url = {https://doi.org/10.1007/BF00994018},
	volume = {20},
	year = {1995},
	bdsk-url-1 = {https://doi.org/10.1007/BF00994018}
  
}

-- Artigo que explica as propriedades estatísticas da função de perda log-cosh
@misc{StatisticalPropetiesLogCosh,
      title={Statistical Properties of the log-cosh Loss Function Used in Machine Learning}, 
      author={Resve A. Saleh and A. K. Md. Ehsanes Saleh},
      year={2024},
      eprint={2208.04564},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/2208.04564}, 
      urldate = {2025-10-21},
}

-- Artigo que usa regressão linear para prever os custos médicos
-- Usam o MAE e o MSE e o Rˆ2 como métricas de avaliação
@article{MedicalCostsEstimationUsingLR,
  author  = {Dwikasaria, Ni Made Dita and Sutramiani, Ni Putu and Putri, Komang Sri Yanisa and Kusuma, Nyoman Tri Rahaditya and Pramana, Made Dimas Aldi Dwi and Darma, I Wayan Agus Surya},
  title   = {Medical Costs Estimation Using Linear Regression Method},
  journal = {JURNAL ILMIAH MERPATI},
  year    = {2023},
  volume  = {11},
  number  = {3},
  pages   = {171--179},
  month   = {December},
  issn    = {2252-3006},
  eissn   = {2685-2411},
  keywords= {Medical Costs, Data Mining, Estimation, Linear Regression},
  url={https://www.google.com/url?sa=t&source=web&rct=j&opi=89978449&url=https://scispace.com/pdf/medical-costs-estimation-using-linear-regression-method-38k6wo4bri.pdf&ved=2ahUKEwiT5fDb8r-QAxVhK7kGHQArCKkQFnoECBgQAQ&usg=AOvVaw22f-dAUr1phgkxjmJ7Nh7a},
  urldate={2025-10-25},
}

-- Artigo que usa técnicas de regressão para prever preços de casas
-- Usam o MAE e o MSE como métricas de avaliação
@article{OptimalHousePricePrediction,
   title={An Optimal House Price Prediction Algorithm: XGBoost},
   volume={3},
   ISSN={2813-2203},
   url={http://dx.doi.org/10.3390/analytics3010003},
   DOI={10.3390/analytics3010003},
   number={1},
   journal={Analytics},
   publisher={MDPI AG},
   author={Sharma, Hemlata and Harsora, Hitesh and Ogunleye, Bayode},
   year={2024},
   month=jan, 
   pages={30–45},
   urldate={2025-10-25}
}

-- Artigo que usam técnicas de regressão e combinam com para prever a produção de milho no cinturão do milho nos eua
-- Usam o RMSE como métrica de avaliacao
@article{CouplingMachineLearningAndCropModeling,
   title={Coupling machine learning and crop modeling improves crop yield prediction in the US Corn Belt},
   volume={11},
   ISSN={2045-2322},
   url={http://dx.doi.org/10.1038/s41598-020-80820-1},
   DOI={10.1038/s41598-020-80820-1},
   number={1},
   journal={Scientific Reports},
   publisher={Springer Science and Business Media LLC},
   author={Shahhosseini, Mohsen and Hu, Guiping and Huber, Isaiah and Archontoulis, Sotirios V.},
   year={2021},
   month=jan,
   urldate={2025-10-25}
}

-- Artigo que usa técnicas de regressão para prever demanda de energia em microgrids
-- Usa o MSE indiretamente, transformando-o em EW-MSE, uma funcao de perda especial para resolver o problema do artigo
@misc{OptimizingFL,
      title={Optimizing Federated Learning for Scalable Power-demand Forecasting in Microgrids}, 
      author={Roopkatha Banerjee and Sampath Koti and Gyanendra Singh and Anirban Chakraborty and Gurunath Gurrala and Bhushan Jagyasi and Yogesh Simmhan},
      year={2025},
      eprint={2508.08022},
      archivePrefix={arXiv},
      primaryClass={cs.DC},
      url={https://arxiv.org/abs/2508.08022}, 
      urldate={2025-10-25},
}

--
--
@misc{nedungadi2025aircastimprovingairpollution,
      title={AirCast: Improving Air Pollution Forecasting Through Multi-Variable Data Alignment}, 
      author={Vishal Nedungadi and Muhammad Akhtar Munir and Marc Rußwurm and Ron Sarafian and Ioannis N. Athanasiadis and Yinon Rudich and Fahad Shahbaz Khan and Salman Khan},
      year={2025},
      eprint={2502.17919},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2502.17919}, 
      urldate={2025-10-27},
}

--
--
@article{MinimumOpenDataSubsetForWindPowerPrediction,
  author = {Zuben, Elizabeth and Schell, Kristen},
  year = {2025},
  month = {03},
  pages = {},
  title = {Minimum Open Data Subset for Wind Power Prediction},
  doi = {10.5194/wes-2025-29},
}

--
--
@misc{Noise2Noise,
      title={Noise2Noise: Learning Image Restoration without Clean Data}, 
      author={Jaakko Lehtinen and Jacob Munkberg and Jon Hasselgren and Samuli Laine and Tero Karras and Miika Aittala and Timo Aila},
      year={2018},
      eprint={1803.04189},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1803.04189}, 
      urldate={2025-10-27},
}

--
--
@misc{chao2022regressionmetriclosslearning,
      title={Regression Metric Loss: Learning a Semantic Representation Space for Medical Images}, 
      author={Hanqing Chao and Jiajin Zhang and Pingkun Yan},
      year={2022},
      eprint={2207.05231},
      archivePrefix={arXiv},
      primaryClass={eess.IV},
      url={https://arxiv.org/abs/2207.05231}, 
      urldate={2025-10-27},
}

-- Utilizam a perda de Huber para criar um algoritmo de emsemble para prever dados médicos
@misc{HuberLossSuperLearner,
      title={A Huber loss-based super learner with applications to healthcare expenditures}, 
      author={Ziyue Wu and David Benkeser},
      year={2022},
      eprint={2205.06870},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/2205.06870}, 
      urldate={2025-10-30}
}

--
--
@misc{RobustTrendHuberLoss,
      title={RobustTrend: A Huber Loss with a Combined First and Second Order Difference Regularization for Time Series Trend Filtering}, 
      author={Qingsong Wen and Jingkun Gao and Xiaomin Song and Liang Sun and Jian Tan},
      year={2019},
      eprint={1906.03751},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1906.03751}, 
      urldate={2025-10-30}
}

--
--
@misc{AdaptiveHuberRegression,
      title={Adaptive Huber Regression}, 
      author={Qiang Sun and Wenxin Zhou and Jianqing Fan},
      year={2018},
      eprint={1706.06991},
      archivePrefix={arXiv},
      primaryClass={math.ST},
      url={https://arxiv.org/abs/1706.06991}, 
      urldate={2025-10-30}
}

@INPROCEEDINGS{RobustAleatoricModelingVehicleLocalization,
      author={Hudnell, Max and Price, True and Frahm, Jan-Michael},
      booktitle={2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)}, 
      title={Robust Aleatoric Modeling for Future Vehicle Localization}, 
      year={2019},
      volume={},
      number={},
      pages={2944-2951},
      keywords={Uncertainty;Predictive models;Two dimensional displays;Task analysis;Robustness;Trajectory;Recurrent neural networks},
      doi={10.1109/CVPRW.2019.00355}
}

--
--
@inproceedings{FinantialMarketForecastingUsingRNN,
author = {Kehinde, Temitope and Khan, Waqar Ahmed and Chung, Sai-Ho},
year = {2023},
month = {10},
pages = {},
title = {Financial Market Forecasting using RNN, LSTM, BiLSTM, GRU and Transformer-Based Deep Learning Algorithms},
doi = {10.46254/EV01.20230037}
}

--
--
@article{SiameseRecurrentNeuralNetwork,
  author = {Fernández-Llaneza, Daniel and Ulander, Silas and Gogishvili, Dea and Nittinger, Eva and Zhao, Hongtao and Tyrchan, Christian},
  title = {Siamese Recurrent Neural Network with a Self-Attention Mechanism for Bioactivity Prediction},
  journal = {ACS Omega},
  volume = {6},
  number = {16},
  pages = {11086-11094},
  year = {2021},
  doi = {10.1021/acsomega.1c01266},
  note ={PMID: 34056263},
  URL = {https://doi.org/10.1021/acsomega.1c01266},
  eprint = {https://doi.org/10.1021/acsomega.1c01266},
  urldate = {2025-10-31},
}

@Article{AnEffectiveMethodForDetectingUnknowTypes,
  AUTHOR = {Yu, Li and Xu, Liuquan and Jiang, Xuefeng},
  TITLE = {An Effective Method for Detecting Unknown Types of Attacks Based on Log-Cosh Variational Autoencoder},
  JOURNAL = {Applied Sciences},
  VOLUME = {13},
  YEAR = {2023},
  NUMBER = {22},
  ARTICLE-NUMBER = {12492},
  URL = {https://www.mdpi.com/2076-3417/13/22/12492},
  ISSN = {2076-3417},
  urldate={2025-10-31},
}

@misc{SunKim-NetOverYourHead,
  author = {Sun, Hongtao and Kim, Ji Hoon "Andy"},
  title  = {A Net Over Your Head: A Neural Network Approach to Home Price Predictions},
  url = {https://cs229.stanford.edu/proj2017/final-reports/5191322.pdf},
  urldate = {2025-11-04}
}

@inproceedings{Winter2023PredictingLOS,
  author    = {Winter, Alexander and Hartwig, Mattis and Kirsten, Toralf},
  title     = {Predicting Hospital Length of Stay of Patients Leaving the Emergency Department},
  booktitle = {Proceedings of the 16th International Joint Conference on Biomedical Engineering Systems and Technologies (BIOSTEC 2023) - Volume 5: HEALTHINF},
  year      = {2023},
  pages     = {124--131},
  isbn      = {978-989-758-631-6},
  issn      = {2184-4305},
  doi       = {10.5220/0011671700003414},
  url = {https://www.scitepress.org/Papers/2023/116717/116717.pdf},
  urldate = {2025-11-04},
}

@misc{Jadon2022ComprehensiveSurvey,
  author = {Jadon, Aryan and Patil, Avinash and Jadon, Shruti},
  title  = {A Comprehensive Survey of Regression Based Loss Functions for Time Series Forecasting},
  year   = {2022},
  eprint = {2211.02989},
  archivePrefix = {arXiv},
  primaryClass = {cs.LG},
  url = {https://arxiv.org/pdf/2211.02989},
  urldate = {2025-11-04}
}

-- Artigo que introduz a regressão quantílica e a perda quantílica
@article{regression-quantiles,
  ISSN = {00129682, 14680262},
  URL = {http://www.jstor.org/stable/1913643},
  abstract = {A simple minimization problem yielding the ordinary sample quantiles in the location model is shown to generalize naturally to the linear model generating a new class of statistics we term "regression quantiles." The estimator which minimizes the sum of absolute residuals is an important special case. Some equivariance properties and the joint asymptotic distribution of regression quantiles are established. These results permit a natural generalization of the linear model of certain well-known robust estimators of location. Estimators are suggested, which have comparable efficiency to least squares for Gaussian linear models while substantially out-performing the least-squares estimator over a wide class of non-Gaussian error distributions.},
  author = {Roger Koenker and Gilbert Bassett},
  journal = {Econometrica},
  number = {1},
  pages = {33--50},
  publisher = {[Wiley, Econometric Society]},
  title = {Regression Quantiles},
  urldate = {2025-11-08},
  volume = {46},
  year = {1978}
}








