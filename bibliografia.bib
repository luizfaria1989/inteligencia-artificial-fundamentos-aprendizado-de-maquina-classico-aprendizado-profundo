@book{DeepLearningBook,
  author    = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  title     = {Deep Learning},
  publisher = {MIT Press},
  year      = {2016},
}

@book{MaosAObra,
  author = {Aurélien Géron},
  title = {Mãos à Obra: Aprendizado de Máquina com Scikit-Learn e TensorFlow},
  publisher = {Alta Books},
  year = {2019},
}

-- Método do gradiente original por Cauchy
@article{CauchyMetodoDoGradiente,
  title   = {M{\'e}thode g{\'e}n{\'e}rale pour la r{\'e}solution des syst{\`e}mes d'{\'e}quations simultan{\'e}es},
  author  = {Cauchy, Augustin-Louis},
  journal = {Comptes Rendus Hebdomadaires des S{\'e}ances de l'Acad{\'e}mie des Sciences},
  volume  = {25},
  pages   = {536--538},
  year    = {1847}
}

-- Artigo que introduz a retropropagação
@article{BackpropagationArticle,
  author  = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
  title   = {Learning Representations by Back-Propagating Errors},
  journal = {Nature},
  year    = {1986},
  volume  = {323},
  pages   = {533--536},
}

-- Artigo que introduz os primeiros teoremas da aproximação universal
@article{Cybenko1989,
  author  = {Cybenko, George},
  title   = {Approximation by Superpositions of a Sigmoidal Function},
  journal = {Mathematics of Control, Signals, and Systems},
  year    = {1989},
  volume  = {2},
  number  = {4},
  pages   = {303--314},
}

-- Artigo que introduz os teoremas da aproximação universal para funções como a relu
@article{Leshno1993,
      title = {Multilayer feedforward networks with a nonpolynomial activation function can approximate any function},
      journal = {Neural Networks},
      volume = {6},
      number = {6},
      pages = {861-867},
      year = {1993},
      issn = {0893-6080},
      doi = {https://doi.org/10.1016/S0893-6080(05)80131-5},
      url = {https://www.sciencedirect.com/science/article/pii/S0893608005801315},
      author = {Moshe Leshno and Vladimir Ya. Lin and Allan Pinkus and Shimon Schocken},
      keywords = {Multilayer feedforward networks, Activation functions, Role of threshold, Universal approximation capabilities, (μ) approximation},
      abstract = {Several researchers characterized the activation function under which multilayer feedforward networks can act as universal approximators. We show that most of all the characterizations that were reported thus far in the literature are special cases of the following general result: A standard multilayer feedforward network with a locally bounded piecewise continuous activation function can approximate any continuous function to any degree of accuracy if and only if the network's activation function is not a polynomial. We also emphasize the important role of the threshold, asserting that without it the last theorem does not hold.}
}
-- Artigo que introduz o método do gradiente com momentum
@article{polyak1964,
  title={Some methods of speeding up the convergence of iteration methods},
  author={Polyak, Boris T.},
  journal={USSR Computational Mathematics and Mathematical Physics},
  volume={4},
  number={5},
  pages={1--17},
  year={1964},
  publisher={Elsevier}
}

-- Artigo que introduz o método de otimização Adam
@misc{AdamMethod,
      title={Adam: A Method for Stochastic Optimization}, 
      author={Diederik P. Kingma and Jimmy Ba},
      year={2017},
      eprint={1412.6980},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1412.6980}, 
      urldate={2025-09-15},
}

-- Artigo que introduz o método de otimização AdaGrad
@article{AdaGradMethod,
  author  = {John Duchi and Elad Hazan and Yoram Singer},
  title   = {Adaptive Subgradient Methods for Online Learning and Stochastic Optimization},
  journal = {Journal of Machine Learning Research},
  year    = {2011},
  volume  = {12},
  number  = {61},
  pages   = {2121--2159},
  url     = {http://jmlr.org/papers/v12/duchi11a.html},
  urldate={2025-09-15},
}

-- Artigo que introduz o método de otimização Nadam
@inproceedings{NadamMethod,
  title={Incorporating Nesterov Momentum into Adam},
  author={Dozat, Timothy},
  booktitle={ICLR 2016 Workshop Track},
  year={2016},
  url={https://openreview.net/forum?id=OM0DEvNMIxAaI-fG_O1-I},
  urldate={2025-09-15},
}

-- Artigo que introduz o método de otimização AdamW
@misc{AdamWMethod,
      title={Decoupled Weight Decay Regularization}, 
      author={Ilya Loshchilov and Frank Hutter},
      year={2019},
      eprint={1711.05101},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1711.05101}, 
      urldate={2025-09-15},
}

-- Artigo que introduz o método de otimização RAdam
@misc{RAdamMethod,
      title={On the Variance of the Adaptive Learning Rate and Beyond}, 
      author={Liyuan Liu and Haoming Jiang and Pengcheng He and Weizhu Chen and Xiaodong Liu and Jianfeng Gao and Jiawei Han},
      year={2021},
      eprint={1908.03265},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1908.03265}, 
      urldate={2025-09-15},
}

-- Artigo que introduz o método do gradiente estocástico
@article{StochasticGradientDescentMethod,
  author    = {Herbert Robbins and Sutton Monro},
  title     = {A Stochastic Approximation Method},
  journal   = {The Annals of Mathematical Statistics},
  year      = {1951},
  volume    = {22},
  number    = {3},
  pages     = {400--407},
  doi       = {10.1214/aoms/1177729586}
}

-- Adeline 
-- SGD aparece pela primeira vez em aprendizado de máquina
@inproceedings{Adeline,
  author    = {Bernard Widrow and Marcian E. Hoff},
  title     = {Adaptive Switching Circuits},
  booktitle = {1960 IRE WESCON Convention Record},
  year      = {1960},
  part      = {4},
  pages     = {96--104},
  publisher = {IRE}
}

-- Artigo que introduz o método de otimização do gradiente acelerado de nesterov (NAG)
@article{NAGMethod,
  title={A method for solving the convex programming problem with convergence rate $O(1/k^2)$},
  author={Yurii Nesterov},
  journal={Proceedings of the USSR Academy of Sciences},
  year={1983},
  volume={269},
  pages={543-547},
  url={https://api.semanticscholar.org/CorpusID:145918791}
}

-- Slides que introduziram o RMSProp
@misc{RMSPropMethod,
  author = {Tieleman, Tijmen and Hinton, Geoffrey},
  title  = {Lecture 6.5---RMSProp: Divide the gradient by a running average of its recent magnitude},
  year   = {2012},
  howpublished = {COURSERA: Neural Networks for Machine Learning},
  url    = {https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf},
  urldate={2025-10-03},
}

-- Um dos artigos que introduziu a sigmoide logística
@article{SigmoidVerhulst1845,
  author = {Verhulst, P. F.},
  title = {Recherches math{\'e}matiques sur la loi d'accroissement de la population},
  journal = {Nouveaux M{\'e}moires de l'Acad{\'e}mie Royale des Sciences et Belles-Lettres de Bruxelles},
  volume = {18},
  pages = {1--42},
  year = {1845},
  publisher = {L'Acad{\'e}mie Royale}
}

-- Um dos artigos que estuda o comportamento de neurônios e os compara com a sigmoide
@article{SigmoidWilsonCowan,
  author = {Wilson, Hugh R. and Cowan, Jack D.},
  title = {Excitatory and inhibitory interactions in localized populations of model neurons},
  journal = {Biophysical Journal},
  volume = {12},
  number = {1},
  pages = {1--24},
  year = {1972},
  publisher = {Biophysical Society},
  doi = {10.1016/S0006-3495(72)86068-5}
}

-- Artigo que introduz as redes de elman, utiliza a sigmoide como funcao de ativacao
@article{ElmanNetwork,
  author = {Elman, Jeffrey L.},
  title = {Finding structure in time},
  journal = {Cognitive Science},
  volume = {14},
  number = {2},
  pages = {179--211},
  year = {1990},
  doi = {10.1207/s15516709cog1402_1},
  publisher = {Wiley Online Library}
}

-- Artigo que introduz o perceptron
@article{PerceptronRosenblatt,
  author = {Rosenblatt, F.},
  title = {The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain},
  journal = {Psychological Review},
  volume = {65},
  number = {6},
  pages = {386--408},
  year = {1958},
  doi = {10.1037/h0042519}
}

-- Artigo que explica algumas funcoes de ativacao e suas propriedades
@misc{ActivationFunctionsLederer,
      title={Activation Functions in Artificial Neural Networks: A Systematic Overview}, 
      author={Johannes Lederer},
      year={2021},
      eprint={2101.09957},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2101.09957}, 
}

-- Artigo que introduziu a tangente hiperbólica
@incollection{TanhLambert,
      author    = {Lambert, J. H.},
      title     = {M{\'e}moire sur quelques propri{\'e}t{\'e}s remarquables des quantit{\'e}s transcendantes circulaires et logarithmiques},
      booktitle = {Pi: A Source Book},
      editor    = {Berggren, Lennart and Borwein, Jonathan M. and Borwein, Peter B.},
      publisher = {Springer-Verlag},
      address   = {New York},
      year      = {2004},
      pages     = {129--140},
      doi       = {10.1007/978-1-4757-4217-6_18},
      note      = {Original work published in 1768}
}

-- Artigo que introduziu a Le-Net-5
@article{LecunLeNet1998,
    author    = {LeCun, Yann and Bottou, L{\'e}on and Bengio, Yoshua and Haffner, Patrick},
    title     = {Gradient-based learning applied to document recognition},
    journal   = {Proceedings of the IEEE},
    volume    = {86},
    number    = {11},
    pages     = {2278--2324},
    year      = {1998},
    doi       = {10.1109/5.726791}
}

-- Artigo que introduziu a função de ativação sigmoidal softsign
@article{Softsign1998,
    author = {Foundation, The and Elliott, David},
    year = {1998},
    month = {12},
    pages = {},
    title = {A better Activation Function for Artificial Neural Networks}
}

-- Artigo que introduziu a função de ativação Leaky ReLU (LReLU)
@article{LeakyReLUArticle,
  author = {Andrew L. Mass, Awni Y. Hannum and Andrew Y. Ng},
  journal = {},
  number = {},
  title = {Rectifier Nonlinearities Improve Neural Networks Acustic Models},
  volume = {},
  year = {2013}
}

-- Artigo que introduziu a função de ativação Parametric ReLU (PReLU)
@misc{PReLUArticle,
      title={Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification}, 
      author={Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
      year={2015},
      eprint={1502.01852},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1502.01852}, 
}

-- Artigo que introduziu a função de ativação Exponential Linear Unit (ELU)
@misc{ELUArticle,
      title={Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)}, 
      author={Djork-Arné Clevert and Thomas Unterthiner and Sepp Hochreiter},
      year={2016},
      eprint={1511.07289},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1511.07289}, 
}

-- Artigo que introduziu a função de ativação SELU
@misc{SELUArticle,
      title={Self-Normalizing Neural Networks}, 
      author={Günter Klambauer and Thomas Unterthiner and Andreas Mayr and Sepp Hochreiter},
      year={2017},
      eprint={1706.02515},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1706.02515}, 
}

-- Artigo que introduziu a função de ativação GELU
@misc{GELUArticle,
      title={Gaussian Error Linear Units (GELUs)}, 
      author={Dan Hendrycks and Kevin Gimpel},
      year={2023},
      eprint={1606.08415},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1606.08415}, 
}

-- Artigo que introduziu o dataset CIFAR
@inproceedings{CIFAR10,
  title={Learning Multiple Layers of Features from Tiny Images},
  author={Alex Krizhevsky},
  year={2009},
  url={https://api.semanticscholar.org/CorpusID:18268744}
}

-- Um dos artigos que citam a função de ativação Noisy ReLU (NReLU)
@inproceedings{Nair2010,
  author    = {Nair, Vinod and Hinton, Geoffrey E.},
  title     = {Rectified Linear Units Improve Restricted Boltzmann Machines},
  booktitle = {Proceedings of the 27th International Conference on Machine Learning (ICML-10)},
  year      = {2010},
  pages     = {807--814},
}

-- Um dos artigos que estuda o comportamento das células nervosas
-- Utiliza uma versão da relu para explicar o comportamento
@article{Householder1941,
  author  = {Householder, Alston S.},
  title   = {A Theory of Steady-State Activity in Nerve-Fiber Networks: I. Definitions and Preliminary Theorems},
  journal = {The Bulletin of Mathematical Biophysics},
  year    = {1941},
  volume  = {3},
  number  = {2},
  pages   = {63--69},
}

-- Artigo que introduz a rede neural convolucional AlexNet
@inproceedings{AlexNet,
      author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
      booktitle = {Advances in Neural Information Processing Systems},
      editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},
      pages = {},
      publisher = {Curran Associates, Inc.},
      title = {ImageNet Classification with Deep Convolutional Neural Networks},
      url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf},
      volume = {25},
      year = {2012}
}

-- Um dos artigos que cita a RReLU
@article{XuRReLU,
  author  = {Xu, Bing and Wang, Naiyan and Chen, Tianqi and Li, Mu},
  title   = {Empirical Evaluation of Rectified Activations in Convolutional Network},
  journal = {arXiv preprint arXiv:1505.00853},
  year    = {2015},
}

-- Um dos artigos que cita propriedades da ReLU
@article{Glorot,
  author = {Xavier Glorot, Antaine Border and Yoshua Bengio},
  journal = {},
  number = {},
  title = {Deep Sparse Rectifier Neural Networks},
  volume = {},
  year = {2011}
}

-- Um dos artigos que cita o problema dos neurônios agonizantes
@misc{DyingReluDouglas,
      title={Why ReLU Units Sometimes Die: Analysis of Single-Unit Error Backpropagation in Neural Networks}, 
      author={Scott C. Douglas and Jiutian Yu},
      year={2018},
      eprint={1812.05981},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1812.05981}, 
}

-- Um dos artigos que cita o problema do gradiente explosivo
@misc{ExplodingGradient,
      title={The exploding gradient problem demystified - definition, prevalence, impact, origin, tradeoffs, and solutions}, 
      author={George Philipp and Dawn Song and Jaime G. Carbonell},
      year={2018},
      eprint={1712.05577},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1712.05577}, 
}

-- Documentação da hard sigmoid do PyTorch
@online{PyTorchHardSigmoid,
  author = {{PyTorch}},
  title = {Hardsigmoid},
  url = {https://pytorch.org/docs/stable/generated/torch.nn.Hardsigmoid.html},
  year = {2025},
  urldate = {2025-10-10}
}

-- Documentação da hard tanh do PyTorch
@online{PyTorchHardTanh,
  author = {{PyTorch}},
  title = {Hardtanh},
  url = {https://pytorch.org/docs/stable/generated/torch.nn.Hardtanh.html},
  year = {2025},
  urldate = {2025-10-10}
}

-- Artigo que discute um pouco das propriedades das funções de ativação
@misc{PropriedadesFuncoesDeAtivacao,
      title={A Survey on Activation Functions and their relation with Xavier and He Normal Initialization}, 
      author={Leonid Datta},
      year={2020},
      eprint={2004.06632},
      archivePrefix={arXiv},
      primaryClass={cs.NE},
      url={https://arxiv.org/abs/2004.06632}, 
}

-- Documentação da leaky relu do PyTorch
@online{PyTorchLeakyReLU,
  author = {{PyTorch}},
  title = {LeakyReLU},
  url = {https://docs.pytorch.org/docs/stable/generated/torch.nn.LeakyReLU.html},
  year = {2025},
  urldate = {2025-10-11}
}

-- Livro que cria a regressão dos mínimos quadrados e a mse
@book{Legendre1805,
  author    = {Legendre, Adrien-Marie},
  title     = {Nouvelles m{\'e}thodes pour la d{\'e}termination des orbites des com{\`e}tes},
  publisher = {Firmin Didot},
  year      = {1805},
  address   = {Paris},
  note      = {Cont{\'e}m o ap{\^e}ndice "Sur la M{\'e}thode des Moindres Quarr{\'e}s", que apresenta a primeira publica{\c c}{\~a}o do m{\'e}todo dos m{\'i}nimos quadrados.}
}

-- Livro do Gauss que cita a regressao dos minimos quadrados e a mse
@book{Gauss1809,
  author    = {Gauss, Carl Friedrich},
  title     = {{Theoria motus corporum coelestium in sectionibus conicis solem ambientium}},
  publisher = {sumtibus F. Perthes et I. H. Besser},
  year      = {1809},
  address   = {Hamburgi}
}

-- Artigo das funções de perda
@article{LossesArticle,
   title={A comprehensive survey of loss functions and metrics in deep learning},
   volume={58},
   ISSN={1573-7462},
   url={http://dx.doi.org/10.1007/s10462-025-11198-7},
   DOI={10.1007/s10462-025-11198-7},
   number={7},
   journal={Artificial Intelligence Review},
   publisher={Springer Science and Business Media LLC},
   author={Terven, Juan and Cordova-Esparza, Diana-Margarita and Romero-González, Julio-Alejandro and Ramírez-Pedraza, Alfonso and Chávez-Urbiola, E. A.},
   year={2025},
   urldate = {2025-10-17},
   month=apr 
}

-- Um dos artigos que faz uso do erro absoluto médio
@article{GreedyFunctionApproximation,
    author = {Jerome H. Friedman},
    title = {{Greedy function approximation: A gradient boosting machine.}},
    volume = {29},
    journal = {The Annals of Statistics},
    number = {5},
    publisher = {Institute of Mathematical Statistics},
    pages = {1189 -- 1232},
    keywords = {boosting, decision trees, Function estimation, robust nonparametric regression},
    year = {2001},
    doi = {10.1214/aos/1013203451},
    URL = {https://doi.org/10.1214/aos/1013203451},
    urldate = {2025-10-17}
}

-- Um dos artigos que faz uso do erro absoluto médio
@misc{ImageToImage,
      title={Image-to-Image Translation with Conditional Adversarial Networks}, 
      author={Phillip Isola and Jun-Yan Zhu and Tinghui Zhou and Alexei A. Efros},
      year={2018},
      eprint={1611.07004},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1611.07004}, 
      urldate={2025-10-17}
}

-- Artigo que introduz a huber loss
@article{HuberLoss,
  author = {Peter J. Huber},
  title = {{Robust Estimation of a Location Parameter}},
  volume = {35},
  journal = {The Annals of Mathematical Statistics},
  number = {1},
  publisher = {Institute of Mathematical Statistics},
  pages = {73 -- 101},
  year = {1964},
  doi = {10.1214/aoms/1177703732},
  URL = {https://doi.org/10.1214/aoms/1177703732},
  urldate={2025-10-17}
}

-- Definição de Entropia por Shannon
@ARTICLE{EntropyShannon,
  author={Shannon, C. E.},
  journal={The Bell System Technical Journal}, 
  title={A mathematical theory of communication}, 
  year={1948},
  volume={27},
  number={3},
  pages={379-423},
  keywords={},
  doi={10.1002/j.1538-7305.1948.tb01338.x}
}

-- Criação da Divergência de Kullback-leibler
@article{KullbackLeiblerDivergence,
  ISSN = {00034851},
  URL = {http://www.jstor.org/stable/2236703},
  author = {S. Kullback and R. A. Leibler},
  journal = {The Annals of Mathematical Statistics},
  number = {1},
  pages = {79--86},
  publisher = {Institute of Mathematical Statistics},
  title = {On Information and Sufficiency},
  urldate = {2025-10-18},
  volume = {22},
  year = {1951},
}

@article{HintonConnectionist,
  title = {Connectionist learning procedures},
  journal = {Artificial Intelligence},
  volume = {40},
  number = {1},
  pages = {185-234},
  year = {1989},
  issn = {0004-3702},
  doi = {https://doi.org/10.1016/0004-3702(89)90049-0},
  url = {https://www.sciencedirect.com/science/article/pii/0004370289900490},
  author = {Geoffrey E. Hinton},
  abstract = {A major goal of research on networks of neuron-like processing units is to discover efficient learning procedures that allow these networks to construct complex internal representations of their environment. The learning procedures must be capable of modifying the connection strengths in such a way that internal units which are not part of the input or output come to represent important features of the task domain. Several interesting gradient-descent procedures have recently been discovered. Each connection computes the derivative, with respect to the connection strength, of a global measure of the error in the performance of the network. The strength is then adjusted in the direction that decreases the error. These relatively simple, gradient-descent learning procedures work well for small tasks and the new challenge is to find ways of improving their convergence rate and their generalization abilities so that they can be applied to larger, more realistic tasks.},
  urldate={2025-10-18}
}

-- Artigo que introduz a Hinge Loss
@article{HingeLoss,
	abstract = {Thesupport-vector network is a new learning machine for two-group classification problems. The machine conceptually implements the following idea: input vectors are non-linearly mapped to a very high-dimension feature space. In this feature space a linear decision surface is constructed. Special properties of the decision surface ensures high generalization ability of the learning machine. The idea behind the support-vector network was previously implemented for the restricted case where the training data can be separated without errors. We here extend this result to non-separable training data.},
	author = {Cortes, Corinna and Vapnik, Vladimir},
	date = {1995/09/01},
	date-added = {2025-10-19 14:29:02 -0300},
	date-modified = {2025-10-19 14:29:02 -0300},
	doi = {10.1007/BF00994018},
	id = {Cortes1995},
	isbn = {1573-0565},
	journal = {Machine Learning},
	number = {3},
	pages = {273--297},
	title = {Support-vector networks},
	url = {https://doi.org/10.1007/BF00994018},
	volume = {20},
	year = {1995},
	bdsk-url-1 = {https://doi.org/10.1007/BF00994018}
  
  }


