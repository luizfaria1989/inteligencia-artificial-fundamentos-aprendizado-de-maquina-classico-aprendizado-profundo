@book{DeepLearningBook,
  author    = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  title     = {Deep Learning},
  publisher = {MIT Press},
  year      = {2016},
}

-- Método do gradiente original por Cauchy
@article{CauchyMetodoDoGradiente,
  title   = {M{\'e}thode g{\'e}n{\'e}rale pour la r{\'e}solution des syst{\`e}mes d'{\'e}quations simultan{\'e}es},
  author  = {Cauchy, Augustin-Louis},
  journal = {Comptes Rendus Hebdomadaires des S{\'e}ances de l'Acad{\'e}mie des Sciences},
  volume  = {25},
  pages   = {536--538},
  year    = {1847}
}

-- Artigo que introduz a retropropagação
@article{BackpropagationArticle,
  author  = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
  title   = {Learning Representations by Back-Propagating Errors},
  journal = {Nature},
  year    = {1986},
  volume  = {323},
  pages   = {533--536},
}

-- Artigo que introduz os primeiros teoremas da aproximção universal
@article{Cybenko1989,
  author  = {Cybenko, George},
  title   = {Approximation by Superpositions of a Sigmoidal Function},
  journal = {Mathematics of Control, Signals, and Systems},
  year    = {1989},
  volume  = {2},
  number  = {4},
  pages   = {303--314},
}

-- Artigo que introduz os teoremas da aproximação universal para funções como a relu
@article{Leshno1993,
      title = {Multilayer feedforward networks with a nonpolynomial activation function can approximate any function},
      journal = {Neural Networks},
      volume = {6},
      number = {6},
      pages = {861-867},
      year = {1993},
      issn = {0893-6080},
      doi = {https://doi.org/10.1016/S0893-6080(05)80131-5},
      url = {https://www.sciencedirect.com/science/article/pii/S0893608005801315},
      author = {Moshe Leshno and Vladimir Ya. Lin and Allan Pinkus and Shimon Schocken},
      keywords = {Multilayer feedforward networks, Activation functions, Role of threshold, Universal approximation capabilities, (μ) approximation},
      abstract = {Several researchers characterized the activation function under which multilayer feedforward networks can act as universal approximators. We show that most of all the characterizations that were reported thus far in the literature are special cases of the following general result: A standard multilayer feedforward network with a locally bounded piecewise continuous activation function can approximate any continuous function to any degree of accuracy if and only if the network's activation function is not a polynomial. We also emphasize the important role of the threshold, asserting that without it the last theorem does not hold.}
}
-- Artigo que introduz o método do gradiente com momentum
@article{polyak1964,
  title={Some methods of speeding up the convergence of iteration methods},
  author={Polyak, Boris T.},
  journal={USSR Computational Mathematics and Mathematical Physics},
  volume={4},
  number={5},
  pages={1--17},
  year={1964},
  publisher={Elsevier}
}

-- Artigo que introduz o método de otimização Adam
@misc{AdamMethod,
      title={Adam: A Method for Stochastic Optimization}, 
      author={Diederik P. Kingma and Jimmy Ba},
      year={2017},
      eprint={1412.6980},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1412.6980}, 
}